\include{begin_article}
\title{RVQ Experiments}
\author{Salman Aslam\\ Georgia Institute of Technology}
\date{}
\include{inkscapeLatex}
\begin{document}
\maketitle
\rule[0pt]{\textwidth}{1pt}
\tableofcontents
\rule[0pt]{\textwidth}{1pt}

%================================
\section{Introduction}
%================================
Our goal is to examine the effects on training and test error of varying $q$, the number of stages in RVQ, while keeping $m$, the number of code-vectors per stage fixed.  For this, we train an 8x4 RVQ (maximum q=8, m=4) using four different data distributions in $\mathbb{R}^{1089}$: (a) Dudek sequence, (b) Uniform random variable, (c) Gaussian random variable, and (d) Gauss-Markov random variable.

%================================
\section{Theory}
%================================
While examining the effects on training and test error of varying $q$, we would also like to relate the notions of generalization ability of RVQ, varying $q$ and VC dimension in statistical learning theory.  The  motivation behind this is to get a theoretical justification for using a particular $q$ in a given situation.  In particular, this is given by the notion of empirical risk minimization.

The concept of VC dimension was introduced by Vapnik~\cite{1999_BOOK_PRML_Vapnik} as a means of specifying the generalization ability of classifiers.  In this work, we look at some definitions leading up to the VC dimension.  It has been shown that the VC dimension of a nearest neighbor classifier is equal to the number of reference points in the training set~\cite{2003_JNL_PRML_Karacali}.  We extend this to show that the VC dimension of an RVQ classifier depends on both its number of stages $Q$ and the number of code-vectors per stage $M$.


In pattern recognition, a \emph{classifier} $g(x)$ is a function that maps $x_i \in \mathbb{R}^D$ to a set of $M$ discrete \emph{class} labels, $\theta  \in \{1, 2, \ldots M\}$, i.e., $g(x):~\mathbb{R}^D~\rightarrow~\{1, 2, \ldots M\}$.  The classifier errs if $g(x) \neq \theta$~\cite{1996_BOOK_PR_DevroyeGyorfiLugosi}.  Since it is not possible to create a classifier that always achieves perfect mapping, we create a probabilistic setting and let $\mathbf{X} \times \mathbf{\Theta}$ be an $\mathbb{R}^D \times \{1, 2, \ldots M\}$-valued random pair.  

\begin{enumerate}
\item \underline{Risk}.  The probability of error for estimator $g(x)$, the {\color{blue}\emph{risk}} is

\begin{equation}
\boxed{
{\color{blue}R(g)} = \mathbf{P}(g(\mathbf{X}) \neq \mathbf{\Theta})}
\label{Eqn:loss}
\end{equation}

\item \underline{Bayes Risk}. In Equation~\ref{Eqn:loss}, the minimal probability of error $\mathbf{P}^*$ is called the \emph{Bayes error} or the \emph{Bayes risk}.  

\item \underline{Bayes Classifier}.  In Equation~\ref{Eqn:loss}, the best classifier $g^*$ is called the \emph{Bayes classifier} or the \emph{Bayes rule}, and is given by

\begin{equation}
g^* = \arg\min_{\tiny g:~\mathbb{R}^D~\rightarrow~\{1, 2, \ldots M\}} {\color{blue}R(g)}
\end{equation}

\item \underline{Empirical risk}. In practical situations, $\mathbf{P}(g(\mathbf{X}) \neq \mathbf{\Theta})$ is generally unknown, and therefore so is $g^*$.  However, we assume that we have $N$ i.i.d. random pairs of training data, $\{(x_1, \theta_1), (x_2, \theta_2), \ldots, (x_N, \theta_N)\}$ with the same distribution as $p(\mathbf{X},\mathbf{\Theta})$.  The empirical risk function for this data is $R_N$, or $R(g_N)$~\footnote{The quantity $\mathbb{E}\left[R_N\right]$ is marginally useful since it is the quality of an average data sequence and not the data sequence at hand.  A \emph{consistent} classifier is one for which $\lim\limits_{N \rightarrow \infty}\mathbb{E}\left[R_N\right] = R^*$~\cite{1996_BOOK_PR_DevroyeGyorfiLugosi}.}.
  
\item \underline{Empirical risk minimization}.
Since we cannot find $g^*$, we change the setting and define $R$ as being the risk of the best classifier in class $\mathcal{C}$, for instance, all $k$-nearest neighbor classifiers with all possible values of $k$.  Then,

\begin{equation}
R \triangleq \inf\limits_{g_N \in \mathcal{C}} \mathbf{P}(g_N(\mathbf{X}) \neq \mathbf{\Theta})
\end{equation}

Using \emph{empirical risk minimization}~\cite{1999_BOOK_PRML_Vapnik}, we select a classifier $g_N$ from a class $\mathcal{C}$ by minimizing, 

\begin{equation}
\frac{1}{N} \sum\limits_{n=1}^N I_{g_N(x_n \neq \theta_n)}
\end{equation}

then the corresponding risk $R_N$ satisfies the following inequality for all $\epsilon > 0$

\begin{equation}
\mathbf{P}(R_N > R + \epsilon) \leq 8(N^h + 1) e^{-N\epsilon^2/128}
\end{equation}

where $h$ is the Vapnik Chervonenkis (VC) dimension.  The concept of VC dimension is related to \emph{shattering}.

\item \underline{Shattering}.  In the two-class classification problem, a given set of $N$ points can be labeled in $2^N$ ways.  If, for all possible labelings, a classifier $g(x)$ from the class $\mathcal{C}$ can be found which correctly assigns those labels, then we say that the set of $N$ points is shattered by the class $\mathcal{C}$~\cite{1998_JNL_SVM_Burges}.  An example of shattering in $\mathbb{R}^2$ is given in Figure~\ref{fig:shattering}.

\item \underline{VC dimension}.  If a classifier class $\mathcal{C}$, such as the class of linear classifiers, or the class of $k$-nearest neighbors with different values of $k$, has VC dimension $h$, then there exists at least one set of $h$ points that can be shattered by it, but in general, not every set of $h$ points can be shattered by the class~\cite{1998_JNL_SVM_Burges}.  For instance, in Figure~\ref{fig:shattering}, three collinear points cannot be shattered by a line.  One motivation to use the VC dimension is that it can be used to place a bound on test error given training error.  For $N$ training examples, the following bound holds with probability $1-\eta$\cite{1999_BOOK_PRML_Vapnik}:

\begin{equation}
R_{tst} \leq R_{trg} + \sqrt\frac{h(\log (2N/h)+1) - \log (\eta/4)}{N}
\end{equation}

This is a loose bound since it is over all possible distributions.  However, in many cases, for a given distribution, the actual bound is tighter~\cite{Videolectures.net}.

\item \underline{Nearest neighbor classifier}.  The most popular nonparametric classifier is the nearest neighbor classifier that assigns the class label of the nearest neighbor in the training set to unknown data~\cite{2003_JNL_PRML_Karacali}.  For the nearest neighbor rule, for all distributions, the risk $R_{1NN}$ is bounded above by twice the Bayes risk,

\begin{equation}
\lim\limits_{N \rightarrow \infty} \sup\mathbb{E}\left[R_{1NN}\right] \leq 2R^*  
\end{equation}

In this sense, half of the available information in an infinite collection of classified samples is contained in the nearest neighbor~\cite{1967_JNL_PRML_Cover}.  This provides strong motivation to use the 1NN classifier.  Moreover, it has been shown to work well under several situations~\cite{1996_BOOK_PR_DevroyeGyorfiLugosi}.

								\begin{figure}[t]
								\centering
								\includegraphics[width=1.0\textwidth]{thesis2/theory_PRML_shattering.pdf}
								\caption{A linear classifier in $\mathbb{R}^D$ can shatter $D+1$ points.  Here, in the first 3 examples, a line in $\mathbb{R}^2$ is able to shatter 3 points.  However, in the fourth example, it is unable to shatter 4 points.}
								\label{fig:shattering}
								\end{figure}

\item \underline{VC dimension of NN classifier}.   It has been shown by Kara\c{c}ali and Krim~\cite{2003_JNL_PRML_Karacali} that the VC dimension of the NN classifier is given by the number of reference points in the training set~\cite{2005_CNF_ML_Angiulli}.  These reference points can be obtained from the codevectors of a VQ codebook, such as an RVQ codebook.  For an RVQ codebook with $Q$ stages, and $M$ code-vectors per stage, there are $M^Q$ equivalent code-vectors.  Moreover, the number of direct-sum code-vectors at the end of the $q$-th stage is $M^q$\footnote{This assumes that there are no null code-vectors, i.e. code-vectors in which all elements are 0.}.   Therefore, the VC dimension of an RVQ nearest neighbor classifier in a variable-rate framework depends on how many stages and codevectors per stage are used.  Changing these numbers can be used to change the VC dimension of the RVQ NN classifier, and hence its generalization ability.  
\end{enumerate}

 


%================================
\section{Experiments}
%================================
In the following experiments, we keep $m$, the number of code-vectors per stage constant and vary $q$, the number of stages.  We see that as stages increase, the training error tends to decrease and so does test error, a sign of increasing VC dimension.

We make the following observations in Figure~\ref{fig:RVQ_results}:

\begin{itemize}
\item Training error decreases monotonically for eRMSE and dRMSE.
\item dRMSE is lower than eRMSE since the decoder is subject to one constraint while the encoder is subject to two.
\item Test error is always larger than training error.
\item For the Dudek experiment
\begin{itemize}
\item The error levels out after about 4 stages telling us that by about q=4, we are able to explain the statistical correlations in the data.  
\item RofE tends to further decrease in error after 5th stage telling us that the test snippet is a lot like the training snippet.  
\item maxQ which is forced to go all the way tells us, as mentioned in~\cite{1996_JNL_AdvancesRVQ_Barnes}, that using more stages in decoding will in many cases decrease rmse even if there are slight "bumps", i.e. increases in rmse along the way.  
\item monR, the greedy approach has highest rmse.
\item The curve has the "knee" that one would expect with a classifier as one increases the DoF.
\item At level 5, we see that there is clearly an increase in rms error which is why monR and nulE level out (monR exits but I continue showing its exit rmse), nulE rides out the wave and in the last stage is able to decrease rms error by another 0.0062.
\item maxQ and RofE take the very small increases in rms error, since they're not greedy, and are rewarded with gains as they use more and more stages.
\end{itemize}
\item The reconstruction error for the Gaussian and uniform random variables is almost constant since adding stages cannot explain a test example that is statistically independent of the training data.  We see the same behavior in PCA and TSVQ as well.
\item For the Gauss-Markov example, the statistical correlation is captured in about 2 stages.
\item Of the random variable experiments, the highest reconstruction errors are for Gaussian, uniform and Gauss-Markov random variables, as expected.  The reason is that the Gaussian distribution has the highest entropy, followed by the uniform random variable, and then followed by the statistically correlated Gauss-Markov random variable.
\end{itemize}

\begin{figure}
\subfigure[Dudek sequence, a 100 33x33 ($\mathbb{R}^{1089}$) face snippets were extracted from the first 100 images and used as training examples.  The test example is a single 33x33 snippet from the 101st image.]{\includegraphics[width=0.45\textwidth]{thesis2/RVQ_8x4_Dudek_trg_1_to_100_tst_101.pdf}}\hspace{0.2in}
\subfigure[Uniform random variable $U\sim$ \texttt{[}0, 1\texttt{]} in $\mathbb{R}^{1089}$, 100 realizations were used for training.  The test example is a single realization in $\mathbb{R}^{1089}$.]{\includegraphics[width=0.45\textwidth]{thesis2/RVQ_8x4_uniform_trg_100_tst_1.pdf}}
\subfigure[Gaussian random variable $\mathcal{N}\sim$(0, 1) in $\mathbb{R}^{1089}$, 100 realizations were used for training.  The test example is a single realization in $\mathbb{R}^{1089}$.]{\includegraphics[width=0.45\textwidth]{thesis2/RVQ_8x4_Gaussian_trg_100_tst_1.pdf}}\hspace{0.55in}
\subfigure[Gauss-Markov random variable $\mathcal{N}\sim$(0, 1) in $\mathbb{R}^{1089}$ with 0.9 correlation, 100 realizations were used for training.  The test example is the 101st realization in $\mathbb{R}^{1089}$ and has correlation with the 100th realization.]{\includegraphics[width=0.45\textwidth]{thesis2/RVQ_8x4_GaussMarkov_trg_100_tst_1.pdf}}
\caption{RVQ experiments with 8x4 codebook. 100 training examples in $\mathbb{R}^{1089}$ were used for each of these experiments.  A single test example in $\mathbb{R}^{1089}$ was reconstructed.}
\label{fig:RVQ_results}
\end{figure}

%================================
\section{Conclusions}
%================================
In these experiments, we see that as $q$, the number of stages increases, training error and test error decrease.  This is as expected.  However, we intend to do more experiments using cross-validation to gain more confidence in these results.

These initial experiments appear to confirm the fact that RVQ has variable VC dimension.  Whereas it is to be expected that increasing stages will decrease reconstruction error, both at design-time and run-time, ERM (empirical risk minimization) from statistical learning theory can be used as a justification for the number of stages $q$ that are used in a given classification or tracking scenario.


\clearpage
\newpage
\normalsize
\bibliographystyle{ieee}
\bibliography{MyCitations}
\end{document}