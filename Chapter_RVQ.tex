\include{Chapter0_begin}
\include{inkscapeLatex}
\begin{document}
\begin{FrontMatter}
\contents %generates the TOC, LOF, and LOT
\end{FrontMatter}
\begin{Body}
%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
\chapter{Residual Vector Quantization (RVQ)}
\label{chap_RVQ}	
%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
%==========================
\section{Introduction}
\label{sec:introduction}
%==========================
Our main emphasis in this chapter is on Residual Vector Quantization (RVQ).  However, in order to understand RVQ, certain definitions need to be presented.  Also, an understanding of quantization, types of vector quantization and a comparison of different types of vector quantization is important.  This chapter is therefore organized as follows:

\begin{enumerate}
\item \underline{Definitions}.  In Section~\ref{sec:definitions}, various definitions related to the study of quantization are presented.
\item \underline{Optimality} In Section~\ref{sec:quantization_optimality}, we discuss optimality issues in quantization.
\item \underline{Types of vector quantization (VQ)}.  In Section~\ref{sec:types_VQ}, we discuss different types of VQ, including exhaustive search vector quantization (ESVQ),  tree structured vector quantization (TSVQ) and residual vector quantization (RVQ).
\item \underline{Comparison of different types of VQ}.  In Section~\ref{sec:comparison_VQ}, we present a comparison of ESVQ, TSVQ and RVQ.
\end{enumerate}

								\begin{figure}[t]	
								\centering		
								\fbox{
								\includegraphics[width=0.6\textwidth]{thesis/Quantization_blockDiagram.pdf}}
								\caption{A quantizer $\mathcal{Q}$ maps symbols from a source alphabet $\mathcal{X}$ to symbols from a reconstruction alphabet $\mathcal{C}$, where in general, the number of elements in $\mathcal{X}$, $N >> K$, the number of elements in $\mathcal{C}$.}
								\label{fig:Quantization_block_diagram}
								\end{figure}


%====================
\section{Definitions}
\label{sec:definitions}
%====================
In this section, we present the following definitions before getting into the details of optimality issues in quantization and its different types:

\begin{enumerate}
\item \underline{Quantization}.  Quantization is the process of representing a large, possibly infinite, set of values with a smaller set of values~\cite{2005_BOOK_DataCompression_Sayood}.  Figure~\ref{fig:Quantization_block_diagram} shows a quantizer $\mathcal{Q}$ that takes values from a source alphabet $\mathcal{X}=\{\mathbf{x} \in \mathbb{R}^D\}$ and maps them to a reconstruction alphabet $\mathcal{C}=\{\mathbf{y}_k \in \ \mathbb{R}^D \ | \ k=1,2, \ldots K\}$.  If the input is scalar, i.e. $D=1$, the quantizer is called a \emph{scalar quantizer}.  For $D>1$, the quantizer is called a \emph{vector quantizer}.  Quantization is used widely for achieving lossy compression in images and videos.  For instance, all current video standards, MPEG-1, MPEG-2, MPEG-4, H.261, H.263 and H.264 rely on a special form of quantization called \emph{transform vector quantization}.

\item \underline{Parititions}.  Quantization creates $K$ partitions $\mathcal{P}_k = \{\mathbf{x} \ \in \ \mathbb{R}^D \ | \ \mathcal{Q}(\mathbf{x}) = \mathbf{y}_k\}$ in the input space $\mathbb{R}^D$ which are mutually exclusive and exhaustive, i.e., $\mathcal{P}_i \bigcap \mathcal{P}_j = \emptyset, \ i \neq j$.  The union of these partitions covers the entire input space, $\bigcup\limits_{k=1}^{K} \mathcal{P}_k=\mathbb{R}^D$.

\item \underline{Codebook}. The reconstruction alphabet $\mathcal{C}$ is known as the \emph{codebook}.  

\item \underline{Code-vectors}.  The $K$ members $\mathbf{y}_k$ of the reconstruction alphabet $\mathcal{C}$ are called \emph{code-vectors}.    

\item \underline{Rate}.  If we have $K$ code-vectors $\mathbf{y}_k$ in $\mathbb{R}^D$, $\log_2 K$ bits are required to represent each code-vector.   The \emph{resolution}, \emph{code rate}, or simply the \emph{rate} $r$ of a quantizer is the number of bits required to represent each sample, i.e., scalar element of $\mathbf{y}_k$.  Since there are $D$ samples, the rate $r=\frac{\log_2 K}{D}$.  

\item \underline{Distortion}.  The difference between original input $\mathbf{x}$ and reconstructed output $\mathcal{Q}(\mathbf{x})=\mathbf{y}$ is known as \emph{distortion} $d(\mathbf{x}, \mathbf{y})$~\cite{2005_BOOK_DataCompression_Sayood}.  A commonly used distortion measure is the squared error criterion, $d(\mathbf{x}, \mathbf{y})=(\mathbf{x}-\mathbf{y})^2$.  Average distortion is given by $\mathcal{D(\mathcal{X}, \mathcal{C})}~=~E\left[d(\mathbf{x}, \mathbf{y}) \right]$.  

%This shows that for the same number of codevectors $K$ and for a given distortion, less rate is required to encode an input source for he rate decreases linearly as the dimensionality $D$ of the data is increased.  For this reason, vector quantization is able to achieve higher rates than scalar quantization.



								\begin{figure}[t]			
								\includegraphics[width=1\textwidth]{thesis/Quantization_MSE.pdf}
								\caption{Computing optimal centroids.}
								\label{fig:computing_optimal_centroids}	
								\end{figure}




\item \underline{Rate-distortion R(D) curve}.  The tradeoff between rate and distortion can be plotted as a \emph{rate-distortion} curve.  A fundamental result of Shannon's rate-distortion theory is that VQ is able to achieve equal or better compression rates than scalar quantization even if the source is memoryless, i.e., emits a sequence of IID random variables~\cite{1984_JNL_VQ_Gray}.   The reason is that VQ is able to take advantage of higher dimensionality using appropriate cell-shapes~\cite{1985_JNL_VQ_Makhoul}.  For instance, it is shown in~\cite{1985_JNL_VQ_Makhoul} that a hexagon quantizer will gain 0.028 bits over a square quantizer in $\mathbb{R}^2$ when the two random variables are independent.  On the other hand, if the source is not memoryless, 3 situations are possible:

\begin{itemize}
\item \underline{Linear correlation only}. Here, with proper rotation, any set of random variables can be rendered uncorrelated, i.e., they will no longer be linearly correlated~\cite{1985_JNL_VQ_Makhoul}.  In this case, since there is no non-linear correlation, the set of random variables after appropriate rotation will be independent.  Scalar quantization along each new dimension will produce lower bit rate than if the rotation had not been carried out.  However, VQ will reduce the bit-rate even more than scalar quantization since, as mentioned above, it can take advantage of appropriate cell-shapes.
\item \underline{Non-linear correlation only}.  Here, rotation cannot be used to remove non-linear correlation and therefore scalar quantization on rotated axes will not improve bit rate.  Moreover, scalar quantization produces rectangular cells, irrespective of the input distribution.  VQ on the other hand is able to place centroids only in regions occupied by the input.  This property can be used to exploit non-linear correlations and lower bit-rate~\cite{1985_JNL_VQ_Makhoul}.  Note that this property is independent of the appropriate cell-shapes property.  Using the appropriate cell-shapes property will lead to further reduction in bit-rate.

\item \underline{Linear and non-linear correlation}.  In this case, VQ can be used to reduce bit rates using all 3 methods mentioned above, rotation, placement of code-vectors in places where the input distribution exists, and using appropriate cell-shapes.
\end{itemize}

\end{enumerate}

								\begin{figure}[t]		
								\center	
								\includegraphics[height=0.4\textheight]{thesis/MPEG4_VTK.png}
								\caption{Scalar quantization in the transform domain for MPEG-4.  The image on the left shows the 3 intensity channels of an input image patch drawn in the YUV color space.  The vertical dimension is the luma (Y) axis.  The right image shows the quantized reconstruction of the input image patch.  No deblocking filter has been used, and so the loss of information is entirely due to quantization.  Notice the straight lines along which the output pixels are aligned due to the quantization process.  The visualization was created using the Visualization Toolkit (VTK) \cite{VTK} in C.}
								\label{fig:MPEG4_VTK}
								\end{figure}

%====================
\section{Quantization optimality}
\label{sec:quantization_optimality}
%====================


								\begin{figure}[t]		
								\center	
								\includegraphics[height=0.5\textheight]{thesis/Quantization_optimalPartitions2.pdf}
								\caption{Computing optimal partitions}
								\label{fig:computing_optimal_partitions}
								\end{figure}

So far, no mention has been made about optimality of the codevectors or partitions.  A widely used algorithm to compute at least locally optimal codevectors and partitions is the Generalized Lloyd Algorithm (GLA) \cite{1991_BOOK_VQ_GershoGray}, also known as the Linde Buzo Gray (LBG) algorithm \cite{1982_JNL_LeastSquaresQuantization_Lloyd} or K-means clustering \cite{1967_CNF_Kmeans_Macqueen}.  Figure~\ref{fig:computing_optimal_centroids} illustrates the scalar quantization case for an input $X$ with distribution $f_X(x)$ and squared error distortion, 

\begin{equation}
e=\sum\limits_{k=1}^{N} \int\limits_{b_k}^{b_{k+1}}(x-\mu_k)^2f_X(x)
\end{equation}

If the partitions $\mathcal{P}_k$,

\begin{equation}
\label{Eq:partitions}
\mathcal{P}_k =  \left\{x \ | \ (\mu_j-x)^2 < (\mu_k-x)^2, \ \forall \ j \neq k \right\}
\end{equation}

are given, an optimal codevector $\mu_k$ can be computed by setting the derivative of the error $e$ with respect to $\mu_k$ equal to 0,

\begin{equation}
\frac{\partial{e}}{\partial{\mu_k}} = 0
\end{equation}

In other words, the optimal centroid for a given partition and the squared error criterion is the centroid of the partition. 

\begin{equation}
\mu_k = \frac
{\int\limits_{b_k}^{b_{k+1}}xf_X(x)dx}
{\int\limits_{b_k}^{b_{k+1}}f_X(x)dx}
\end{equation}

To compute optimal partitions, given the centroids, we can rewrite Equation~\ref{Eq:partitions} as 

\begin{equation}
\label{Eq:partitions2}
P_j=\left\{x \ | \ (\mu_k -\mu_j) \left(x - \frac{1}{2} \left(\mu_k + \mu_j \right)\right) < 0, \ \forall j \neq k 
\right\}
\end{equation}

For the scalar case in Figure~\ref{fig:computing_optimal_partitions}, if $k>j$, i.e. $(\mu_k -\mu_j) > 0$, then to satisfy Equation~\ref{Eq:partitions2}

\begin{align}
\label{Eq:partitions3}
x - \frac{1}{2} \left(\mu_k + \mu_j \right) &< 0\notag\\
\Rightarrow x &< \frac{1}{2} \left(\mu_k + \mu_j \right)
\end{align}

Conversely, if $k<j$, i.e. $(\mu_k -\mu_j) < 0$, then to satisfy Equation~\ref{Eq:partitions2}

\begin{align}
\label{Eq:partitions4}
x - \frac{1}{2} \left(\mu_k + \mu_j \right) &> 0 \notag\\
\Rightarrow x &> \frac{1}{2} \left(\mu_k + \mu_j \right)
\end{align}

The only way to satisfy both Equations~\ref{Eq:partitions3} and \ref{Eq:partitions4} is for the partition boundary to be half-way between the centroids, i.e. $\frac{1}{2} \left(\mu_k + \mu_j \right)$.  This notion can be generalized to the vector case.

%====================
\section{Types of VQ}
\label{sec:types_VQ}
%====================

								\begin{figure}[htp]				
								\includegraphics[width=1.1\textwidth]{thesis/RVQ_comparisonWithESVQ_TSVQ.pdf}
								\caption{Graphical comparison of ESVQ, TSVQ and RVQ, 16 codevectors}
								\label{fig:comparison_ESVQ_TSVQ_RVQ}
								\end{figure}


We now list the main types of VQ that appear in the literature.  The goal of VQ design is to have output distortion as close as possible to the rate-distortion curve.  However, in general, optimal coding of source vectors is not possible unless an exhaustive search over all code-vectors is carried out, as in structurally unconstrained \emph{Exhaustive Search Vector Quantizers} (ESVQs) \cite{1992_JNL_RVQ_Barnes}.  For a rate $r$ and dimension $D$, there are $K=2^{rD}$ code-vectors.  The computational costs of $EVSQ$, $C_{ESVQ}$, and memory requirements $M_{ESVQ}$ are $\approx 2^{rD}$.  A solution to this problem is to impose constraints on the VQ structure.  One possible solution is the Tree Structured VQ (TSVQ) proposed in \cite{1980_JNL_TSVQ_Buzo}.  A $T$-level, $m$-ary TSVQ has search complexity $C_{TSVQ} \approx mT$ but double storage requirements, $M_{TSVQ} \approx 2 M_{ESVQ}$.   So, although the $TVSQ$ solves the search complexity problem, it further aggravates the storage problem.  A method of reducing both computational and storage complexity is to use a product code VQ~\cite{1991_BOOK_VQ_GershoGray}.  The basic idea in a product code VQ is to break a bigger problem into several smaller problems.  Examples include mean-residual VQ, gain-shape VQ and mean-gain-shape VQ \cite{1996_JNL_AdvancesRVQ_Barnes}.  Residual Vector Quantizers (RVQ) also fall under this category, and are of interest to us in this work.  A comparison of ESVQ, TSVQ and RVQ is given in Table~\ref{tab:comparison_ESVQ_TSVQ_RVQ}.  A graphical comparison with 16 codevectors is given in Figure~\ref{fig:comparison_ESVQ_TSVQ_RVQ}.

%----------------------------------
\subsection{TSVQ}
%----------------------------------
The Tree Structured Vector Quantizer (TSVQ) has received a lot of attention in the literature~\cite{1991_BOOK_VQ_GershoGray}.  The reason is that the codebook produced by a TSVQ approximates the codebook produced by an Exhaustive Search Vector (ESVQ) but the run-time computational cost is logarithmic in the number of code-vectors.  The storage requirements however are greater.  A comparison of ESVQ, TSVQ and RVQ can be seen in Table~\ref{tab:comparison_ESVQ_TSVQ_RVQ}.

In TSVQ design, the first step is to compute the mean of the data.  All the data is mapped to this mean.  The mean is then split off into $M_{TSVQ}$ centroids (code-vectors).  For a binary TSVQ, $M=2$.  The data is mapped to these centroids using the Nearest Neighbor rule.  Each of these $M_{TSVQ}$ parent centroids is then again split into $M_{TSVQ}$ child centroids.  Splitting can be achieved by multiple iterations of the K-means algorithm to get centroids that give low mean squared error.  At every stage of the tree, data belonging to a parent code-vector is mapped to the child code-vectors after the splitting occurs.  Notice that the important code-vectors are the last stage children code-vectors, i.e. the terminal leaves of the tree.  However, during run-time, the parent code-vectors, i.e., the non-terminal nodes of the tree have to be stored in order to be able to traverse the tree to get to the terminal code-vectors.  The process of mapping data at run-time to the terminal code-vectors is quite straightforward.  The data is first mapped to the mean, which is a trivial step, since all data starts at the top of the tree, i.e., the mean.  Each data-point is then mapped to one of the two children nodes at the first stage using the Nearest Neighbor method.  This process continues till a terminal code-vector is reached which is then used as an approximation to the input data-point.

In this work, we use a binary and balanced TSVQ.  In the binary case, the storage requirements are double the storage requirements for an equivalent ESVQ.  However, the run-time savings decrease logarithmically.  For instance, a codebook size of $K=256$ requires 256 matches for ESVQ but only 8 matches for a binary TSVQ.  

During tracking, a TSVQ codebook is designed every $N_B=5$ frames using $N_w$ images in the training buffer.  The particle filter candidate target regions in the current frame are tested against this code-book, i.e., for each candidate region, the terminal code-vector  to the mean squared error between that region and the terminal code-vector in the tree that 


%----------------------------------
\subsection{RVQ}
%----------------------------------
Residual Vector Quantizers were introduced by Juang et al. \cite{1982_CNF_SpeechRVQ_JuangGray}.  The RVQ structure is shown in Figure \ref{fig:RVQ_block_diagram}.  Initially a crude quantization of the input vector is carried out using a small codebook.  Then a second stage quantizer operates on the error of the first quantizer.  A third quantizer may be used to quantize the second error vector, and so on.  In other words, RVQs are realized as a sequence of small ESVQs.  Each stage operates on the error vector or \emph{residual} of the preceding ESVQ \cite{1991_CNF_DesignPerformanceRVQ_Frost}.  This direct sum structure coupled with a sequential search procedure enables the RVQ to have linear rather than exponential complexity, both in computations and in memory.  Two alternate graphical representations of this direct sum structure are given in Figure~\ref{fig:RVQ_sigma_tree}.

\begin{figure}				
	\includegraphics[width=1\textwidth]{thesis/RVQ_blockDiagram.pdf}
	\caption{RVQ: block diagram}
	\label{fig:RVQ_block_diagram}
\end{figure}

An RVQ is different from a traditional VQ in the sense that it partitions the input space $\mathbb{R}^D$ into $M$ cells.  The residual space, also in $\mathbb{R}^D$, is then partitioned again into $M$ cells.  This process is repeated $T$ times.  The advantage of this approach is that in obtaining $M^T$ partitions, we need to run our partitioning algorithm $T$ times and generate $M$ partitions at each stage.  In traditional VQ, the partitioning algorithm would run once but have to create $M^T$ partitions.  For the binary case (two code-vectors per stage, $M=2$) and a total of 8 stages ($T$=8), RVQ only requires 16 searches.  In $ESVQ$, this would require 256.  The exponential complexity is reduced to linear complexity.  Moreover, even the distortion of an ESVQ can be attained.  In general, all structurally constrained quantizers cannot provide performance as good as ESVQ.  However, since they are able to more efficiently implement codes, larger and larger vector sizes can be used, and if carefully designed, can achieve better performance that ESVQ \cite{1996_JNL_AdvancesRVQ_Barnes}.

\begin{figure}[ht]
	\centering	
	\subfigure[]
	{
		\includegraphics[width=0.65\textwidth]{thesis/RVQ_graphicalReconstruction.pdf}
		\label{fig:MTT_codebooks}	
	}
	\subfigure[]
	{
		\includegraphics[width=0.65\textwidth]{thesis/RVQ_trellis.pdf}
		\label{fig:MTT_reconstruction}	
	}
	\caption{RVQ $\sigma$-tree, $T$=3, $S=2$, two alternate representations} 
	\label{fig:RVQ_sigma_tree}				
\end{figure}



\subsubsection{Encoder}
%----------------------------
A vector quantizer with a direct sum codebook,

\begin{itemize}
\item is the source coding dual of a $\sigma$-tree structure \cite{1993_sigmaTrees_Barnes}, and
\item its design is the covering design problem
\end{itemize}

Since the dual of the covering problem is the packing problem \cite{BOOK_spheres_Conway}, it is shown in \cite{1993_sigmaTrees_Barnes} that $\sigma$-tree structures exist that provide good packing.  The design of RVQ direct sum codebooks is therefore studied in the context of $\sigma$-trees.

The set $\mathcal{S}$ of $K$ leaf nodes of a $T$-level $M$-ary $\sigma$-tree is the set of $M^T$ direct sums of $T$ vectors in $\mathbb{R}^D$, one vector per level $t$ being picked from a \emph{constituent set} $G_t = \{m_{t,m} \in \mathbb{R}^D \ | m=1, 2, \ldots M\}$ \cite{2002_JNL_SigmaTrees_Barnes},

\begin{equation}
\mathcal{S} = \{\hat{\mu}^{(k)} \in \mathbb{R}^D \ | \ \hat{\mu}_k = \sum\limits_{t=1}^T m_t^{(k)}, k=1, 2, \ldots, K=M^T\}
\end{equation}

An example 3-level binary $\sigma$ tree is given in Figure~\ref{fig:RVQ_sigma_tree}.  The RVQ encoder can be designed in two major phases which are explained below and illustrated in Figure~\ref{fig:RVQ_encoder_flowDiagram}. 
 


 

\begin{figure}[ht]
\centering
\includegraphics[height=0.6\textheight]{thesis/RVQ_encoder_flowDiagram.pdf}
\caption{Encoder flow diagram, generating codebooks $\Phi$}
\label{fig:RVQ_encoder_flowDiagram}
\end{figure}


\begin{enumerate}
\item \underline{Generating the $\sigma$ tree}.  The first design step is to generate the RVQ $\sigma$-tree by repeated application of the K-means algorithm to every stage in the following manner:  
\begin{enumerate}
\item \underline{First stage}. As in the K-means algorithm, standard practice is to generate several arbitrary partitionings and to retain the partition that results in least error as given by Equation~\ref{Eq:computingClusterCentroids}.  The objective function for the discrete case is,

\begin{equation}
\label{Eq:KmeansError}
e = \KmeansError
\end{equation}

The means of this partitioned data constitute the first stage codevectors.  In effect, this step generates $S$ stage codevectors, where typically, $S << K$.  For instance, if $K=16$ clusters are desired in ESVQ, RVQ can attain that many clusters with say $T=4$ stages and $S=2$ codevectors per stage.  Notice that in this equation, it is implicit that the partitions are known.  The outer summation that sums over the $K$ clusters is what makes this an NP hard problem, otherwise this would be a standard least squares problem.  Once the partitions are fixed, this can be solved as a standard least squares problem by setting the derivatives equal to 0.  

\begin{align}
\frac{\partial{e}}{\partial{\mu_k}} &= 0\notag\\
\KmeansInnerSum 2 \KmeansInner &= 0\notag\\
\KmeansInnerSum x_i - \KmeansSum \mu_k &= 0\notag\\
\KmeansInnerSum x_i - N_k \mu_k &= 0\notag\\
\mu_k &= \frac{\KmeansInnerSum x_i}{N_k}
\label{Eq:computingClusterCentroids}
\end{align}

In the equation above, $\mu_k$ is the cluster centroid for the $k$-th partition $\mathcal{K}_k$.  Once the $K$ cluster centroids have been computed, the data is repartitioned.  In other words, data points are mapped to the nearest new cluster centroids.  The cluster centroids are recomputed using Equation~\ref{Eq:computingClusterCentroids}.  This process of partitioning followed by centroid computation followed by repartitioning followed by centroid computation is repeated till the centroids converge.

\item \underline{Generating residual stages}.  In the step above, stage codevectors are subtracted from the data points that map to them.  This step generates a new set of data points, the first stage \emph{residual} data points.  The data at the output of stage $t$ is called the $t$-th residual data.  This causes each cluster in the first stage to be centered around the origin and also causes each data point to move closer to the origin.  These residual data points are now input into the K-means algorithm which generates a new set of partitions and new cluster centroids.  Using the same procedure discussed earlier, second stage residuals are generated and input to the third stage.  This process is repeated till the desired number of stages.  This Markovian style design process generates the initial RVQ $\sigma$-tree trellis.  
\end{enumerate}
\item \underline{Applying the CAC condition}.  The above step generates cluster centroids that are locally optimal at every stage.  Morever, the design of each stage depends on the previous or causal stage designs but does not depend on subsequent or anti-casual stage designs.   This can lead to a propagation of reconstruction error.  No optimality claims can be made regarding this design process.  An attempt to address this is a joint design strategy, also known as the Causal Anti-causal (CAC) condition.  This is explained next.
\end{enumerate}

\begin{figure}
\center
\includegraphics[width=0.5\textwidth]{thesis/RVQ_CAC_toyExample2_2x2.pdf}
\caption{2x2 RVQ example}
\label{fig:Figure1}
\end{figure}


To understand CAC, it is beneficial to first introduce some notation:

\begin{itemize}
\item \emph{Path}.  A finite directed path through the RVQ $\sigma$-tree has as its vertices one stage codevector from every stage.  For $T$ stages and $S$ codevectors per stage, there are a total of $K = S^T$ possible paths.
\item \emph{Stage codevector}.  A stage codevector $m_{t, s}$ is the $s$-th codevector at the $t$-th stage.  An alternate notation used is $m^{(k)}_t$ which denotes the stage codevector at the $t$-th stage for the $k$-th path.
\item \emph{Optimal reconstruction}.  There are $K=S^T$ possible paths through the RVQ $\sigma$-tree.  Each data point $x_i$ propagates through the $\sigma$-tree on one of these $K$ paths.  If $x_i \mapsto \mathcal{K}_k$, then for optimal reconstruction, the following condition must hold true,

\begin{equation*}
\RVQunit = 0, \ \ \forall x_i \mapsto \mathcal{K}_k, \ \ i =1, 2, \ldots N\\
\end{equation*}

where $m^{(k)}_t$ is the stage codevector at the $t$-th stage of the $k$-th path through the $\sigma$-tree.  The above condition shows that

\begin{itemize} 
\item There are at most $K=S^T$ distinct data points that can result in 0 reconstruction error, irrespective of the dimensionality $D$ of the data. 
\item The non-Markovian reconstruction function produces a Direct Sum Successive Approximation (DSSA) of the input.
\end{itemize}\item \emph{Equivalent codevector}.  For every path through the RVQ $\sigma$-tree, we have an equivalent codevector $\hat{\mu}^{(k)}$ given by,
\begin{equation}
\hat{\mu}^{(k)} = \RVQequivalentCodevector, \ \ \ k=\{1, 2, \ldots, K\}
\end{equation}
In general, the $K$ equivalent codevectors and the accompanying $K$ partitions will approximate the $K$ cluster centroids and $K$ partitions produced by ESVQ.  The equivalent codevector can also be written as,
\begin{equation}
\label{Eq:RVQequivalentCodevectorBroken}
\hat{\mu}^{(k)} = \RVQequivalentCodevectorBroken
\end{equation}
\item \emph{RVQ objective function}.  The ESVQ objective function in Equation~\ref{Eq:KmeansError} can now be re-written in terms of the equivalent codevectors as 
\begin{equation}
e = \RVQerror
\end{equation}
\end{itemize}

Substituting for the equivalent codevector from Equation~\ref{Eq:RVQequivalentCodevectorBroken} gives

\begin{equation}
e=\KmeansSum{\bigg[\RVQmultipleKmeans\bigg]}^2, \ \ \tau=\{1, 2, \ldots T\}
\label{Eq:RVQmultipleKmeans}
\end{equation}

This equation states that for RVQ, the original objective function can be written as a series of coupled K-means equations, one equation per stage.    
 
\begin{align}
e&= \KmeansSum{\bigg[\RVQmultipleKmeansone\bigg]}^2, \ \ \tau=1\notag\\
&= \KmeansSum{\bigg[\RVQmultipleKmeanstwo\bigg]}^2, \ \ \tau=2\notag\\
&\ \ \ \  \ \ \ \vdots\notag\\
&=\KmeansSum{\bigg[\RVQmultipleKmeansT\bigg]}^2, \ \ \tau=T
\end{align}

Equation~\ref{Eq:RVQmultipleKmeans} can now be regrouped as,

\begin{align}
\label{Eq:RVQmeans}
e&= \KmeansSum{\bigg[\RVQmultipleKmeansonealternate\bigg]}^2, \ \ \tau=\{1, 2, \ldots T\}\notag\\
&={\RVQerroralternate}, \ \ \tau=\{1, 2, \ldots T\}
\end{align}

where $g_i$ is the \emph{graft residual} and is formed using the following steps,

\begin{itemize}
\item \emph{List all reconstruction stage codevectors in path}.  For a data point $x_i$, list all stage codevectors that are used to reconstruct it
\item \emph{Find graft residual at stage $\tau$}.  Subtract all stage codevectors in above step from $x_i$ except the stage codevector at the $\tau$-th stage  
 \end{itemize}

Equation~\ref{Eq:RVQmeans} is now quite clearly a K-means objective function for the $\tau$-th stage.  Here, $m_{\tau}^{(k)}$ is a single codevector at the $\tau$-th stage even though it has a superscript $k$ which tends to indicate that it might correspond to several codevectors.  The superscript $k$ refers to the fact that there are $K$ possible paths, but for a particular $g_i$, we choose the $m_{\tau}^{(k)}$ that lies in the reconstruction path of the corresponding $x_i$.  We can therefore rewrite Equation~\ref{Eq:RVQmeans} as,

\begin{equation}
e = \RVQOuterSum\RVQInnerSum\RVQinneralternatealternate
\end{equation} 

Now, to compute a particular stage codevector $m_{\tau, s}$, we use the standard GLA algorithm as in Equation~\ref{Eq:computingClusterCentroids} to get

\begin{equation}
m_{\tau, s} = \frac{\RVQInnerSum g_i}{N_{g_i \mapsto m_{\tau, s}}}
\label{Eq:finalCAC}
\end{equation}

\begin{figure}[ht]
\centering
\includegraphics[height=0.6\textheight]{thesis/RVQ_explorer_flowDiagram.pdf}
\caption{Decoder flow diagram}
\label{fig:RVQ_decoderFlowDiagram}
\end{figure}


Equation~\ref{Eq:finalCAC} can now be used to compute the $S$ stage codevectors at the $\tau$-th stage.  $N_{g_i \mapsto m_{\tau, s}}$ is the number of graft residuals $g_i$ whose corresponding data points $x_i$ contain $m_{\tau, s}$ in their reconstruction path.  Recall that in ordinary K-means, computation of the centroids is followed by a repartitioning which is followed by a centroid computation and so on.  This process is repeated till convergence.  However, in this case of coupled K-means, two issues arise:

\begin{itemize}
\item repartitioning affects all stages.  
\item Moreover, recomputing the centroids for one stage changes the graft residuals for all other stages, since each graft residual depends on $S-1$ stage codevectors.  
\end{itemize}

Since RVQ update strategies are not the focus of this research, the reader is referred to  \cite{1996_JNL_AdvancesRVQ_Barnes} for more details.  The result of these steps is an RVQ codebook with $K$ codevectors,

\begin{equation}
\mathcal{M} = \{\hat{\mu}_1, \hat{\mu}_2, \ldots, \hat{\mu}_K\}
\end{equation}

Figure~\ref{fig:Figure1} shows a simple example of a 2x2 RVQ with 4 input data points.  The error is given by



\begin{equation}
\begin{array}{lllll}
e &=& \KmeansError \\
&=& {(x_1 - a - c)}^2 + {(x_2- a - d)}^2 + {(x_3 - b - c)}^2 + {(x_4 - b - d)}^2\\
&=& {e_1}^2 + {e_2}^2 + {e_3}^2 + {e_4}^2
\end{array}
\end{equation}

Applying the CAC condition to optimize stage 1, we get the following equations by grouping all input data points with stage codevectors that do not belong to stage 1,

\begin{equation}
\begin{array}{lllll}
e &=& {((x_1 - c) - a)}^2 + {((x_2- d) - a)}^2 + {((x_3 - c) - b)}^2 + {((x_4 - d) - b)}^2
\end{array}
\label{Eqn:2x2RVQ_stage1}
\end{equation}

And similarly, to optimize stage 2, we get,

\begin{equation}
\begin{array}{lllll}
e &=& {((x_1 - a) - c)}^2 + {((x_2- a) - d)}^2 + {((x_3 - b) - c)}^2 + {((x_4 - b) - d)}^2
\end{array}
\label{Eqn:2x2RVQ_stage2}
\end{equation}

which leads to the optimal stage codevectors,

\begin{equation}
\begin{array}{lllll}
a = \frac{(x_1 - c) + (x_2 - d)}{2}\\
b = \frac{(x_3 - c) + (x_4 - d)}{2}\\
c = \frac{(x_1 - a) + (x_3 - b)}{2}\\
d = \frac{(x_2 - a) + (x_4 - b)}{2}\\
\end{array}
\end{equation}


\subsubsection{Decoder}
%----------------------------
The decoder has a DSSA (direct sum successive approximation) structure.  In other words, at the first stage, the best first stage codevector in the $L_2$ norm sense is picked.  This codevector is subtracted from the input signal to form a first stage residual signal.  This signal is fed as input to the second stage where again the best second stage codevector in the $L_2$ norm sense is picked.  The residual from this stage is fed as input to the third stage.  This process is repeated for all $T$ stages.  The final residual output from the $T$-th stage is the error signal.  The reconstructed output signal is a sum of the best codevectors at every stage.  This process is shown in detail in Figure~\ref{fig:RVQ_decoderFlowDiagram}.  In this diagram, the reconstructed signal is first initialized to the zero vector.  At every stage, the SNR of the reconstructed signal is computed.  The process of decoding is stopped when either the maximum number of stages have been achieved or if the SNR does not increase at any stage.  In other words, a monotonic SNR increase condition is imposed.  The computational complexity of this decoding process is $O(N)$.  


%====================
\section{Comparison of different types of VQ}
\label{sec:comparison_VQ}
%====================
\begin{table}[htp]
\begin{longtable}{| p{2.2in} || p{1in} | p{1.5in} | p{1.5in}|}
\hline
											&ESVQ						&TSVQ																&RVQ								\\ 
\hline
\# data points								&$N$						&same																&same								\\ 
\hline
Vertical dimension, $T$					&-							&depth																&\# stages							\\
Horizontal dimension, $M$					&-							&breadth															&\# templates						\\
Encoding indeces							&-							&path map	(T-tuple)												&XDR (T-tuple)						\\ 
\hline
Input dimension							&$D$						&same																&same								\\
Rate, $r$ (bits/vector component)			&$(\log_2K)/D$			&same																&same								\\ 
\hline
\# non-terminal codevectors, $N_{ntc}$	&0							&$1+M+M^2+ \ldots + M^{T-1}=\frac{M^T-1}{M-1}$			&$MT$								\\
\# terminal codevectors, $K$				&$2^{rD}$					&$2^{rD}=M^T$													&same as TSVQ					\\
\hline
computations: search ops/stage			&1 stage, $O(2^{rD})$	&$M$																&same as TSVQ					\\
computations: search complexity 			&$O(2^{rD})$				&O($MT$)															&same  as TSVQ					\\ 
memory 									&$2^{rD}$					&$MN_{ntc} = M\frac{M^T-1}{M-1}\approx\frac{M}{M-1}M^T$ &$MT$								\\ 
\hline
\# Input data at stage $t$					&1 stage, $N$				&$\frac{N}{M^t}$													&$N$								\\
\hline
\end{longtable}
\caption{Comparison of ESVQ, TSVQ and RVQ}
\label{tab:comparison_ESVQ_TSVQ_RVQ}
\end{table}




%\mbox{testing method 1} &&& \mbox{maximize log likelihood}\\
%c^*_b&=& \mbox{arg}\max\limits_c & \log p(\mathcal{C}_c | \mathbf{Y})\\
%&=& \mbox{arg}\max\limits_c & \log p(\mathbf{Y} | \mathcal{C}_c)\\
%&=& \mbox{arg}\max\limits_c & \log p(Y_1 = \mathbf{b}_n(1), Y_2 = \mathbf{b}_n(2),  \ldots , Y_T = \mathbf{b}_n(T) | \mathcal{C}_c)\\
%&=& \mbox{arg}\max\limits_c & \sum\limits_{t=1}^{T} \log p \bigg(Y_t | Y_{t-1}, \mathcal{C}_c \bigg) \\ \\ \\ \\
%\mbox{testing method 2} &&& \mbox{maximize  KL divergence (relative entropy)}\\
%c^*_{r,c}&=& \mbox{arg}\max\limits_c & \sum \log \frac{p(\mathbf{X}|\mathcal{C}_c)}{p(\mathbf{Y})}p(\mathbf{X}|\mathcal{C}_c) \\
%
%&=& \mbox{arg}\max\limits_c & \bigg[\log p \bigg(\mathcal{C}_c|X_1=\mathbf{b}_{r,c}(1) \bigg) + \sum\limits_{t=2}^{T} \log p \bigg(\mathcal{C}_c | X_t = \mathbf{b}_{m,n}(t), X_{t-1} = \mathbf{b}_{r,c}(t-1) \bigg) - \\
%
%
%&&& \log p \bigg(Y_1=\mathbf{b}_{r,c}(1) \bigg) + \sum\limits_{t=2}^{T} \log p \bigg(Y_t = \mathbf{b}_{r,c}(t), Y_{t-1} = \mathbf{b}_{r,c}(a-1) \bigg) \bigg]p(X_1|\mathcal{C}_c)p(X_2|X_1, \mathcal{C}_c)p(X_3|X_2, \mathcal{C}_c) \ldots p(X_T|X_{T-1}, \mathcal{C}_c)
%\\ \\ \\ \\
%
%
%
%\mbox{testing method 3} &&& \mbox{maximize mutual information}\\
%p(\mathbf{X};\mathbf{Y}) &=&& H(\mathbf{X}) - H(\mathbf{X|Y})
%\end{array}
%\end{equation}

%$
%\mbox{- for an RVQ with $T$ stages, an SoC is a $T$-tuple containing reconstruction indeces for a block of source symbols} \\
%\mbox{- during training, each reconstruction index takes on values from the alphabet } \Phi_{trg}\\
%\mbox{- during testing, each reconstruction index takes on values from the alphabet } \Phi_{tst}\\
%\mbox{- for stage $t$, i.e. the $t$-th component of the SoC descriptor, the reconstruction index is the stage-codevector index $m$ for that stage as long as $m \leq M$} \\
%\mbox{- during testing, $M+1$ and $M+2$ are used as fillers to make sure that the test SoC has length $T$}
%$







%Another question here concerns the optimality of RVQ.  An RVQ is said to be \emph{jointly optimal} if a local or global minimum value of the average distortion $\mathcal{D}(E,D) = E[m(X_1, D(E(X_1)))]$ is achieved.  Here, $E$ is the encoder, $D$ is the decoder, $m(.,.)$ is a distortion metric, and $E[.]$ is the expectation operator.  The necessary condition for joint decoder optimality is that the code-vectors $y_p(i)$  of the $p$-th stage must satisfy the following condition:
%
%\begin{equation}
%\mathcal{D}=\frac{\partial D}{\partial y_p(i)}=0
%\end{equation}
%
%This condition is satisfied when the stage code-vectors are centroids of residuals formed from the encoding decisions of both \emph{causal} and \emph{anticausal} stages \cite{1995_JNL_OptimalityRVQ_Kossentini}.  On the other hand, if only causal stages are considered, then satisfying the condition above will help achieve \emph{sequential} optimality.  For the encoder case, it is not possible to design optimal stages.  Instead an overall global unconstrained encoder is designed, and then individual encoder stages are designed using nearest neighbor rules that try to match the performance of the global encoder.
%
%A final note is that due to the multiple stages of RVQ, it is possible to design with few code-vectors per stage.  This can be useful if the training data is limited, since this would necessitate the use of small stage-codebook sizes \cite{1996_JNL_AdvancesRVQ_Barnes}.


%Channel coding is sphere packing and rate distortion coding is sphere covering \cite{2006_BOOK_InformationTheory_Cover}. 

\end{Body}
%##############################################################################################################
\begin{EndMatter}
\references 				%generates the bibliography page
\end{EndMatter}
\end{document}
%##############################################################################################################

