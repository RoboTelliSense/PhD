\include{begin_article}
\title{Incremental SVD with Mean Update}
\author{Salman Aslam\\ Georgia Institute of Technology}
\begin{document}
\maketitle
\rule[0pt]{\textwidth}{1pt}
\tableofcontents
\rule[0pt]{\textwidth}{1pt}
%================================
\section{Introduction}
%================================
Dimensionality reduction is a commonly used method to remove redundancies in data.  In tracking applications, a dimensionality reduction method such as PCA can be used to create an eigenbasis based appearance model.  Using a batch approach in which the eigenbasis is recomputed at every frame is costly in terms of memory and computations.  Therefore, in online applications, it is desired to have an incremental update to the eigenbasis.  This can be performed using an incremental SVD procedure.

We assume that we start with some initial data, such as $N$ vectors in $\mathbb{R}^D$.  Each of these vectors could represent a vectorized training image.  These vectors are arranged in a $D$x$N$ data matrix for which the SVD is computed such that $\mathbf{A}=\mathbf{U}\mathbf{\Lambda}\mathbf{V}^T$.  Now, an additional $M$ vectors are made available in an online framework.  These vectors are placed in a $D$x$M$ data matrix $\mathbf{B}$.  The goal is to compute the SVD of $\mathbf{C} = [\mathbf{A} \ \ \mathbf{B}]$.  The naive approach is to recompute the SVD of this augmented data matrix.  If this naive approach is used whenever new data arrives, it is clear that the computations will grow as more and more data arrives.  Moreover, all data will have to be stored.

We now explain the incremental SVD approach in which the existing SVD is used and all prior data is discarded.  For our example, this would mean that $\mathbf{A}$ could be discarded, and only ${\color{darkgreen}}\mathbf{U}$ and $\mathbf{B}$ would be used to compute the SVD of $\mathbf{C}$.

%================================
\section{Theory}
%================================
The augmented data matrix $\mathbf{C}$ can be written as follows,

\begin{equation}
\begin{array}{llll}
\mathbf{C} &= \left[{\mathbf{A}} \ \ \mathbf{B}\right]\\
&= \left[\mathbf{U}\mathbf{\Lambda}\mathbf{V}^T \ \ \mathbf{U}\mathbf{U}^T\mathbf{B} + {\color{red}\mathbf{\tilde{B}}} {\color{red}\mathbf{\tilde{B}}^T}{\mathbf{B}}\right]\\
&= \left[\mathbf{U} \ \ {\color{red}\mathbf{\tilde{B}}}\right]\left[\begin{array}{llll}\mathbf{\Lambda}\mathbf{V}^T &\mathbf{U}^T\mathbf{B}\\ \mathbf{0} & {\color{red}\mathbf{\tilde{B}}}^T\mathbf{B}\end{array}\right]\\
&= \left[\mathbf{U} \ \ {\color{red}\mathbf{\tilde{B}}}\right]\left[\begin{array}{llll}\mathbf{\Lambda} & \mathbf{U}^T\mathbf{B}\\ \mathbf{0} & {\color{red}\mathbf{\tilde{B}}}^T\mathbf{B}\end{array}\right] \left[\begin{array}{llll}\mathbf{V}^T & \mathbf{0}\\ \mathbf{0} & \mathbf{I}\end{array}\right]\\
&= \left[\mathbf{U} \ \ {\color{red}\mathbf{\tilde{B}}}\right]\mathbf{R} \left[\begin{array}{llll}\mathbf{V}^T & \mathbf{0}\\ \mathbf{0} & \mathbf{I}\end{array}\right]\\
 &= \left[\mathbf{U} \ \ {\color{red}\mathbf{\tilde{B}}}\right]\mathbf{\tilde{U}}\mathbf{\tilde{\Lambda}}\mathbf{\tilde{V}}^T \left[\begin{array}{llll}\mathbf{V}^T & \mathbf{0}\\ \mathbf{0} & \mathbf{I}\end{array}\right]\\
&= \bigg(\left[\mathbf{U} \ \ {\color{red}\mathbf{\tilde{B}}}\right]\mathbf{\tilde{U}}\bigg) \mathbf{\tilde{\Lambda}} \bigg(\mathbf{\tilde{V}}^T \left[\begin{array}{llll}\mathbf{V}^T & \mathbf{0}\\ \mathbf{0} & \mathbf{I}\end{array}\right]\bigg)\\
&=\mathbf{U'}\mathbf{\tilde{\Lambda}}\mathbf{V'}^T
\end{array}
\label{Eqn:short}
\end{equation}

The significance of this decomposition is that the new basis, $\mathbf{U'}$ depends only on the computation of ${\color{red}\mathbf{\tilde{B}}}$ which in turn only depends on the prior basis $\mathbf{U}$ and the new data  $\mathbf{B}$.  It does not depend on the prior data $\mathbf{A}$ which can be discarded.  (There is also a dependence on $\mathbf{R}$ but that too depends on ${\color{red}\mathbf{\tilde{B}}}$).

We now turn to the computation of $ {\color{red}\mathbf{\tilde{B}}}$.  We know from Equation~\ref{Eqn:short} that $\mathbf{B}=\mathbf{U}\mathbf{U}^T\mathbf{B} + {\color{red}\mathbf{\tilde{B}}} {{\color{red}\mathbf{\tilde{B}}}^T}\mathbf{B}$.  Here, $\mathbf{U}\mathbf{U}^T\mathbf{B}$ is a matrix of projections of $\mathbf{B}$ onto the subspace $\mathbf{U}$.  Similarly, ${\color{red}\mathbf{\tilde{B}}} {\color{red}\mathbf{\tilde{B}}^T}\mathbf{B}$ is a matrix of projections of $\mathbf{B}$ onto the subspace ${\color{red}\mathbf{\tilde{B}}}$.  This tells us that the goal of ${\color{red}\mathbf{\tilde{B}}}$ is to capture the variation in the data that is orthogonal to the existing subspace $\mathbf{U}$.  Therefore, ${\color{red}\mathbf{\tilde{B}}}$ has to be computed so that it forms a basis that is orthogonal to $\mathbf{U}$.  Quite simply then, ${\color{red}\mathbf{\tilde{B}}}$ can be computed using a QR decomposition of $\mathbf{B}-\mathbf{U}\mathbf{U}^T\mathbf{B}$. 

Therefore, the computation of our goal, the SVD of $\mathbf{C}$, is a four step process: (a) compute ${\color{red}\mathbf{\tilde{B}}}$, (b) use ${\color{red}\mathbf{\tilde{B}}}$ to compute $\mathbf{R}$ as given in Equation~\ref{Eqn:short}, (c) compute the SVD of $\mathbf{R}$, and (d) use the SVD of $\mathbf{R}$ to to compute the SVD of $\mathbf{C}$ as given in Equation~\ref{Eqn:short}.  

								\begin{figure}[t]
								\centering
								\includegraphics[width=0.8\textwidth]{figs/iPCA_results.pdf}
								\caption{Error in basis between incremental and batch SVD.}
								\label{iPCA_results}
								\end{figure}

Two questions now remain.  What can be said of the covariance matrix and mean of $\left[{\mathbf{A}} \ \ \mathbf{B}\right]$ in relation to this incremental SVD procedure?  We now address these questions.


\subsection{Mean and covariance of incremental SVD}
%--------------------
The mean $\mathbf{\mu}_\mathbf{C}$ and covariance matrix $\mathbf{\Sigma}_C$ of input matrix $\mathbf{C}$ is given by,

\begin{equation}
\begin{array}{llll}
\mathbf{\mu}_\mathbf{C} &= \frac{N}{N+M}\mathbf{\mu}_\mathbf{A} + \frac{M}{N+M}\mathbf{\mu}_\mathbf{B}\\\\
\mathbf{\Sigma}_C &= \mathbb{E}[(\mathbf{C}-\mathbf{\mu}_\mathbf{C})(\mathbf{C}-\mu_\mathbf{C})^T] & \textrm{\scriptsize (subtract $\mathbf{\mu}_\mathbf{C}$ from every column of \textbf{C})}\\
&=\frac{1}{N+M}\sum\limits_{i=1}^{N+M} (\mathbf{c}_i-\mathbf{\mu}_\mathbf{C})(\mathbf{c}_i-\mu_\mathbf{C})^T & \textrm{\scriptsize ($\mathbf{c}_i$ is the $i$-th column of matrix $\mathbf{C}$)}\\
\end{array}
\end{equation}

Manipulating the mean equation, we can write
\begin{equation}
\begin{array}{llll}
\mathbf{\mu}_\mathbf{A} -\mathbf{\mu}_\mathbf{C} &= \mathbf{\mu}_\mathbf{A} - \frac{N}{N+M}\mathbf{\mu}_\mathbf{A} - \frac{M}{N+M}\mathbf{\mu}_\mathbf{B} \ \ \scriptsize{\textrm{(subtracting from $\mathbf{\mu}_\mathbf{A}$ on both sides)}}\\\\

&=\frac{M}{N+M}(\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{B})\\\\

\mathbf{\mu}_\mathbf{B} -\mathbf{\mu}_\mathbf{C} &= \mathbf{\mu}_\mathbf{B} - \frac{N}{N+M}\mathbf{\mu}_\mathbf{A} - \frac{M}{N+M}\mathbf{\mu}_\mathbf{B} \ \ \scriptsize{\textrm{(subtracting from $\mathbf{\mu}_\mathbf{B}$ on both sides)}}\\\\

&=\frac{-N}{N+M}(\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{B})
\end{array}
\end{equation}


Manipulating the covariance equation, we can write the \emph{scatter matrix} $\mathbf{S}_C$ of input matrix $\mathbf{C}$ as,

\begin{changemargin}{-0.5in}{0in}

\begin{equation}
\begin{array}{llll}
\mathbf{S}_C &= (N+M) * \mathbf{\Sigma}_C \\
&= \sum\limits_{i=1}^{N+M} (\mathbf{c}_i-\mathbf{\mu}_\mathbf{C})(\mathbf{c}_i-\mu_\mathbf{C})^T \\
&= \sum\limits_{i=1}^{N} (\mathbf{c}_i-\mathbf{\mu}_\mathbf{C})(\mathbf{c}_i-\mu_\mathbf{C})^T &+ \sum\limits_{i=N+1}^{N+M} (\mathbf{c}_i-\mathbf{\mu}_\mathbf{C})(\mathbf{c}_i-\mu_\mathbf{C})^T\\
&= \sum\limits_{i=1}^{N}
 (\mathbf{c}_i-\mathbf{\mu}_\mathbf{A} +\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{C}) (\mathbf{c}_i-\mathbf{\mu}_\mathbf{A} +\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{C})^T &+  \sum\limits_{i=N+1}^{N+M}
 (\mathbf{c}_i-\mathbf{\mu}_\mathbf{B} +\mathbf{\mu}_\mathbf{B} - \mathbf{\mu}_\mathbf{C}) (\mathbf{c}_i-\mathbf{\mu}_\mathbf{B} +\mathbf{\mu}_\mathbf{B} - \mathbf{\mu}_\mathbf{C})^T \\
&=\mathbf{S}_A + N(\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{C})(\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{C})^T &+ \mathbf{S}_B +  M(\mathbf{\mu}_\mathbf{B} - \mathbf{\mu}_\mathbf{C})(\mathbf{\mu}_\mathbf{B} - \mathbf{\mu}_\mathbf{C})^T\\\\

&=\mathbf{S}_A + N\frac{M^2}{(N+M)^2}(\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{B})(\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{B})^T  &+\mathbf{S}_B +  M\frac{N^2}{(N+M)^2}(\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{B})(\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{B})^T\\\\

&=\mathbf{S}_A + \mathbf{S}_B + \frac{NM}{N+M}(\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{B})(\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{B})^T  
\end{array}
\end{equation}
\end{changemargin}


We therefore see that the scatter matrix $\mathbf{S}_C$ of the output is the sum of the input scatter matrices $\mathbf{S}_A$, $\mathbf{S}_B$ and an extra term, the scaled outer product of the input mean difference $\mathbf{\mu}_\mathbf{A} - \mathbf{\mu}_\mathbf{B}$.  To account for this term, we adjust $\mathbf{B}$ as follows before using it to compute  ${\color{red}\mathbf{\tilde{B}}}$,

\begin{equation}
\begin{array}{lll}
\
\end{array}
\end{equation}

%\begin{enumerate}
%\item \underline{Subtract mean from $\mathbf{B}$}.  Compute mean removed $\mathbf{B}_{mr} = \mathbf{B} - \mu_\mathbf{B}$, where $\mu_\mathbf{B}$ is the mean of $\mathbf{B}$.
%\item \underline{Compute weight $w$}.  Compute weight $w=(N*M)/(N+M)$.
%\item \underline{Compute weighted mean difference}.  $\mathrm{wmd}_{A\_B} = w(\mu_\mathbf{A}-\mu_\mathbf{B})$.
%\item \underline{Augment $\mathbf{B}$ with weighted mean difference}. $\hat{\mathbf{B}}=\left[\mathbf{B} \ \ \mathrm{wmd}_{A\_B}\right]$
%\item \underline{Find component of $\mathbf{B}$ orthogonal to $\color{darkgreen}\mathbf{U}$}.
%\end{enumerate}

For clarity, we explicitly write out the dimensions of all matrices in Equation~\ref{Eqn:short}

\scriptsize
\begin{equation}
\begin{array}{llll}
\mathbf{C}_{D\mathrm{x}(N+M)} &= \left[{\mathbf{A}_{D\mathrm{x}N}} \ \ \mathbf{B}_{D\mathrm{x}M}\right]\\
&= \left[\mathbf{U}_{D\mathrm{x}N}\mathbf{\Lambda}_{N\mathrm{x}N}\mathbf{V}_{N\mathrm{x}N}^T \ \ \mathbf{U}_{D\mathrm{x}N}\mathbf{U}_{D\mathrm{x}N}^T\mathbf{B}_{D\mathrm{x}M} + {\color{red}\mathbf{\tilde{B}}_{D\mathrm{x}(M+1)}} {{\color{red}\mathbf{\tilde{B}}_{D\mathrm{x}(M+1)}}^T}\mathbf{B}_{D\mathrm{x}M}\right]_{D\mathrm{x}(N+M)}\\
&= \left[\mathbf{U}_{D\mathrm{x}N} \ \ {\color{red}\mathbf{\tilde{B}}_{D\mathrm{x}(M+1)}}\right]_{D\mathrm{x}(N+M+1)}\left[\begin{array}{llll}\mathbf{\Lambda}_{N\mathrm{x}N}\mathbf{V}_{N\mathrm{x}N}^T &\mathbf{U}_{D\mathrm{x}N}^T\mathbf{B}_{D\mathrm{x}M}\\ \mathbf{0}_{(M+1)\mathrm{x}N} & {\color{red}\mathbf{\tilde{B}}_{D\mathrm{x}(M+1)}}^T\mathbf{B}_{D\mathrm{x}M}\end{array}\right]_{(N+M+1)\mathrm{x}(N+M)}\\
&= \left[\mathbf{U}_{D\mathrm{x}N} \ \ {\color{red}\mathbf{\tilde{B}}_{D\mathrm{x}(M+1)}}\right]_{D\mathrm{x}(N+M+1)}\left[\begin{array}{llll}\mathbf{\Lambda}_{N\mathrm{x}N} & \mathbf{U}_{D\mathrm{x}N}^T\mathbf{B}_{D\mathrm{x}M}\\ \mathbf{0}_{(M+1)\mathrm{x}N} & {\color{red}\mathbf{\tilde{B}}_{D\mathrm{x}(M+1)}}^T\mathbf{B}_{D\mathrm{x}M}\end{array}\right]_{(N+M+1)\mathrm{x}(N+M)} \left[\begin{array}{llll}\mathbf{V}_{N\mathrm{x}N}^T & \mathbf{0}_{M\mathrm{x}M}\\ \mathbf{0}_{M\mathrm{x}M} & \mathbf{I}_{M\mathrm{x}M}\end{array}\right]_{(N+M)\mathrm{x}(N+M)}\\
&= \left[\mathbf{U}_{D\mathrm{x}N} \ \ {\color{red}\mathbf{\tilde{B}}_{D\mathrm{x}(M+1)}}\right]_{D\mathrm{x}(N+M+1)}\mathbf{R}_{(N+M+1)\mathrm{x}(N+M)} \left[\begin{array}{llll}\mathbf{V}_{N\mathrm{x}N}^T & \mathbf{0}_{M\mathrm{x}M}\\ \mathbf{0}_{M\mathrm{x}M} & \mathbf{I}_{M\mathrm{x}M}\end{array}\right]_{(N+M)\mathrm{x}(N+M)}
\end{array}
\label{Eqn:long}
\end{equation}
\normalsize


\cite{2008_JNL_subspaceTRK_Ross}

%================================
\section{Experiments}
%================================
In order to show that incremental SVD produces results close to batch SVD, we run two experiments.

In the first experiment, we create matrix $\mathbf{A}$ as 400 observations of a gaussian random variable with unit variance in $\mathbb{R}^{10000}$.  Matrix $\mathbf{B}$ is created as 300 observations of a gaussian random variable with unit variance in $\mathbb{R}^{10000}$.  

In the second experiment, we create matrix $\mathbf{A}$ as 10 observations of a gaussian random variable with unit variance in $\mathbb{R}^{1089}$ to simulate a 33x33 image snippet.  Matrix $\mathbf{B}$ is created as 5 observations of a gaussian random variable with unit variance in $\mathbb{R}^{1089}$. 



%================================
\section{Results}
%================================
In the first experiment, the rmse of the error in the basis is 0.0143.

In the second experiment, the rmse of the error in the basis is 0.0378.  A plot of the error values for every point in the basis is given in Figure~\ref{iPCA_results}.




%================================
\section{Conclusions}
%================================
In this report, we have shown that the incremental SVD method produces results very close to batch SVD.


\appendix
\section{Source code}
\scriptsize
\lstinputlisting[language=Matlab, caption={demo\_iPCA.m.}, 	label=lst:demo_iPCA]	{demo_iPCA.m}
\lstinputlisting[language=Matlab, caption={sklm2.m.}, 			label=lst:sklm2]		{sklm2.m}

\normalsize
\bibliographystyle{ieee}
\bibliography{MyCitations}
\end{document}