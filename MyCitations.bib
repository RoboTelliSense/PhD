% This file was created with JabRef 2.7b2.
% Encoding: Cp1252

@ARTICLE{2004_JNL_PedDet_Agarwal,
  author = {Agarwal, S. and Awan, A. and Roth, D.},
  title = {Learning to detect objects in images via a sparse, part-based representation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2004},
  volume = {26},
  pages = {1475 -1490},
  number = {11},
  month = nov.,
  abstract = {We study the problem of detecting objects in still, gray-scale images.
	Our primary focus is the development of a learning-based approach
	to the problem that makes use of a sparse, part-based representation.
	A vocabulary of distinctive object parts is automatically constructed
	from a set of sample images of the object class of interest; images
	are then represented using parts from this vocabulary, together with
	spatial relations observed among the parts. Based on this representation,
	a learning algorithm is used to automatically learn to detect instances
	of the object class in new images. The approach can be applied to
	any object with distinguishable parts in a relatively fixed spatial
	configuration; it is evaluated here on difficult sets of real-world
	images containing side views of cars, and is seen to successfully
	detect objects in varying conditions amidst background clutter and
	mild occlusion. In evaluating object detection approaches, several
	important methodological issues arise that have not been satisfactorily
	addressed in the previous work. A secondary focus of this paper is
	to highlight these issues, and to develop rigorous evaluation standards
	for the object detection problem. A critical evaluation of our approach
	under the proposed standards is presented.},
  comment = {TRK_PED},
  doi = {10.1109/TPAMI.2004.108},
  file = {:papers\\2004 JNL, Learning to detect objects in images via a sparse, part-based representation (Agarwal).pdf:PDF},
  issn = {0162-8828},
  keywords = {background clutter;cars;distinctive object parts;gray scale images;image
	representation;image sampling;learning algorithm;learning based method;mild
	occlusion;object detection;part based representation;real world images;rigorous
	evaluation standards;sparse representation;still images;automobiles;image
	representation;image sampling;learning (artificial intelligence);object
	detection;Algorithms;Artificial Intelligence;Cluster Analysis;Computer
	Graphics;Computer Simulation;Image Enhancement;Image Interpretation,
	Computer-Assisted;Information Storage and Retrieval;Models, Statistical;Numerical
	Analysis, Computer-Assisted;Pattern Recognition, Automated;Reproducibility
	of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Subtraction
	Technique;User-Computer Interface;},
  timestamp = {00,380}
}

@ARTICLE{1999_JNL_SurveyMotion_Aggarwal,
  author = {Aggarwal, J. K. and Cai, Q.},
  title = {Human motion analysis: A review},
  journal = {Computer Vision and Image Understanding},
  year = {1999},
  volume = {73},
  pages = {428-440},
  abstract = {Human motion analysis is receiving increasing attention from computer
	vision researchers. This interest is motivated by a wide spectrum
	of applications, such as athletic performance analysis, surveillance,
	man-machine interfaces, content-based image storage and retrieval,
	and video conferencing. This paper gives an overview of the various
	tasks involved in motion analysis of the human body. We focus on
	three major areas related to interpreting human motion: (1) motion
	analysis involving human body parts, (2) tracking a moving human
	from a single view or multiple camera perspectives, and (3) recognizing
	human activities from image sequences. Motion analysis of human body
	parts involves the low-level segmentation of the human body into
	segments connected by joints and recovers the 3D structure of the
	human body using its 2D projections over a sequence of images. Tracking
	human motion from a single view or multiple perspectives focuses
	on higher-level processing, in which moving humans are tracked without
	identifying their body parts. After successfully matching the moving
	human image from one frame to another in an image sequence, understanding
	the human movements or activities comes naturally, which leads to
	our discussion of recognizing human activities.},
  comment = {survey},
  file = {:papers\\1999 JNL, Human motion analysis_ A review (SURVEY, Aggarwal, Qai, CVIU, 915).pdf:PDF},
  keywords = {Computer vision Image segmentation Image understanding Pattern matching},
  timestamp = {01,000}
}

@INPROCEEDINGS{2004_CNF_SURVEYaction_Aggarwal,
  author = {Aggarwal, J. K. and Sangho, Park},
  title = {Human motion: modeling and recognition of actions and interactions},
  booktitle = {Proceedings. 2nd International Symposium on 3D Data Processing, Visualization,
	and Transmission, 6-9 Sept.},
  year = {2004},
  abstract = {Processing of image sequences has progressed from simple structure
	from motion paradigm to the recognition of actions/interactions as
	events. Understanding human activities in video has many potential
	applications including automated surveillance, video archival/retrieval,
	medical diagnosis, sports analysis, and human-computer interaction.
	Understanding human activities involves various steps of low-level
	vision processing such as segmentation, tracking, pose recovery,
	and trajectory estimation as well as high-level processing tasks
	such as body modeling and representation of action. While low-level
	processing has been actively studied, high-level processing is just
	beginning to receive attention. This is partly because high-level
	processing depends on the results of low-level processing. However,
	high-level processing also requires some independent and additional
	approaches and methodologies. In this paper, we focus on the following
	aspects of high-level processing: (1) human body modeling, (2) level
	of detail needed to understand human actions, (3) approaches to human
	action recognition, and (4) high-level recognition schemes with domain
	knowledge. The review is illustrated by examples of each of the areas
	discussed, including recent developments in our work on understanding
	human activities},
  comment = {survey},
  file = {:papers\\2004 CNF, Human motion_ modeling and recognition of actions and interactions (SURVEY, Aggarwal, Park, DPVT, 54).pdf:PDF},
  keywords = {image motion analysis image recognition image sequences solid modelling},
  owner = {50},
  timestamp = {00,300}
}

@INPROCEEDINGS{2008_CNF_SurveyHumanActivityRecognition_Ahad,
  author = {Ahad, Md Atiqur Rahman and Tan, J. K. and Kim, H. S. and Ishikawa,
	S.},
  title = {Human activity recognition: Various paradigms},
  booktitle = {International Conference on Control, Automation and Systems, ICCAS
	2008},
  year = {2008},
  abstract = {Action and activity representation and recognition are very demanding
	research area in computer vision and man-machine interaction. Though
	plenty of researches have been done in this arena, the field is still
	immature. Over the last decades, extensive research methodologies
	have been developed on human activity analysis and recognition for
	various applications. This paper overviews various recent methods
	for human activity recognition with analysis. We attempt to sum up
	the various methods related to human motion representation and recognition.
	We make an effort to categorize the recent methods from the best
	in the business, and finally figure out the short-comings and challenges
	to dig out in future to develop robust action recognition approaches.
	This work exclusively endeavors to encompass the researches related
	only to human action recognition mainly from 2001 till-to-date with
	critical assessment of the methods. We also present our work along
	with to solve some of the shortcomings. It will widely benefit the
	researchers to understand and compare the related advancements in
	this area.},
  comment = {survey},
  file = {:papers\\2008 CNF, Human activity recognition_ Various paradigms (SURVEY, Ishikawa, ICCAS, 8, not recommended).pdf:PDF},
  keywords = {Human computer interaction Computer vision Gesture recognition Hidden
	Markov models Image processing},
  owner = {salman},
  review = {salman: not recommended paper},
  timestamp = {-}
}

@INPROCEEDINGS{2001_CNF_DetectionRecognitionEvents_Ali,
  author = {Ali, A. and Aggarwal, J.K.},
  title = {Segmentation and recognition of continuous human activity},
  booktitle = {Detection and Recognition of Events in Video, 2001. Proceedings.
	IEEE Workshop on},
  year = {2001},
  pages = {28 -35},
  abstract = {This paper presents a methodology for automatic segmentation and recognition
	of continuous human activity. We segment a continuous human activity
	into separate actions and correctly identify each action. The camera
	views the subject from the lateral view: there are no distinct breaks
	or pauses between the execution of different actions. We have no
	prior knowledge about the commencement or termination of each action.
	We compute the angles subtended by three major components of the
	body with the vertical axis, namely the torso, the upper component
	of the leg and the lower component of the leg. Using these three
	angles as a feature vector we classify frames into breakpoint and
	non-breakpoint frames. Breakpoints indicate an action's commencement
	or termination. We use single action sequences for the training data
	set. The test sequences, on the other hand are continuous sequences
	of human activity that consist of three or more actions in succession.
	The system has been tested on continuous activity sequences containing
	actions such as walking, sitting down, standing up, bending, getting
	up, squatting and rising. It detects the breakpoints and classifies
	the actions between them},
  comment = {recog_action},
  doi = {10.1109/EVENT.2001.938863},
  file = {:papers\\2001 CNF, Segmentation and recognition of continuous human activity (Ali, Aggarwal, WDREV, 83).pdf:PDF},
  keywords = {automatic recognition;automatic segmentation;breakpoint frames;camera;continuous
	human activity recognition;continuous human activity segmentation;feature
	vector;frame classification;nonbreakpoint frames;single action sequences;test
	sequences;torso;training data set;feature extraction;image classification;image
	recognition;image segmentation;image sequences;},
  timestamp = {00,100}
}

@ARTICLE{2010_JNL_ActionReconKinematic_Ali,
  author = {Ali, Saad and Shah, Mubarak},
  title = {Human Action Recognition in Videos Using Kinematic Features and Multiple
	Instance Learning},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2010},
  volume = {32},
  pages = {288-303},
  number = {2},
  abstract = {We propose a set of kinematic features that are derived from the optical
	flow for human action recognition in videos. The set of kinematic
	features includes divergence, vorticity, symmetric and antisymmetric
	flow fields, second and third principal invariants of flow gradient
	and rate of strain tensor, and third principal invariant of rate
	of rotation tensor. Each kinematic feature, when computed from the
	optical flow of a sequence of images, gives rise to a spatiotemporal
	pattern. It is then assumed that the representative dynamics of the
	optical flow are captured by these spatiotemporal patterns in the
	form of dominant kinematic trends or kinematic modes. These kinematic
	modes are computed by performing Principal Component Analysis (PCA)
	on the spatiotemporal volumes of the kinematic features. For classification,
	we propose the use of multiple instance learning (MIL) in which each
	action video is represented by a bag of kinematic modes. Each video
	is then embedded into a kinematic-mode-based feature space and the
	coordinates of the video in that space are used for classification
	using the nearest neighbor algorithm. The qualitative and quantitative
	results are reported on the benchmark data sets.},
  comment = {recog_action_opticalFlow},
  file = {:papers\\2010 JNL, Human Action Recognition in Videos Using Kinematic Features and Multiple Instance Learning (Saad Ali, Mubarak Shah, PAMI).pdf:PDF},
  keywords = {Action recognition Feature representation Motion Video analysis kinematic
	features. principal component analysis},
  timestamp = {-}
}

@INPROCEEDINGS{2008_CNF_FloorFieldsCrowdTracking_Ali,
  author = {Ali, S. and Shah, M.},
  title = {Floor fields for tracking in high density crowd scenes},
  booktitle = {Computer Vision. Proceedings 10th European Conference on Computer
	Vision, ECCV 2008},
  year = {2008},
  volume = {pt.2},
  pages = {1 - 14},
  abstract = {This paper presents an algorithm for tracking individual targets in
	high density crowd scenes containing hundreds of people. Tracking
	in such a scene is extremely challenging due to the small number
	of pixels on the target, appearance ambiguity resulting from the
	dense packing, and severe inter-object occlusions. The novel tracking
	algorithm, which is outlined in this paper, will overcome these challenges
	using a scene structure based force model. In this force model an
	individual, when moving in a particular scene, is subjected to global
	and local forces that are functions of the layout of that scene and
	the locomotive behavior of other individuals in the scene. The key
	ingredients of the force model are three floor fields, which are
	inspired by the research in the field of evacuation dynamics, namely
	static floor field (SFF), Dynamic Floor- Field (DFF), and Boundary
	Floor Field (BFF). These fields determine the probability of move
	from one location to another by converting the long-range forces
	into local ones. The SFF specifies regions of the scene which are
	attractive in nature (e.g. an exit location). The DFF specifies the
	immediate behavior of the crowd in the vicinity of the individual
	being tracked. The BFF specifies influences exhibited by the barriers
	in the scene (e.g. walls, no-go areas). By combining cues from all
	three fields with the available appearance information, we track
	individual targets in high density crowds.},
  comment = {trk_people},
  copyright = {Copyright 2008, The Institution of Engineering and Technology},
  file = {:papers\\2008 CNF, Floor fields for tracking in high density crowd scenes (Ali, Mubarakshah, ECCV, 21).pdf:PDF},
  keywords = {image motion analysis;image resolution;target tracking;},
  language = {English},
  timestamp = {00,021}
}

@INPROCEEDINGS{2005_CNF_ActionsSketch_MubarakShah,
  author = {Alper, Yilmaz and Mubarak, Shah},
  title = {Actions sketch: a novel action representation},
  booktitle = {Computer Vision and Pattern Recognition, IEEE Computer Society Conference
	on},
  year = {2005},
  abstract = {In this paper, we propose to model an action based on both the shape
	and the motion of the performing object. When the object performs
	an action in 3D, the points on the outer boundary of the object are
	projected as 2D (x, y) contour in the image plane. A sequence of
	such 2D contours with respect to time generates a spatiotemporal
	volume (STV) in (x, y, t), which can be treated as 3D object in the
	(x, y, t) space. We analyze STV by using the differential geometric
	surface properties to identify action descriptors capturing both
	spatial and temporal properties. A set of action descriptors is called
	an action sketch. The first step in our approach is to generate STV
	by solving the point correspondence problem between consecutive frames.
	The correspondences are determined using a two-step graph theoretical
	approach. After the STV is generated, actions descriptors are computed
	by analyzing the differential geometric properties of STV. Finally,
	using these descriptors, we perform action recognition, which is
	also formulated as graph theoretical problem. Several experimental
	results are presented to demonstrate our approach.},
  comment = {recog_action_volume},
  file = {:papers\\2005 CNF, Actions sketch_ a novel action representation (Mubarak Shah, CVPR, 102).pdf:PDF},
  keywords = {computational geometry graph theory image sequences motion estimation
	object recognition 2D contour 3D object action descriptor action
	recognition action representation action sketch differential geometric
	surface property point correspondence problem spatiotemporal volume},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{2002_JNL_TRKoccl_Altunbasak,
  author = {Altunbasak, Y. and Tekalp, A.M.},
  title = {Occlusion-adaptive, content-based mesh design and forward tracking},
  journal = {Image Processing, IEEE Transactions on},
  year = {2002},
  volume = {6},
  pages = {1270--1280},
  number = {9},
  comment = {TRKoccl},
  file = {:C\:\\salman\\work\\writing\\papers\\2002 JNL, Occlusion-adaptive, content-based mesh design and forward tracking (Altunbasak).pdf:PDF},
  issn = {1057-7149},
  publisher = {IEEE},
  timestamp = {00,070}
}

@ARTICLE{2002_JNL_MPEG2traffic_Ansari,
  author = {Ansari, N. and Hai Liu and Shi, Y.Q. and Hong Zhao},
  title = {On modeling MPEG video traffics},
  journal = {Broadcasting, IEEE Transactions on},
  year = {2002},
  volume = {48},
  pages = { 337-347},
  number = {4},
  month = {Dec},
  abstract = { This paper traces the development/evolution of three of our previously
	proposed MPEG coded video traffic models, that can capture the statistical
	properties of MPEG video data. The basic ideas behind these models
	are to decompose an MPEG compressed video sequence into several parts
	according to motion/scene complexity or data structure. Each part
	is described by a self-similar process. These different self-similar
	processes are then combined to form the respective models. In addition,
	Beta distribution is used to characterize the marginal cumulative
	distribution (CDF) of the self-similar processes. Comparison among
	the three models shows that the latest model (called the simple IPB
	composite model) is the most practical one in terms of accuracy and
	complexity. Simulations based on many real MPEG compressed movie
	sequences, including StarWars, have demonstrated that the simple
	model can capture the autocorrelation function (ACF) and the marginal
	CDF very closely.},
  doi = {10.1109/TBC.2002.806794},
  issn = {0018-9316},
  keywords = { image motion analysis, image sequences, telecommunication traffic,
	video coding, visual communication ACF, Beta distribution, MPEG compressed
	movie sequences, MPEG compressed video sequence, MPEG video traffic,
	StarWars, autocorrelation function, data structure, marginal cumulative
	distribution, motion pictures, motion/scene complexity, self-similar
	process, simple IPB composite model, statistical properties}
}

@TECHREPORT{2009_TECH_VideoStabilization_Arici,
  author = {Arici, Tarik and Aslam, Salman},
  title = {Video Stabilization},
  institution = {Georgia Institute of Technology},
  year = {2009},
  comment = {salman_aslam_rep_3},
  file = {:papers\\2009 TECH, Video Stabilization (Aslam).pdf:PDF},
  timestamp = {-}
}

@PHDTHESIS{2010_THE_TargetLocalizationSegmentation_OmarArif,
  author = {Omar Arif},
  title = {Robust Target Localization And Segmentation Using Statistical Methods},
  school = {Georgia Institute of Technology},
  year = {2010},
  file = {:papers\\2010 THE, Robust Target Localization And Segmentation Using Statistical Methods (Omar Arif).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{2002_JNL_PF_Arulampalam,
  author = {Arulampalam, Sanjeev and Maskell, Simon and Gordon, Neil and Clapp,
	Tim},
  title = {A Tutorial on Particle Filters for On-line Non-linear/Non-Gaussian
	Bayesian Tracking},
  journal = {IEEE Transactions on Signal Processing},
  year = {2002},
  volume = {50},
  pages = {174-188},
  abstract = {Increasingly for many application areas it is becoming important to
	include elements of non-linearity and non-Gaussianity, in order to
	model accurately the underlying dynamics of a physical system. Moreover,
	it is typically crucial to process data on-line as it arrives, both
	from the point of view of storage costs and also for rapid adaptation
	to changing signal characteristics. In this paper we review both
	optimal and suboptimal Bayesian algorithms for non-linear/non-Gaussian
	tracking problems, with a focus on Particle lters. Particle lters
	are sequential Monte Carlo methods based upon point mass (or `particle&#039;)
	representations of probability densities, which can be applied to
	any state space model, and which generalise the traditional Kalman
	ltering methods. Several variants of the particle lter such as SIR,
	ASIR, and RPF, are introduced within a generic framework of the Sequential
	Importance Sampling (SIS) algorithm. These are discussed and compared
	with the standard EKF through an illustrative example.},
  comment = {tutorial},
  file = {:papers\\2002 JNL, A tutorial on particle filters for online nonlinear non-Gaussian Bayesian tracking (TUTORIAL, TSP, 2864).pdf:PDF},
  owner = {salman},
  timestamp = {03,000}
}

@TECHREPORT{2008_TECH_3DvideoColorEnhancement_Aslam,
  author = {Aslam, Salman},
  title = {3D Video Color Enhancement},
  institution = {Georgia Institute of Technology},
  year = {2008},
  comment = {salman_aslam_rep_2},
  file = {:papers\\2008 TECH, Color Enhancement, invention submission-1 (Aslam).doc:Word},
  timestamp = {-}
}

@TECHREPORT{2008_TECH_MGDSP_Aslam,
  author = {Aslam, Salman},
  title = {Implementing the Multi Gaussian Algorithm on the {TI 320DM642 DSP
	} chip},
  institution = {Georgia Institute of Technology},
  year = {2007},
  comment = {salman_aslam_rep_1},
  file = {:papers\\2008 TECH, Implementing the Multi Gaussian Algorithm on the TI TMS320DM642 DSP chip.pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{2009_CNF_RobustSurveillance_Aslam,
  author = {Aslam, S.M. and Bobick, A.F. and Barnes, C.F.},
  title = {Robust Surveillance on Compressed Video: Uniform Performance from
	High to Low Bitrates},
  booktitle = {Advanced Video and Signal Based Surveillance, 2009. AVSS 2009. IEEE
	Conference on},
  year = {2009},
  month = {Sep.},
  comment = {salman_aslam_x},
  keywords = {Computer vision, MPEG-4, mean shift tracker, video compression}
}

@INPROCEEDINGS{2010_CNF_Quant_Aslam,
  author = {Aslam, S. M. and Barnes, C. F. and Bobick, A.F.},
  title = {Modeling the Quantization Staircase Function},
  booktitle = {Data Compression Conference, 2010. Proceedings. DCC 2010},
  year = {2010},
  comment = {salman_aslam_3},
  file = {:papers\\2010 CNF, Modeling the Quantization Staircase Function (Aslam, Barnes, Bobick).pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{2010_CNF_HMMRVQ_Aslam,
  author = {Aslam, S. M. and Barnes, C. F. and Bobick, A. F.},
  title = {Video Action Recognition Using Residual Vector Quantization and Hidden
	Markov Models},
  booktitle = {International Conference on Image Processing, Computer Vision, and
	Pattern Recognition},
  year = {2010},
  comment = {salman_aslam_5},
  file = {:papers\\2010 CNF, Video Action Recognition Using Residual Vector Quantization and Hidden Markov Models (Aslam, Barnes, Bobick).pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{2010_CNF_TrkRVQ_Aslam,
  author = {Aslam, S. M. and Barnes, C. F. and Bobick, A. F.},
  title = {Multi Target Video Tracking Using Residual Vector Quantization},
  booktitle = {International Conference on Image Processing, Computer Vision, and
	Pattern Recognition},
  year = {2010},
  comment = {salman_aslam_6},
  file = {:papers\\2010 CNF, Multi Target Video Tracking Using Residual Vector Quantization (Aslam, Barnes, Bobick).pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{2010_CNF_VehicleContour_Aslam,
  author = {Aslam, S. M. and Barnes, C. F. and Bobick, A. F.},
  title = {Robust Real Time Vehicle Contour Tracking on Low Quality Aerial Infra
	Red Imagery},
  booktitle = {IEEE International Geoscience and Remote Sensing Symposium (IGARSS2010)},
  year = {2010},
  comment = {salman_aslam_4},
  file = {:papers\\2010 CNF, Robust Real Time Vehicle Contour Tracking on Low Quality Aerial Infra Red Imagery (Aslam, Barnes, Bobick).pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{2009_CNF_Compensation_Aslam,
  author = {Aslam, S. M. and Barnes, C. F. and Bobick, A. F.},
  title = {Compensation methods using Signal Processing and adaptive quantization
	for better Mean Shift tracking on compressed video},
  booktitle = {International Conference on Image Processing, Computer Vision, and
	Pattern Recognition},
  year = {2009},
  comment = {salman_aslam_2},
  file = {:papers\\2009 CNF, Compensation methods using Signal Processing and adaptive quantization for better  Mean Shift tracking on compressed video (Aslam, Barnes, Bobick).pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{2009_CNF_CVcompMS1_Aslam,
  author = {Aslam, S. M. and Bobick, A. F. and Barnes, C. F.},
  title = {Better Computer Vision Under Video Compression, An Example Using
	Mean Shift Tracking},
  booktitle = {IEEE International Conference on Image Processing},
  year = {2009},
  comment = {salman_aslam_1},
  file = {:papers\\2009 CNF, Better Computer Vision Under Video Compression, An Example Using Mean Shift Tracking.pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{1988_CNF_VectorCoding_Aslanis,
  author = {Aslanis, J.T. and Kasturia, S. and Dudevoir, G.P. and Cioffi, J.M.},
  title = {Vector coding for partial response channels},
  booktitle = {Military Communications Conference, 1988. MILCOM 88, Conference record.
	'21st Century Military Communications - What's Possible?'. 1988 IEEE},
  year = {1988},
  pages = {667 -671 vol.2},
  month = {oct},
  abstract = {The authors apply multidimensional trellis-coded modulation schemes
	to spectrally shaped channels with additive white Gaussian noise.
	Traditional equalization methods that flatten the channel pulse response
	incur large losses on channels with severe intersymbol interference
	(ISI). Using block symbols, vector coding divides the channel into
	a set of parallel ISI-free channels. By transmitting many codes in
	parallel, each with a different size signal constellation, the input
	signal power is redistributed to the subchannels with superior transfer
	characteristic. The authors examine two different discrete-time channels,
	the duobinary partial-response channel and a representative twisted-wire-pair
	local loop. In both cases, they show that vector coding in concert
	with a good coset code operates near the computational cutoff rate
	for that channel},
  doi = {10.1109/MILCOM.1988.13461},
  file = {:papers\\1988 CNF, Vector coding for partial response channels.pdf:PDF},
  keywords = {discrete-time channels;duobinary partial-response channel;intersymbol
	interference;partial response channels;trellis-coded modulation;twisted-wire-pair
	local loop;vector coding;encoding;modulation;}
}

@ARTICLE{2007_JNL_EnsembleTracking_Avidan,
  author = {Avidan, S.},
  title = {Ensemble Tracking},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2007},
  volume = {29},
  pages = {261-271},
  number = {2},
  abstract = {We consider tracking as a binary classification problem, where an
	ensemble of weak classifiers is trained online to distinguish between
	the object and the background. The ensemble of weak classifiers is
	combined into a strong classifier using AdaBoost. The strong classifier
	is then used to label pixels in the next frame as either belonging
	to the object or the background, giving a confidence map. The peak
	of the map and, hence, the new position of the object, is found using
	mean shift. Temporal coherence is maintained by updating the ensemble
	with new weak classifiers that are trained online during tracking.
	We show a realization of this method and demonstrate it on several
	video sequences},
  comment = {trk_people},
  file = {:papers\\2007 JNL, Ensemble Tracking (Avidan, PAMI, 208).pdf:PDF},
  keywords = {image classification image sequences tracking AdaBoost binary classification
	ensemble tracking mean shift object position temporal coherence video
	sequences visual tracking Algorithms Artificial Intelligence Image
	Enhancement Image Interpretation, Computer-Assisted Information Storage
	and Retrieval Motion Pattern Recognition, Automated Signal Processing,
	Computer-Assisted Subtraction Technique},
  owner = {salman},
  timestamp = {00,200}
}

@ARTICLE{2004_JNL_SVMtracking_Avidan,
  author = {Avidan, Shai.},
  title = {Support vector tracking},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2004},
  volume = {26},
  pages = {1064-1072},
  number = {8},
  abstract = {Support Vector Tracking (SVT) integrates the Support Vector Machine
	(SVM) classifier into an optic-flow-based tracker. Instead of minimizing
	an intensity difference function between successive frames, SVT maximizes
	the SVM classification score. To account for large motions between
	successive frames, we build pyramids from the support vectors and
	use a coarse-to-fine approach in the classification stage. We show
	results of using SVT for vehicle tracking in image sequences.},
  comment = {trk_cars},
  file = {:papers\\2004 JNL, Support Vector Tracking (Avidan, PAMI, 233).pdf:PDF},
  keywords = {image classification image sequences minimisation support vector machines
	vehicles SVM classification coarse to fine approach intensity difference
	function maximization minimization optic flow based tracker successive
	frames support vector machine classifier support vector tracking
	vehicle tracking Algorithms Artificial Intelligence Computer Graphics
	Image Enhancement Image Interpretation, Computer-Assisted Imaging,
	Three-Dimensional Information Storage and Retrieval Movement Numerical
	Analysis, Computer-Assisted Pattern Recognition, Automated Reproducibility
	of Results Sensitivity and Specificity Signal Processing, Computer-Assisted
	Subtraction Technique Video Recording},
  owner = {salman},
  timestamp = {00,230}
}

@ARTICLE{2007_CNF_CornerMatching_Awrangjeb,
  author = {Awrangjeb, M. and Guojun Lu},
  title = {A Robust Corner Matching Technique},
  journal = {Multimedia and Expo, 2007 IEEE International Conference on},
  year = {2007},
  pages = {1483-1486},
  month = {July},
  abstract = {In contour-based comer detectors, absolute curvature values of many
	corners either remain unaltered or change slightly under affine transformations.
	Moreover, affine-length of a curve is relatively invariant to affine
	transformations. This paper presents a novel corner matching technique
	using corners detected by contour-based detectors. For each corner
	we use its position, absolute curvature value, and affine-lengths
	between this corner and other corners on the same curve. The iterative
	matching procedure tries to find three corner matches with minimum
	absolute curvature difference to calculate affine transformation
	parameters. Original corners are transformed with the estimated parameters
	prior to matching with the test corner set.},
  doi = {10.1109/ICME.2007.4284942},
  keywords = {affine transforms, edge detection, image matchingabsolute curvature
	value, affine transformation parameters, affine-lengths, contour-based
	corner detectors, iterative matching procedure, robust corner matching
	technique}
}

@ARTICLE{2001_JNL_HumanBehaviorOffice_Ayers,
  author = {Ayers, D. and Shah, M.},
  title = {Monitoring human behavior from video taken in an office environment},
  journal = {Image and Vision Computing},
  year = {2001},
  volume = {19},
  pages = {833-46},
  abstract = {In this paper, we describe a system which automatically recognizes
	human actions from video sequences taken of a room. These actions
	include entering a room, using a computer terminal, opening a cabinet,
	picking up a phone, etc. Our system recognizes these actions by using
	prior knowledge about the layout of the room. In our system, action
	recognition is modeled by a state machine, which consists of `states'
	and `transitions' between states. The transitions from different
	states can be made based on a position of a person, scene change
	detection, or an object being tracked. In addition to generating
	textual description of recognized actions, the system is able to
	generate a set of key frames from video sequences, which is essentially
	content-based video compression. The system has been tested on several
	video sequences and has performed well. A representative set of results
	is presented in this paper. The ideas presented in this system are
	applicable to automated security},
  comment = {recog_action_appearance},
  file = {:papers\\2001 JNL, Monitoring Human Behavior From Video Taken In An Office Environment (Mubarak Shah, IVC, 95).pdf:PDF},
  keywords = {computer vision data compression image recognition video coding},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{1992_JNL_HausdorffMetric_Baddeley,
  author = {Baddeley, A.},
  title = {Errors in binary images and an Lp version of the HausDorff metric},
  journal = {Nieuw Archief voor Wiskunde},
  year = {1992},
  volume = {10},
  pages = {157-183},
  comment = {math},
  file = {:papers\\1992 JNL, Errors in binary images and an Lp version of the HausDorff metric (Baddeley, 60).pdf:PDF},
  owner = {salman},
  timestamp = {00,050}
}

@BOOK{1982_BOOK_ComputerVision_Ballard,
  title = {Computer Vision},
  publisher = {Prentice Hall},
  year = {1982},
  author = {Ballard, Dana H. and Brown, Christopher M.},
  chapter = {8},
  comment = {CV_book},
  day = {30},
  howpublished = {Hardcover},
  isbn = {0131653164},
  posted-at = {2010-04-14 17:14:09},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0131653164}
}

@ARTICLE{1978_JNL_SURVEYmtt_Barshalom,
  author = { Bar-Shalom, Y.},
  title = {Tracking methods in a multitarget environment},
  journal = {Automatic Control, IEEE Transactions on},
  year = {1978},
  volume = {23},
  pages = { 618 - 626},
  number = {4},
  month = {aug},
  abstract = { The objective of this paper is to survey and put in perspective the
	existing methods of tracking in multitarget environment. In such
	an environment the origin of the measurements can be uncertain: they
	could have come from the target(s) of interest or clutter or false
	alarms or be due to the background. This compact and unified presentation
	of the state-of-art in multitarget tracking was motivated by the
	recent surge of interest in this problem. It is also hoped to be
	useful in view of the need to adapt and modify existing techniques
	before using them for specific problems. Particular attention is
	paid to the assumptions underlying each algorithm and its applicability
	to various situations.},
  comment = {survey},
  file = {:papers\\1978 JNL, Tracking methods in a multitarget environment (SURVEY, Bar-Shalom, TAC, 188).pdf:PDF},
  issn = {0018-9286},
  keywords = { Bayes procedures; Linear systems, stochastic discrete-time; State
	estimation; Tracking; maximum-likelihood (ML) estimation;},
  timestamp = {00,200}
}

@ARTICLE{2009_JNL_PDAF_Barshalom,
  author = {Bar-Shalom, Y. and Daum, F. and Huang, J.},
  title = {The probabilistic data association filter},
  journal = {Control Systems Magazine, IEEE},
  year = {2009},
  volume = {29},
  pages = {82 -100},
  number = {6},
  month = {dec. },
  abstract = {The measurement selection for updating the state estimate of a target's
	track, known as data association, is essential for good performance
	in the presence of spurious measurements or clutter. A classification
	of tracking and data association approaches has been presented, as
	a pure MMSE approach, which amounts to a soft decision, and single
	best-hypothesis approach, which amounts to a hard decision. It has
	been shown that the optimal state estimator in the presence of data
	association uncertainty consists of the computation of the conditional
	pdf of the state x(k) given all information available at time k,
	namely, the prior information about the initial state, the intervening
	known inputs, and the sets of measurements through time k. It has
	also been pointed out that if the exact conditional pdf, which is
	a mixture, is available, then its recursion requires only the probabilities
	of the most recent association events. The conditions under which
	this result holds, namely whiteness of the noise, detection, and
	clutter processes, were presented. The PDAF and JPDAF algorithms,
	which carry out data association and state estimation in clutter,
	have been described. A simple example was given to illustrate how
	the clutter and occasional missed detections can lead to track loss
	for a standard tracking filter, and how PDAF can keep the target
	in track under such circumstances. By using the Monte Carlo in a
	simulated based surveillance as an exampled shown. The numerous applications
	of the PDAF/JPDAF illustrated in "Real-World Applications of PDAF
	and JPDAF" show the potential pitfalls of using sophisticated algorithms
	for tracking in difficult environments as well as how to overcome
	them. The effect of finite sensor resolution can be a more severe
	problem than the data association and deserves special attention.},
  comment = {trk_correspondence},
  doi = {10.1109/MCS.2009.934469},
  file = {:papers\\2009 JNL, The Probabilistic Data Association Filter (Bar-Shalom, ControlSysMag).pdf:PDF},
  issn = {0272-1708},
  keywords = {Kalman filter;MMSE approach;Monte Carlo;clutter;finite sensor resolution;optimal
	state estimator;probabilistic data association filter;simulated based
	surveillance;target tracking;tracking filter;Kalman filters;Monte
	Carlo methods;clutter;least mean squares methods;probability;sensor
	fusion;state estimation;target tracking;tracking filters;},
  timestamp = {-}
}

@INPROCEEDINGS{1972_CNF_PDAF_BarShalom,
  author = {Bar-Shalom, Y. and Jaffer, A.G.},
  title = {Adaptive nonlinear filtering for tracking with measurements of uncertain
	origin},
  booktitle = {Decision and Control, 1972 and 11th Symposium on Adaptive Processes.
	Proceedings of the 1972 IEEE Conference on},
  year = {1972},
  volume = {11},
  comment = {trk},
  doi = {10.1109/CDC.1972.268994},
  file = {:papers\\1972 CNF, Adaptive nonlinear filtering for tracking with measurements of uncertain (Bar-Shalom, Jaffer, SAP, 34).pdf:PDF},
  timestamp = {-}
}

@ARTICLE{1975_JNL_PDAF_BarShalom,
  author = {Bar-Shalom, Yaakov and Tse, Edison},
  title = {Tracking in a Cluttered Environment With Probabilistic Data Association},
  journal = {Automatica},
  year = {1975},
  volume = {11},
  pages = {451-460},
  number = {Compendex},
  abstract = {Tracking a target with uncertainty in the origin of measurements is
	accomplished with an algorithm, suitable for real-time implementation,
	which utilizes the a posteriori probabilities of the measurements
	having originated from the target.},
  comment = {trk_correspondence},
  file = {:papers\\1975 JNL, Tracking in a Cluttered Environment With Probabilistic Data Association (Bar-Shalom, Tse, Automatica, 389).pdf:PDF},
  keywords = {RADAR},
  owner = {salman},
  timestamp = {00,400}
}

@INPROCEEDINGS{2004_CNF_DSSAdataMining_Barnes,
  author = {Barnes, C.F.},
  title = {Successive approximation source coding and image enabled data mining},
  booktitle = {Data Compression Conference, 2004. Proceedings. DCC 2004},
  year = {2004},
  abstract = { This paper deals with successive approximation source coding and
	image enabled data mining. Successive approximation source codes
	provide query returns consisting of sequences of aggregate data-tuples
	with image sets. A data mining statistical analysis or pattern search
	over a sequence of aggregates provides a sequence of data mining
	answers that is desirably fuzzy. Residual vector quantization (RVQ)
	provides a successive approximation source code with utility in image-enabled
	queries in image data mining tasks.},
  comment = {VQ_RVQ},
  doi = {10.1109/DCC.2004.1281501},
  file = {:papers\\2004 CNF, Successive approximation source coding and image enabled data mining (Barnes).pdf:PDF},
  issn = {1068-0314 },
  keywords = { data-tuple sequence; image enabled data mining; image set; pattern
	search; query return; residual vector quantization; statistical analysis;
	successive approximation source coding; data mining; image coding;
	image sequences; query processing; source coding; statistical analysis;
	vector quantisation;},
  timestamp = {-}
}

@INPROCEEDINGS{1996_CNF_VQclassification_Barnes,
  author = {Barnes, C.F.},
  title = {A new approach to VQ-based compression and classification of sensor
	data},
  booktitle = {Aerospace and Electronics Conference, 1996. NAECON 1996., Proceedings
	of the IEEE 1996 National},
  year = {1996},
  abstract = {The size and power resource constraints associated with the airborne
	platforms of various sensor systems require that sensor data be transmitted
	to ground based automatic target recognition (ATR) processing stations.
	However, the bandwidth limitations of feasible communication channels
	restrict the amount of data that can be reliably transmitted. This
	presentation gives an overview of a new technology that permits efficient
	clipping service on board the airframe that is based on the use of
	vector quantization (VQ). The new technology generates a mathematical
	decomposition of sensor data called ldquo;direct sum successive approximations
	rdquo; (DSSA). DSSA technologies permit low-level ATR functions to
	be combined with VQ data compression functions to form an intelligent
	clipping service},
  comment = {VQ_RVQ},
  doi = {10.1109/NAECON.1996.517657},
  file = {:papers\\1996 CNF, A new approach to VQ-based compression and classification of sensor data (Barnes).pdf:PDF},
  keywords = {VQ-based compression;airborne platforms;automatic target recognition;bandwidth
	limitations;classification;communication channels;data compression
	functions;direct sum successive approximation;ground based processing
	stations;intelligent clipping service;mathematical decomposition;power
	resource constraints;sensor data;vector quantization;aerospace computing;aircraft
	instrumentation;image classification;image sensors;vector quantisation;},
  timestamp = {-}
}

@CONFERENCE{1993_sigmaTrees_Barnes,
  author = {Barnes, C.F.},
  title = {Tree structured signal space codes},
  booktitle = {Coding and quantizatiion: DIMACS/IEEE Workshop, October 19-21, 1992},
  year = {1993},
  pages = {33},
  organization = {Amer Mathematical Society},
  isbn = {0821866036}
}

@ARTICLE{2007_JNL_Katrina_Barnes,
  author = {Barnes, C.F. and Fritz, H. and Jeseon Yoo},
  title = {Hurricane Disaster Assessments With Image-Driven Data Mining in High-Resolution
	Satellite Imagery},
  journal = {Geoscience and Remote Sensing, IEEE Transactions on},
  year = {2007},
  volume = {45},
  pages = {1631 -1640},
  number = {6},
  month = {june },
  abstract = {Detection, classification, and attribution of high-resolution satellite
	image features in nearshore areas in the aftermath of Hurricane Katrina
	in Gulfport, MS, are investigated for damage assessments and emergency
	response planning. A system-level approach based on image-driven
	data mining with sigma-tree structures is demonstrated and evaluated.
	Results show a capability to detect hurricane debris fields and storm-impacted
	nearshore features (such as wind-damaged buildings, sand deposits,
	standing water, etc.) and an ability to detect and classify nonimpacted
	features (such as buildings, vegetation, roadways, railways, etc.).
	The sigma-tree-based image information mining capability is demonstrated
	to be useful in disaster response planning by detecting blocked access
	routes and autonomously discovering candidate rescue/recovery staging
	areas},
  comment = {VQ_RVQ},
  doi = {10.1109/TGRS.2007.890808},
  file = {:papers\\2007 JNL, Hurricane disaster assessments with image-driven data mining in high-resolution satellite imagery (Barnes).pdf:PDF},
  issn = {0196-2892},
  keywords = {Gulfport;Hurricane Katrina;Massachusetts;data mining;disaster assessment;emergency
	response planning;image attribution;image classification;image detection;satellite
	imaging;sigma-tree structure;data mining;disasters;geography;image
	classification;storms;terrain mapping;}
}

@ARTICLE{1992_JNL_RVQ_Barnes,
  author = {Barnes, C.F. and Frost, R.},
  title = {Residual vector quantizers with jointly optimized code books},
  journal = {Advances in Electronics and Electron Physics},
  year = {1992},
  volume = {84},
  pages = {1-59},
  comment = {VQ_RVQ},
  file = {:papers\\1992 JNL, Residual vector quantizers with jointly optimized code books (Barnes, Frost).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{2007_JNL_IDDM_Barnes,
  author = {Barnes, C. F.},
  title = {Image-Driven Data Mining for Image Content Segmentation, Classification,
	and Attribution},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  year = {2007},
  volume = {45},
  pages = {2964-2978},
  abstract = {Image-driven data mining methods are described for image content segmentation,
	classification, and attribution, where each pixel location of an
	image-under-analysis is the center point of a pixel-block query that
	returns an estimated class label. Feature attribute estimates may
	also be mined when sufficient attribute strata exist in the data
	warehouse. Novel methods are presented for pixel-block mining, pattern
	similarity scoring, class label assignments, and attribute mining.
	These methods are based on a direct sum tree structure called a sigma-tree
	that is utilized with near-neighbor similarity scoring. The sigma-tree
	structure provides a solution to the challenge of high computation/memory
	costs of pixel-block similarity searching. The sigma-trees are integrated
	into warehouse subsystems that provide referential capability into
	feature attribute data, resulting in a foundation for data mining
	called Source Optimized, Labeled, DIgital Expanded Representations
	(SOLDIER). The variable depth "bit-plane" data representations produced
	by sigma-tree path selections provide an approach to image content
	segmentation, and provide a structure for formulation of Bayesian
	classification with data-adaptive Parzen classifiers with variably
	sized windows. Preliminary methods and results for postprocessing
	of mined feature-thematic layers for higher level scene understanding
	are also presented. Sample results are shown with synthetic aperture
	radar images and with high-resolution pan-sharpened satellite images
	of the Payagala, Sri Lanka area before the site was devastated by
	the 2004 Asian Tsunami.},
  comment = {VQ_RVQ},
  file = {:papers\\2007 JNL, Image-Driven Data Mining for Image Content Segmentation, Classification, and Attribution (Barnes, TGRS, 3).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{1993_JNL_RVQDSC_Barnes,
  author = {Barnes, C. F. and Frost, R. L.},
  title = {Vector quantizers with direct sum codebooks},
  journal = {IEEE Transactions on Information Theory},
  year = {1993},
  volume = {39},
  pages = {565-80},
  number = {Copyright 1993, IEE},
  abstract = {The use of direct sum codebooks to minimize the memory requirements
	of vector quantizers is investigated. Assuming arbitrary fixed partitions,
	necessary conditions for minimum distortion codebooks are derived,
	first for scalar codebooks, assuming mean-squared error distortion,
	and then for vector codebooks and a broader class of distortion measures.
	An iterative procedure is described for designing locally optimal
	direct sum codebooks. Both optimal and computationally efficient
	suboptimal encoding schemes are considered. It is shown that although
	an optimal encoding can be implemented by a sequential encoder, the
	complexity of implementing optimal stagewise partitions generally
	exceeds the complexity of an exhaustive search of the direct sum
	codebook. It is also shown that sequential nearest-neighbor encoders
	can be extremely inefficient. The M-search method is explored as
	one method of improving the effectiveness of suboptimal sequential
	encoders. Representative results for simulated direct sum quantizers
	are presented},
  comment = {VQ_RVQ},
  file = {:papers\\1993 JNL, Vector quantizers with direct sum codebooks (Barnes).pdf:PDF},
  keywords = {encoding iterative methods optimisation search problems vector quantisation},
  owner = {salman},
  timestamp = {00,050}
}

@ARTICLE{1996_JNL_AdvancesRVQ_Barnes,
  author = {Barnes, C. F. and Rizvi, S. A. and Nasrabadi, N. M.},
  title = {Advances in residual vector quantization: a review},
  journal = {Image Processing, IEEE Transactions on},
  year = {1996},
  volume = {5},
  pages = {226-262},
  number = {2},
  abstract = {Advances in residual vector quantization (RVQ) are surveyed. Definitions
	of joint encoder optimality and joint decoder optimality are discussed.
	Design techniques for RVQs with large numbers of stages and generally
	different encoder and decoder codebooks are elaborated and extended.
	Fixed-rate RVQs, and variable-rate RVQs that employ entropy coding
	are examined. Predictive and finite state RVQs designed and integrated
	into neural-network based source coding structures are revisited.
	Successive approximation RVQs that achieve embedded and refinable
	coding are reviewed. A new type of successive approximation RVQ that
	varies the instantaneous block rate by using different numbers of
	stages on different blocks is introduced and applied to image waveforms,
	and a scalar version of the new residual quantizer is applied to
	image subbands in an embedded wavelet transform coding system},
  comment = {survey, VQ},
  file = {:papers\\1996 JNL, Advances in residual vector quantization_ a review (SURVEY, Barnes, TIP, 68).pdf:PDF},
  keywords = {decoding entropy codes finite state machines image coding neural nets
	reviews source coding transform coding vector quantisation embedded
	coding embedded wavelet transform coding system entropy coding finite
	state RVQs fixed-rate RVQ image subbands image waveforms instantaneous
	block rate joint decoder optimality joint encoder optimality neural-network
	based source coding structures predictive RVQs refinable coding residual
	quantizer residual vector quantization review successive approximation
	RVQ variable-rate RVQ},
  owner = {salman},
  timestamp = {00,070}
}

@ARTICLE{1994_JNL_PerfOpticalFlow_Barron,
  author = {Barron, J. L. and Fleet, D. J. and Beauchemin, S. S. and Burkitt,
	T. A.},
  title = {Performance Of Optical Flow Techniques},
  journal = {International Journal of Computer Vision},
  year = {1994},
  volume = {12},
  pages = {43-77},
  number = {1},
  abstract = {While different optical flow techniques continue to appear, there
	has been a lack of quantitative evaluation of existing methods. For
	a common set of real and synthetic image sequences, we report the
	results of a number of regularly cited optical flow techniques, including
	instances of differential, matching, energy-based and phase-based
	methods. Our comparisons are primarily empirical, and concentrate
	on the accuracy, reliability and density of the velocity measurements;
	they show that...},
  comment = {motion},
  file = {:papers\\1994 JNL, Performance of optical flow techniques (Barron, Fleet, Beauchemin, IJCV, 3036).pdf:PDF},
  keywords = {computer-vision optical-flow},
  owner = {salman},
  timestamp = {03,000}
}

@INPROCEEDINGS{2006_CNF_Stabilization_Batur,
  author = {Batur, A.U. and Flinchbaugh, B.},
  title = {Video Stabilization with Optimized Motion Estimation Resolution},
  booktitle = {Image Processing, 2006 IEEE International Conference on},
  year = {2006},
  abstract = {We propose a video stabilization algorithm that removes camera jitter
	in complex scenes with moving objects. Our algorithm is based on
	analyzing the content of the video according to the accumulated motion
	between the scene and the camera, which provides a simple and robust
	way of estimating jitter in the presence of camera panning and moving
	objects. We also describe a method of reducing the computational
	complexity of our algorithm by using a hierarchical clustering technique
	that optimizes the motion estimation resolution at different scene
	locations},
  comment = {trk_stabilization},
  doi = {10.1109/ICIP.2006.312494},
  file = {:papers\\2006 CNF, Video Stabilization with Optimized Motion Estimation Resolution (Batur, Flinchbaugh).pdf:PDF},
  issn = {1522-4880},
  keywords = {camera panning;hierarchical clustering technique;jitter estimation;motion
	estimation resolution;moving objects;video stabilization algorithm;image
	resolution;jitter;motion estimation;video cameras;},
  timestamp = {-}
}

@INPROCEEDINGS{1994_CNF_ContourTracking_BaumbergHogg,
  author = {Baumberg, A. M. and Hogg, D. C.},
  title = {An efficient method for contour tracking using active shape models},
  booktitle = {Motion of Non-Rigid and Articulated Objects, 1994., Proceedings of
	the 1994 IEEE Workshop on},
  year = {1994},
  pages = {194-199},
  comment = {trk_contour},
  file = {:papers\\1994 CNF,  An efficient  method for contour tracking using active shape models (Baumberg, Hogg, MNRAO, 247).pdf:PDF},
  keywords = {Kalman filters computer vision Kalman filter active shape models contour
	localisation contour tracking dynamic filtering feature search iterative
	refinement modal-based flexible shape model real image data silhouette},
  owner = {salman},
  timestamp = {00,250}
}

@ARTICLE{1989_JNL_HierarchyPictureSegmentation_BeaulieuGoldberg,
  author = {Beaulieu, J.-M. and Goldberg, M.},
  title = {Hierarchy in picture segmentation: a stepwise optimization approach
	},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1989},
  volume = {11},
  pages = {150 -163},
  number = {2},
  month = {feb},
  abstract = {A segmentation algorithm based on sequential optimization which produces
	a hierarchical decomposition of the picture is presented. The decomposition
	is data driven with no restriction on segment shapes. It can be viewed
	as a tree, where the nodes correspond to picture segments and where
	links between nodes indicate set inclusions. Picture segmentation
	is first regarded as a problem of piecewise picture approximation,
	which consists of finding the partition with the minimum approximation
	error. Then, picture segmentation is presented as an hypothesis-testing
	process which merges only segments that belong to the same region.
	A hierarchical decomposition constraint is used in both cases, which
	results in the same stepwise optimization algorithm. At each iteration,
	the two most similar segments are merged by optimizing a stepwise
	criterion. The algorithm is used to segment a remote-sensing picture,
	and illustrate the hierarchical structure of the picture},
  comment = {seg},
  doi = {10.1109/34.16711},
  file = {:papers\\1989 JNL, Hierarchy in picture segmentation_ a stepwise optimization approach (Beaulieu, Goldberg, PAMI, 161).pdf:PDF},
  issn = {0162-8828},
  keywords = {computerised picture processing;data structure;hierarchical decomposition;iterative
	methods;picture segmentation;sequential optimization;stepwise optimization;tree;computerised
	picture processing;iterative methods;optimisation;trees (mathematics);},
  timestamp = {00,200}
}

@INPROCEEDINGS{2009_CNF_HumanDetection_Beleznai,
  author = {Beleznai, C. and Bischof, H.},
  title = {Fast human detection in crowded scenes by contour integration and
	local shape estimation},
  booktitle = {Computer Vision and Pattern Recognition, IEEE Conference on},
  year = {2009},
  abstract = {The complexity of human detection increases significantly with a growing
	density of humans populating a scene. This paper presents a Bayesian
	detection framework using shape and motion cues to obtain a maximum
	a posteriori (MAP) solution for human configurations consisting of
	many, possibly occluded pedestrians viewed by a stationary camera.
	The paper contains two novel contributions for the human detection
	task: 1. computationally efficient detection based on shape templates
	using contour integration by means of integral images which are built
	by oriented string scans; (2) a non-parametric approach using an
	approximated version of the shape context descriptor which generates
	informative object parts and infers the presence of humans despite
	occlusions. The outputs of the two detectors are used to generate
	a spatial configuration of hypothesized human body locations. The
	configuration is iteratively optimized while taking into account
	the depth ordering and occlusion status of the hypotheses. The method
	achieves fast computation times even in complex scenarios with a
	high density of people. Its validity is demonstrated on a substantial
	amount of image data using the CAVIAR and our own datasets. Evaluation
	results and comparison with state of the art are presented.},
  comment = {trk_crowd},
  file = {:papers\\2009 CNF, Fast human detection in crowded scenes by contour integration and local shape estimation (C. Belezani, H. Bischof Graz, CVPR).pdf:PDF},
  keywords = {Bayes methods edge detection image motion analysis maximum likelihood
	estimation object detection Bayesian detection framework CAVIAR contour
	integration crowded scenes fast human detection integral images local
	shape estimation maximum a posteriori solution motion cues nonparametric
	approach oriented string scans shape context descriptor shape templates},
  timestamp = {-}
}

@ARTICLE{1997_JNL_EigenVsFisherFaces_Bel,
  author = {Belhumeur, P.N. and Hespanha, J.P. and Kriegman, D.J.},
  title = {Eigenfaces vs. Fisherfaces: recognition using class specific linear
	projection},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1997},
  volume = {19},
  pages = {711 -720},
  number = {7},
  month = jul,
  abstract = {We develop a face recognition algorithm which is insensitive to large
	variation in lighting direction and facial expression. Taking a pattern
	classification approach, we consider each pixel in an image as a
	coordinate in a high-dimensional space. We take advantage of the
	observation that the images of a particular face, under varying illumination
	but fixed pose, lie in a 3D linear subspace of the high dimensional
	image space-if the face is a Lambertian surface without shadowing.
	However, since faces are not truly Lambertian surfaces and do indeed
	produce self-shadowing, images will deviate from this linear subspace.
	Rather than explicitly modeling this deviation, we linearly project
	the image into a subspace in a manner which discounts those regions
	of the face with large deviation. Our projection method is based
	on Fisher's linear discriminant and produces well separated classes
	in a low-dimensional subspace, even under severe variation in lighting
	and facial expressions. The eigenface technique, another method based
	on linearly projecting the image space to a low dimensional subspace,
	has similar computational requirements. Yet, extensive experimental
	results demonstrate that the proposed ldquo;Fisherface rdquo; method
	has error rates that are lower than those of the eigenface technique
	for tests on the Harvard and Yale face databases},
  comment = {TRK_subspace},
  file = {:papers\\1997 JNL, Eigenfaces vs. Fisherfaces_ recognition using class specific linear projection (Belhumeur).pdf:PDF},
  issn = {0162-8828},
  keywords = {3D linear subspace;Fisherfaces;Lambertian surface;class specific linear
	projection;computational requirements;eigenfaces;face recognition;facial
	expression insensitivity;high-dimensional space;lighting direction
	insensitivity;linear discriminant;linear projection;low-dimensional
	subspace;pattern classification;face recognition;pattern classification;},
  timestamp = {05,000}
}

@ARTICLE{1982_JNL_MinENT_Berger,
  author = {Berger, T.},
  title = {Minimum entropy quantizers and permutation codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1982},
  volume = {28},
  pages = { 149 - 157},
  number = {2},
  month = {mar},
  abstract = { Amplitude quantization and permutation encoding are two approaches
	to efficient digitization of analog data. It has been proven that
	they are equivalent in the sense that their optimum rate versus distortion
	performances are identical. Reviews of the aforementioned results
	and of work performed in the interim by several investigators are
	presented. Equations which must be satisfied by the thresholds of
	the minimum entropy quantizer that achieves a prescribed meanrth
	power distortion are derived, and an iterative procedure for solving
	them is developed. It is shown that these equations often have many
	families of solutions. In the case of the Laplacian distribution,
	for which we had previously shown that quantizers with uniformly
	spaced thresholds satisfy the equations whenr=2, other families of
	solutions with nonuniform spacing are exhibited. What had appeared
	to be a discrepancy between the performances of optimum permutation
	codes and minimum entropy quantizers is resolved by the resulting
	optimum quantizers, which span all entropy rates from zero to infinity.},
  comment = {IT},
  file = {:papers\\1982 JNL, Minimum entropy quantizers and permutation codes (Berger).pdf:PDF},
  issn = {0018-9448},
  keywords = { Permutation coding; Quantization (signal); Signal quantization;},
  timestamp = {00,050}
}

@ARTICLE{2000_JNL_MorphingActiveContours_BertalmioSapiroRandall,
  author = {Bertalmio, M. and Sapiro, G. and Randall, G.},
  title = {Morphing active contours},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {733 -737},
  number = {7},
  month = {jul},
  abstract = {A method for deforming curves in a given image to a desired position
	in a second image is introduced. The algorithm is based on deforming
	the first image toward the second one via a partial differential
	equation (PDE), while tracking the deformation of the curves of interest
	in the first image with an additional, coupled PDE; both the images
	and the curves on the frame/slices of interest are used for tracking.
	The technique can be applied to object tracking and sequential segmentation.
	The topology of the deforming curve can change without any special
	topology handling procedures added to the scheme. This permits, for
	example, the automatic tracking of scenes where, due to occlusions,
	the topology of the objects of interest changes from frame to frame.
	In addition, this work introduces the concept of projecting velocities
	to obtain systems of coupled PDEs for image analysis applications.
	We show examples for object tracking and segmentation of electronic
	microscopy},
  comment = {trk_contour},
  doi = {10.1109/34.865191},
  file = {:papers\\2000 JNL, Morphing active contours (Bertalmio, Sapiro, Randall, PAMI, 69).pdf:PDF},
  issn = {0162-8828},
  keywords = {active contour morphing;image analysis;image curve deformation;object
	tracking;partial differential equation;sequential segmentation;topology;image
	morphing;image segmentation;partial differential equations;target
	tracking;topology;},
  timestamp = {00,070}
}

@INPROCEEDINGS{1999_CNF_RTtrackingMultiplePeople_Beymer,
  author = {Beymer, D and Konolige, K},
  title = {Real-Time Tracking of Multiple People Using Continuous Detection},
  booktitle = {IEEE International Conference on Computer Vision (ICCV) Frame-Rate
	Workshop..},
  year = {1999},
  abstract = {Recent investigations have shown the advantages of keep- ing multiple
	hypotheses during visual tracking. In this paper we explore an alternative
	method that keeps just a single hypothesis per tracked object for
	computational eciency, but displays robust performance and recov-
	ery from error by employing continuous detection during tracking.
	The method is implemented in the domain of people-tracking, using
	a novel combination of stereo in- formation for continuous detection
	and intensity image correlation for tracking. Real-time stereo provides
	ex- tended information for 3D detection and tracking, even in the
	presence of crowded scenes, obscuring objects, and large scale changes.
	We are able to reliably detect and track people in natural environments,
	on an implemented system that runs at more than 10 Hz on standard
	PC hardware.},
  comment = {trk_people},
  file = {:papers\\1999 CNF, Real-time tracking of multiple people using continuous detection (Beymer, Konolige, ICCV, 100).pdf:PDF},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{1994_JNL_FaceTop_Bichsel,
  author = {Bichsel, M. and Pentland, A.P.},
  title = {Human face recognition and the face image set's topology},
  journal = {CVGIP: Image Understanding},
  year = {1994},
  volume = { 59},
  pages = {254 - 61},
  number = { 2},
  note = {face recognition;face image;geometrical transformations;topology;object
	recognition;physical transformations;small rotations;},
  abstract = {Considering an n&times;n image as an n<sup>2</sup>-dimensional vector,
	then images of faces can be considered as points in this n<sup>2</sup>-dimensional
	image space. Previous studies of physical transformations of the
	face, including translation, small rotations, and illumination changes,
	showed that the set of face images consists of relatively simple
	connected subregions in image space. Consequently linear matching
	techniques can be used to obtain reliable face recognition. However,
	for more general transformations, such as large rotations or scale
	changes, the face subregions become highly non-convex. We have therefore
	developed a scale-space. matching technique that allows us to take
	advantage of knowledge about important geometrical transformations
	and about the topology of the face subregion in image space. While
	recognition of faces is the focus of the paper, the algorithm is
	sufficiently general to be applicable to a large variety of object
	recognition tasks},
  address = {USA},
  comment = {TRK_subspace},
  copyright = {Copyright 1994, IEE},
  file = {:papers\\1994 JNL, Human face recognition and the face image sets topology (Bichsel).pdf:PDF},
  issn = {1049-9660},
  keywords = {face recognition;topology;},
  language = {English},
  timestamp = {00,140},
  url = {http://dx.doi.org/10.1006/ciun.1994.1017}
}

@INPROCEEDINGS{1982_CNF_SpeechRVQ_JuangGray,
  author = {Biing-Hwang, Juang and Gray, A., Jr.},
  title = {Multiple stage vector quantization for speech coding},
  booktitle = {Acoustics, Speech, and Signal Processing, IEEE International Conference
	on ICASSP '82.},
  year = {1982},
  abstract = {In this paper, we present a multiple stage vector quantization technique
	which allows easy expansion of the original vector quantizer design
	to operate at higher bit rates for lower distortion. The computation
	and storage reduction is achieved by the fact that the overall requirements
	are the sum of the requirements of each stage instead of an exponentially
	increasing function of the bit rate as in the original one stage
	design. In the case of Euclidean distance measures such as the log
	area ratio measure, experimental results show that the quantizer
	performance is very close to a theoretically predicted asymptotically
	optimal rate distortion relationship.},
  comment = {VQ_RVQ},
  file = {:papers\\1982 CNF, Multiple stage vector quantization for speech coding (Juang, Gray, ICASSP, 174).pdf:PDF},
  owner = {salman},
  timestamp = {00,200}
}

@INPROCEEDINGS{1998_CNF_HeadTracking_Birchfield,
  author = {Birchfield, S.},
  title = {Elliptical head tracking using intensity gradients and color histograms},
  booktitle = {Computer Vision and Pattern Recognition, Proceedings. IEEE Computer
	Society Conference on},
  year = {1998},
  pages = {232-237},
  abstract = {An algorithm for tracking a person's head is presented. The head's
	projection onto the image plane is modeled as an ellipse whose position
	and size are continually updated by a local search combining the
	output of a module concentrating on the intensity gradient around
	the ellipse's perimeter with that of another module focusing on the
	color histogram of the ellipse's interior. Since these two modules
	have roughly orthogonal failure modes, they serve to complement one
	another. The result is a robust, real-time system that is able to
	track a person's head with enough accuracy to automatically control
	the camera's pan, tilt, and zoom in order to keep the person centered
	in the field of view at a desired size. Extensive experimentation
	shows the algorithm's robustness with respect to full 360-degree
	out-of-plane rotation, up to 90-degree tilting, severe but brief
	occlusion, arbitrary camera movement, and multiple moving people
	in the background},
  comment = {trk_people},
  file = {:papers\\1998 CNF, Elliptical head tracking using intensity gradients and color histograms (Birchfield, CVPR, 556).pdf:PDF},
  keywords = {object recognition search problems tracking camera movement color
	histograms ellipse head tracking intensity gradients local search
	occlusion out-of-plane rotation real-time system robustness},
  owner = {salman},
  timestamp = {00,600}
}

@ARTICLE{2010_JNL_Tracking_Bishop,
  author = {Bishop, A. N. and Savkin, A. V. and Pathirana, P. N.},
  title = {Vision-Based Target Tracking and Surveillance With Robust Set-Valued
	State Estimation},
  journal = {Signal Processing Letters, IEEE},
  year = {2010},
  volume = {17},
  pages = {289-292},
  number = {3},
  abstract = {<para> Tracking a target from a video stream (or a sequence of image
	frames) involves nonlinear measurements in Cartesian coordinates.
	However, the target dynamics, modeled in Cartesian coordinates, result
	in a linear system. We present a robust linear filter based on an
	analytical nonlinear to linear measurement conversion algorithm.
	Using ideas from robust control theory, a rigorous theoretical analysis
	is given which guarantees that the state estimation error for the
	filter is bounded, i.e., a measure against filter divergence is obtained.
	In fact, an ellipsoidal set-valued estimate is obtained which is
	guaranteed to contain the true target location with an arbitrarily
	high probability. The algorithm is particularly suited to visual
	surveillance and tracking applications involving targets moving on
	a plane. </para>},
  comment = {trk},
  file = {:papers\\2010 JNL, Vision-Based Target Tracking and Surveillance With Robust Set-Valued State Estimation (Bishop, SPLETTERS).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@BOOK{2007_BOOK_PRML_Bishop,
  title = {Pattern Recognition and Machine Learning (Information Science and
	Statistics)},
  publisher = {Springer},
  year = {2007},
  author = {Bishop, Christopher M.},
  edition = {1st},
  month = {October},
  abstract = {{The dramatic growth in practical applications for machine learning
	over the last ten years has been accompanied by many important developments
	in the underlying algorithms and techniques. For example, Bayesian
	methods have grown from a specialist niche to become mainstream,
	while graphical models have emerged as a general framework for describing
	and applying probabilistic techniques. The practical applicability
	of Bayesian methods has been greatly enhanced by the development
	of a range of approximate inference algorithms such as variational
	Bayes and expectation propagation, while new models based on kernels
	have had a significant impact on both algorithms and applications.
	This completely new textbook reflects these recent developments while
	providing a comprehensive introduction to the fields of pattern recognition
	and machine learning. It is aimed at advanced undergraduates or first-year
	PhD students, as well as researchers and practitioners. No previous
	knowledge of pattern recognition or machine learning concepts is
	assumed. Familiarity with multivariate calculus and basic linear
	algebra is required, and some experience in the use of probabilities
	would be helpful though not essential as the book includes a self-contained
	introduction to basic probability theory. The book is suitable for
	courses on machine learning, statistics, computer science, signal
	processing, computer vision, data mining, and bioinformatics. Extensive
	support is provided for course instructors, including more than 400
	exercises, graded according to difficulty. Example solutions for
	a subset of the exercises are available from the book web site, while
	solutions for the remainder can be obtained by instructors from the
	publisher. The book is supported by a great deal of additional material,
	and the reader is encouraged to visit the book web site for the latest
	information. A forthcoming companion volume will deal with practical
	aspects of pattern recognition and machine learning, and will include
	free software implementations of the key algorithms along with example
	data sets and demonstration programs. Christopher Bishop is Assistant
	Director at Microsoft Research Cambridge, and also holds a Chair
	in Computer Science at the University of Edinburgh. He is a Fellow
	of Darwin College Cambridge, and was recently elected Fellow of the
	Royal Academy of Engineering. The author's previous textbook "Neural
	Networks for Pattern Recognition" has been widely adopted.}},
  comment = {PRML_book},
  day = {01},
  file = {:papers\\2007 BOOK, Pattern Recognition and Machine Learning (Bishop, Springer).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0387310738},
  posted-at = {2010-04-14 17:26:08},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387310738}
}

@ARTICLE{1996_CNF_MultipleMotionsFlowFields_Black,
  author = {Black, M. J. and Anandan, P.},
  title = {The robust estimation of multiple motions: parametric and piecewise-smooth
	flow fields},
  journal = {Computer Vision and Image Understanding},
  year = {1996},
  volume = {63},
  pages = {75-104},
  number = {Copyright 1996, IEE},
  abstract = {Most approaches for estimating optical flow assume that, within a
	finite image region, only a single motion is present. This single
	motion assumption is violated in common situations involving transparency,
	depth discontinuities, independently moving objects, shadows, and
	specular reflections. To robustly estimate optical flow, the single
	motion assumption must be relaxed. This paper presents a framework
	based on robust estimation that addresses violations of the brightness
	constancy and spatial smoothness assumptions caused by multiple motions.
	We show how the robust estimation framework can be applied to standard
	formulations of the optical flow problem thus reducing their sensitivity
	to violations of their underlying assumptions. The approach has been
	applied to three standard techniques for recovering optical flow:
	area-based regression, correlation, and regularization with motion
	discontinuities. This paper focuses on the recovery of multiple parametric
	motion models within a region, as well as the recovery of piecewise-smooth
	flow fields, and provides examples with natural and synthetic image
	sequences},
  comment = {motion},
  file = {:papers\\1996 JNL, The robust estimation of multiple motions_ parametric and piecewise-smooth flow fields (Black, Anandan, CVIU, 875).pdf:PDF},
  keywords = {estimation theory image sequences motion estimation},
  owner = {salman},
  timestamp = {00,900}
}

@ARTICLE{1998_JNL_Eigentracking_Black,
  author = {Black, M. J. and Jepson, A. D.},
  title = {EigenTracking: robust matching and tracking of articulated objects
	using a view-based representation},
  journal = {International Journal of Computer Vision},
  year = {1998},
  volume = {26},
  pages = {63-84},
  number = {Copyright 1998, IEE},
  abstract = {This paper describes an approach for tracking rigid and articulated
	objects using a view-based representation. The approach builds on
	and extends work on eigenspace representations, robust estimation
	techniques, and parameterized optical flow estimation. First, we
	note that the least-squares image reconstruction of standard eigenspace
	techniques has a number of problems and we reformulate the reconstruction
	problem as one of robust estimation. Second we define a subspace
	constancy assumption that allows us to exploit techniques for parameterized
	optical flow estimation to solve for both the view of an object and
	the affine transformation between the eigenspace and the image. To
	account for large affine transformations between the eigenspace and
	the image we define a multi-scale eigenspace representation and a
	coarse-to-fine matching strategy. Finally, we use these techniques
	to track objects over long image sequences in which the objects simultaneously
	undergo both affine image motions and changes of view. In particular
	we use this EigenTracking technique to track and recognize the gestures
	of a moving hand},
  comment = {TRK_subspace},
  file = {:papers\\1998 JNL, EigenTracking_ robust matching and tracking of articulated objects using a view-based representation (Black, Jepson, IJCV, 810).pdf:PDF},
  keywords = {estimation theory image reconstruction image sequences least squares
	approximations motion estimation object recognition},
  owner = {salman},
  timestamp = {00,800}
}

@BOOK{2000_BOOK_ActiveVision_Blake,
  title = {Active Contours: The Application of Techniques from Graphics, Vision,
	Control Theory and Statistics to Visual Tracking of Shapes in Motion},
  publisher = {Springer},
  year = {2000},
  author = {Blake, Andrew and Isard, Michael},
  edition = {1},
  month = {August},
  comment = {CV_book},
  day = {23},
  file = {:papers\\2000 BOOK, Active Contours (Blake).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {3540762175},
  posted-at = {2010-06-17 14:52:30},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/3540762175}
}

@INPROCEEDINGS{1998_CNF_LabeledUnlabeled_BlumMitchell,
  author = {Blum, Avrim and Mitchell, Tom},
  title = {Combining labeled and unlabeled data with co-training},
  booktitle = {Proceedings of the 1998 11th Annual Conference on Computational Learning
	Theory, July 24, 1998 - July 26, 1998},
  year = {1998},
  abstract = {The problem of using a large unlabeled sample is considered to boost
	the performance of a learning algorithm when only a small set of
	labeled examples is available. In particular, a problem setting is
	considered to classify web pages, in which the description of each
	example can be partitioned into two distinct views. A PAC-style analysis
	for this setting, and, more broadly, a PAC-style framework for the
	general problem of learning from both labeled and unlabeled data
	are presented. Also, empirical results on real web-page is giving,
	indicating that this use of unlabeled examples can lead to significant
	improvement of hypotheses in practice.},
  comment = {PRML},
  file = {:papers\\1998 CNF, Combining labeled and unlabeled data with co-training (Blum, Mitchell, CLT, 1688).pdf:PDF},
  keywords = {Learning algorithms Computational linguistics Data processing Problem
	solving World Wide Web},
  owner = {salman},
  timestamp = {01,700}
}

@ARTICLE{1997_JNL_SURVEYprml_Blum,
  author = {Blum, A. L. and Langley, P.},
  title = {Selection of relevant features and examples in machine learning},
  journal = {Artificial Intelligence},
  year = {1997},
  volume = {97},
  pages = {245-71},
  number = {Copyright 1998, IEE},
  abstract = {In this survey, we review work in machine learning on methods for
	handling data sets containing large amounts of irrelevant information.
	We focus on two key issues: the problem of selecting relevant features,
	and the problem of selecting relevant examples. We describe the advances
	that have been made on these topics in both empirical and theoretical
	work in machine learning, and we present a general framework that
	we use to compare different methods. We close with some challenges
	for future work in this area},
  comment = {survey},
  file = {:papers\\1997 JNL, Selection of relevant features and examples in machine learning (Blum, Mitchell, AI, 1189).pdf:PDF},
  keywords = {learning by example},
  owner = {salman},
  timestamp = {01,200}
}

@TECHREPORT{1996_TECH_Kidsroom_Bobick,
  author = {Bobick Aaron F., Intille Stephen S., Davis James W. Baird Freedom,
	Pinhanez Claudio S., Campbell Lee W., Ivanov Yuri A., Schutte Arjan,
	Wilson Andrew},
  title = {The KidsRoom: A Perceptually-Based Interactive and Immersive Story
	Environment},
  institution = {M.I.T Media Laboratory Perceptual Computing Section},
  year = {1996},
  comment = {trk_people},
  file = {:papers\\1996 TECH, The KidsRoom_ A Perceptually-Based Interactive and Immersive Story Environment.pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{1996_CNF_AppearanceAction_Bobick,
  author = {Bobick, A. and Davis, J.},
  title = {An appearance-based representation of action},
  booktitle = {Proceedings of 13th International Conference on Pattern Recognition,
	25-29 Aug. 1996},
  year = {1996},
  abstract = {A new view-based approach to the representation of action is presented.
	Our underlying representations are view-based descriptions of the
	coarse image motion associated with viewing given actions from particular
	directions. Using these descriptions, we propose an appearance-based
	action-recognition strategy comprised of two stages: 1) a motion
	energy image (MEI) is computed that grossly describes the spatial
	distribution of motion energy for a given view of a given action,
	and the input MEI is matched against stored models which span the
	range of views of known actions; 2) any models that plausibly match
	the input are tested for a coarse, categorical agreement between
	a stored motion model of the action and a parametrization of the
	input motion. Using a sitting action as an example, and using a manually
	placed stick model, we develop a representation and verification
	technique that collapses the temporal variations of the motion parameters
	into a single, low-order vector},
  comment = {recog_action_appearance},
  file = {:papers\\1996 CNF, An appearance-based representation of action (Bobick, ICPR, 97).pdf:PDF},
  keywords = {computer vision image recognition image representation image sequences
	motion estimation},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{1999_JNL_Kidsroom_Bobick,
  author = {Bobick, A.F. and Intille, S.S. and Davis, J.W. and Baird, F. and
	Pinhanez, C.S. and Campbell, L.W. and Ivanov, Y.A. and Schutte, A.
	and Wilson, A.},
  title = {THE KIDSROOM: a perceptually-based interactive and immersive story
	environment},
  journal = {Presence},
  year = {1999},
  volume = { 8},
  pages = {369 - 93},
  abstract = {The KidsRoom is a perceptually-based, interactive, narrative playspace
	for children. Images, music, narration, light and sound effects are
	used to transform a normal child's bedroom into a fantasy land where
	children are guided through a reactive adventure story. The fully
	automated system was designed with the following goals: to keep the
	focus of user action and interaction in the physical and not virtual
	space; to permit multiple, collaborating people to simultaneously
	engage in an interactive experience combining both real and virtual
	objects; to use computer-vision algorithms to identify activity in
	the space without requiring the participants to wear any special
	clothing or devices; to use narrative to constrain the perceptual
	recognition, and to use perceptual recognition to allow participants
	to drive the narrative; and to create a truly immersive and interactive
	room environment. We believe the KidsRoom is the first multi-person,
	fully-automated, interactive, narrative environment ever constructed
	using non-encumbering sensors. This paper describes the KidsRoom,
	the technology that makes it work, and the issues that were raised
	during the system's development},
  address = {USA},
  comment = {trk_people},
  copyright = {Copyright 2000, IEE},
  file = {:papers\\1999 JNL, The KidsRoom_A perceptually-based interactive and immersive story environment (Bobick et al., 289, Presence).pdf:PDF},
  issn = {1054-7460},
  keywords = {computer vision;entertainment;home computing;music;virtual reality;},
  language = {English},
  timestamp = {00,300},
  url = {http://dx.doi.org/10.1162/105474699566297}
}

@ARTICLE{2001_JNL_MotionTemplates_Bobick,
  author = {Bobick, A. F. and Davis, J. W.},
  title = {The recognition of human movement using temporal templates},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2001},
  volume = {23},
  pages = {257-267},
  number = {3},
  abstract = {A view-based approach to the representation and recognition of human
	movement is presented. The basis of the representation is a temporal
	template-a static vector-image where the vector value at each point
	is a function of the motion properties at the corresponding spatial
	location in an image sequence. Using aerobics exercises as a test
	domain, we explore the representational power of a simple, two component
	version of the templates: The first value is a binary value indicating
	the presence of motion and the second value is a function of the
	recency of motion in a sequence. We then develop a recognition method
	matching temporal templates against stored instances of views of
	known actions. The method automatically performs temporal segmentation,
	is invariant to linear changes in speed, and runs in real-time on
	standard platforms},
  comment = {recog_action_shape},
  file = {:papers\\2001 JNL, The recognition of human movement as temporal templates (Bobick, PAMI, 725).pdf:PDF},
  keywords = {computer vision image motion analysis image reconstruction image sequences
	aerobics exercises human movement recognition human movement representation
	motion properties recognition method temporal segmentation temporal
	templates view-based approach},
  owner = {salman},
  timestamp = {00,700}
}

@INPROCEEDINGS{2005_CNF_DetectingIrregularities_Boiman,
  author = {Boiman, O. and Irani, M.},
  title = {Detecting irregularities in images and in video},
  booktitle = {Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference
	on},
  year = {2005},
  volume = {1},
  abstract = {We address the problem of detecting irregularities in visual data,
	e.g., detecting suspicious behaviors in video sequences, or identifying
	salient patterns in images. The term "irregular" depends on the context
	in which the "regular" or "valid" are defined. Yet, it is not realistic
	to expect explicit definition of all possible valid configurations
	for a given context. We pose the problem of determining the validity
	of visual data as a process of constructing a puzzle: We try to compose
	a new observed image region or a new video segment ("the query")
	using chunks of data ("pieces of puzzle") extracted from previous
	visual examples ("the database "). Regions in the observed data which
	can be composed using large contiguous chunks of data from the database
	are considered very likely, whereas regions in the observed data
	which cannot be composed from the database (or can be composed, but
	only using small fragmented pieces) are regarded as unlikely/suspicious.
	The problem is posed as an inference process in a probabilistic graphical
	model. We show applications of this approach to identifying saliency
	in images and video, and for suspicious behavior recognition.},
  comment = {recog_action_appearance},
  doi = {10.1109/ICCV.2005.70},
  file = {:papers\\2005 CNF, Detecting irregularities in images and in video (Boiman, Irani, ICCV, 140).pdf:PDF},
  issn = {1550-5499},
  keywords = { image irregularity detection; image pattern; image region; inference
	process; probabilistic graphical model; video segmentation; video
	sequence; visual data; feature extraction; image recognition; image
	sequences; video signal processing;},
  timestamp = {00,150}
}

@BOOK{2003_BOOK_Manifolds_Boothby,
  title = {An Introduction to Differentiable Manifolds and Riemannian Geometry,
	Revised, Volume 120, Second Edition (Pure and Applied Mathematics)},
  publisher = {Academic Press},
  year = {2002},
  author = {Boothby, William M.},
  edition = {2},
  comment = {math_book},
  day = {19},
  file = {:papers\\2003 BOOK, An Introduction to Differentiable Manifolds and Riemannian Geometry (Boothby, revised 2nd ed).pdf:PDF},
  howpublished = {Paperback},
  isbn = {0121160513},
  posted-at = {2011-03-10 14:48:51},
  priority = {2},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0121160513}
}

@INPROCEEDINGS{1992_CNF_TrgOptimalMarginClassifiers_Boser,
  author = {Boser, B. E. and Guyon, I. M. and Vapnik, V. N.},
  title = {A training algorithm for optimal margin classifiers},
  booktitle = {Proceedings of the Fifth Annual ACM Workshop on Computational Learning
	Theory, 27-29 July 1992},
  year = {1992},
  abstract = {A training algorithm that maximizes the margin between the training
	patterns and the decision boundary is presented. The technique is
	applicable to a wide variety of classification functions, including
	Perceptrons, polynomials, and Radial Basis Functions. The effective
	number of parameters is adjusted automatically to match the complexity
	of the problem. The solution is expressed as a linear combination
	of supporting patterns. These are the subset of training patterns
	that are closest to the decision boundary. Bounds on the generalization
	performance based on the leave-one-out method and the VC-dimension
	are given. Experimental results on optical character recognition
	problems demonstrate the good generalization obtained when compared
	with other learning algorithms},
  comment = {PRML},
  file = {:papers\\1992 CNF, A training algorithm for optimal margin classifiers (Boser, Guyon, Vapnik, CLT, 2799).pdf:PDF},
  keywords = {learning (artificial intelligence) pattern recognition},
  owner = {salman},
  timestamp = {03,000}
}

@ARTICLE{2001_JNL_EdgeDetectorEvaluation_Bowyer,
  author = {Bowyer, K. and Kranenburg, C. and Dougherty, S.},
  title = {Edge detector evaluation using empirical ROC curves},
  journal = {Computer Vision and Image Understanding},
  year = {2001},
  volume = {84},
  pages = {77-103},
  number = {Copyright 2002, IEE},
  abstract = {We demonstrate a method for evaluating edge detector performance based
	on receiver operating characteristic (ROC) curves. The edge detector
	output is matched against ground truth to count true positive and
	false positive edge pixels. The detector parameter settings are trained
	to give the best ROC curve on one image and then tested on separate
	images. We compute aggregate ROC curves based on 1 set of 50 object
	images and another set of 10 aerial images. We analyze the performance
	of 11 different edge detectors reported in the literature},
  comment = {feature_edge},
  file = {:papers\\2001 JNL, Edge Detector Evaluation Using Empirical ROC Curves (Bowyer, Kranenburg, Dougherty, CVIU, 107).pdf:PDF},
  keywords = {computer vision edge detection performance evaluation},
  owner = {salman},
  timestamp = {00,100}
}

@BOOK{2008_BOOK_OpenCV_Bradski,
  title = {Learning OpenCV: Computer Vision with the OpenCV Library},
  publisher = {O'Reilly Media},
  year = {2008},
  author = {Bradski, Gary and Kaehler, Adrian},
  abstract = {Learning OpenCV puts you right in the middle of the rapidly expanding
	field of computer vision. Written by the creators of OpenCV, the
	widely used free open- source library, this book introduces you to
	computer vision and demonstrates how you can quickly build applications
	that enable computers to "see" and make decisions based on the data.
	Computer vision is everywhere -- in security systems, manufacturing
	inspection systems, medical image analysis, Unmanned Aerial Vehicles,
	and more. It helps robot cars drive by themselves, stitches Google
	maps and Google Earth together, checks the pixels on your laptop's
	LCD screen, and makes sure the stitches in your shirt are OK. OpenCV
	provides an easy-to-use computer vision infrastructure along with
	a comprehensive library containing more than 500 functions that can
	run vision code in real time. With Learning OpenCV, any developer
	or hobbyist can get up and running with the framework quickly, whether
	it's to build simple or sophisticated vision applications. The book
	includes: A thorough introduction to OpenCV Getting input from cameras
	Transforming images Shape matching Pattern recognition, including
	face detection Segmenting images Tracking and motion in 2 and 3 dimensions
	Machine learning algorithms
	
	Hands-on exercises at the end of each chapter help you absorb the
	concepts, and an appendix explains how to set up an OpenCV project
	in Visual Studio. OpenCV is written in performance optimized C/C++
	code, runs on Windows, Linux, and Mac OS X, and is free for commercial
	and research use under a BSD license. Getting machines to see is
	a challenging but entertaining goal. If you're intrigued by the possibilities,
	Learning OpenCV gets you started onbuilding computer vision applications
	of your own.},
  comment = {CV_book},
  day = {24},
  file = {:papers\\2008 BOOK, Learning OpenCV (Bradski, Kaehler, OReilly).pdf:PDF},
  howpublished = {Paperback},
  isbn = {0596516134},
  posted-at = {2010-04-19 18:44:39},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0596516134}
}

@ARTICLE{1998_JNL_FaceObjectTracking_Bradski,
  author = {Gary R. Bradski},
  title = {Real Time Face and Object Tracking as a Component of a Perceptual
	User Interface},
  journal = {Applications of Computer Vision, IEEE Workshop on},
  year = {1998},
  volume = {0},
  pages = {214},
  address = {Los Alamitos, CA, USA},
  comment = {TRK},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ACV.1998.732882},
  publisher = {IEEE Computer Society},
  timestamp = {00,180}
}

@INPROCEEDINGS{1997_CNF_RecognizeHumanDynamics_Bregler,
  author = {Bregler, C.},
  title = {Learning and recognizing human dynamics in video sequences},
  booktitle = {Proceedings of IEEE Computer Society Conference on Computer Vision
	and Pattern Recognition},
  year = {1997},
  abstract = {This paper describes a probabilistic decomposition of human dynamics
	at multiple abstractions, and shows how to propagate hypotheses across
	space, time, and abstraction levels. Recognition in this framework
	is the succession of very general low level grouping mechanisms to
	increased specific and learned model based grouping techniques at
	higher levels. Hard decision thresholds are delayed and resolved
	by higher level statistical models and temporal context. Low-level
	primitives are areas of coherent motion found by EM clustering, mid-level
	categories are simple movements represented by dynamical systems,
	and high-level complex gestures are represented by Hidden Markov
	Models as successive phases of ample movements. We show how such
	a representation can be learned from training data, and apply It
	to the example of human gait recognition},
  comment = {recog_action_appearance},
  file = {:papers\\1997 CNF, Learning and recognizing human dynamics in video sequences (Bregler, CVPR, 509).pdf:PDF},
  keywords = {hidden Markov models image recognition image representation image
	sequences motion estimation},
  timestamp = {00,500}
}

@INPROCEEDINGS{2000_CNF_3DshapeImageStreams_Bregler,
  author = {Bregler, C. and Hertzmann, A. and Biermann, H.},
  title = {Recovering non-rigid 3D shape from image streams},
  booktitle = {Computer Vision and Pattern Recognition, 2000. Proceedings. IEEE
	Conference on},
  year = {2000},
  volume = {2},
  abstract = {The paper addresses the problem of recovering 3D non-rigid shape models
	from image sequences. For example, given a video recording of a talking
	person, we would like to estimate a 3D model of the lips and the
	full face and its internal modes of variation. Many solutions that
	recover 3D shape from 2D image sequences have been proposed; these
	so-called structure-from-motion techniques usually assume that the
	3D object is rigid. For example, C. Tomasi and T. Kanades' (1992)
	factorization technique is based on a rigid shape matrix, which produces
	a tracking matrix of rank 3 under orthographic projection. We propose
	a novel technique based on a non-rigid model, where the 3D shape
	in each frame is a linear combination of a set of basis shapes. Under
	this model, the tracking matrix is of higher rank, and can be factored
	in a three-step process to yield pose, configuration and shape. To
	the best of our knowledge, this is the first model free approach
	that can recover from single-view video sequences nonrigid shape
	models. We demonstrate this new algorithm on several video sequences.
	We were able to recover 3D non-rigid human face and animal models
	with high accuracy},
  comment = {3D},
  doi = {10.1109/CVPR.2000.854941},
  file = {:papers\\2000 CNF, Recovering non-rigid 3D shape from image streams (Bregler, Hertzmann, Biermann, CVPR, 238).pdf:PDF},
  keywords = {2D image sequences;3D model;3D non-rigid human face recovery;3D non-rigid
	shape models;animal models;basis shapes;image sequences;image streams;linear
	combination;model free approach;non-rigid 3D shape recovery;nonrigid
	shape models;single-view video sequences;structure-from-motion techniques;talking
	person;three-step process;tracking matrix;video recording;image reconstruction;image
	sequences;matrix algebra;video recording;},
  timestamp = {00,250}
}

@ARTICLE{1992_JNL_VBR_Breuel,
  author = {Breuel, T.M.},
  title = {View-based recognition},
  journal = {IAPR Workshop on Machine Vision Applications},
  year = {1992},
  pages = {29-32},
  comment = {TRK_subspace},
  file = {:papers\\1992 JNL, View-Based Recognition (Breuel).pdf:PDF},
  publisher = {Citeseer},
  timestamp = {00,020}
}

@ARTICLE{9938715,
  author = {Briassouli, A. and Mezaris, V. and Kompatsiaris, I.},
  title = {Color aided motion-segmentation and object tracking for video sequences
	semantic analysis},
  journal = {International Journal of Imaging Systems and Technology},
  year = {2007},
  volume = { 17},
  pages = {174 - 89},
  number = {3},
  note = {]},
  abstract = {The high rates at which digital multimedia is being generated and
	used makes it necessary to develop systems that can process it in
	an efficient manner. This can be achieved by extracting semantics
	from processing the video's low-level information. We present a novel
	algorithm which fuses color and motion information, in order to extract
	semantics from the video sequence. The motion estimates are processed
	statistically to give areas of activity in the video. Color segmentation
	is applied to these areas, and also to their complementary regions
	in each frame, in order to achieve the moving object segmentation.
	The extracted color layers in the activity and background areas are
	compared using the earth mover's distance (EMD), and a novel method,
	which we introduce, and which is based on a likelihood ratio test
	(LRT). The segmentation results of our LRT-based approach are shown
	to be more robust than the EMD results, and both methods are shown
	to be more accurate than the existing combined color-motion approaches.
	Furthermore, the LRT method allows the retrieval of additional semantics,
	namely of 'maps' that indicate with what likelihood a pixel belongs
	to a moving object. The areas of activity can be used to retrieve
	semantics for the kind of activity taking place. The color-aided
	segmentation of the moving entities provides a full description of
	their appearance, so it can be used, for example, to classify the
	video based on the objects in it. Experiments with real sequences
	show that this method leads to accurate results and useful semantics.
	&copy; 2007 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 17,
	174-189, 2007},
  address = {USA},
  comment = {trk_color},
  copyright = {Copyright 2008, The Institution of Engineering and Technology},
  file = {:papers\\2007 JNL, Color aided motion-segmentation and object tracking for video sequences semantic analysis (Briassouli, Mezaris, Kompatsiaris).pdf:PDF},
  issn = {0899-9457},
  keywords = {image colour analysis;image segmentation;image sequences;motion estimation;multimedia
	systems;object detection;video signal processing;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1002/ima.20113}
}

@ARTICLE{1986_JNL_ObjectMotionNoisyImage_Broida,
  author = {Broida, Ted J. and Chellappa, Rama},
  title = {Estimation of Object Motion Parameters from Noisy Images},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1986},
  volume = {PAMI-8},
  pages = {90-99},
  number = {1},
  abstract = {An approach is presented for the estimation of object motion parameters
	based on a sequence of noisy images. The problem considered is that
	of a rigid body undergoing unknown rotational and translational motion.
	The measurement data consists of a sequence of noisy image coordinates
	of two or more object correspondence points. By modeling the object
	dynamics as a function of time, estimates of the model parameters
	(including motion parameters) can be extracted from the data using
	recursive and/or batch techniques. This permits a desired degree
	of smoothing to be achieved through the use of an arbitrarily large
	number of images. Some assumptions regarding object structure are
	presently made. Results are presented for a recursive estimation
	procedure: the case considered here is that of a sequence of one
	dimensional images of a two dimensional object. Thus, the object
	moves in one transverse dimension, and in depth, preserving the fundamental
	ambiguity of the central projection image model (loss of depth information).
	An iterated extended Kalman filter is used for the recursive solution.
	Noise levels of 5-10 percent of the object image size are used. Approximate
	Cramer-Rao lower bounds are derived for the model parameter estimates
	as a function of object trajectory and noise level. This approach
	may be of use in situations where it is difficult to resolve large
	numbers of object match points, but relatively long sequences of
	images (10 to 20 or more) are available.},
  comment = {motion},
  file = {:papers\\1986 JNL, Estimation of object motion parameters from noisy images (Broida, Chellappa, PAMI, 284).pdf:PDF},
  owner = {salman},
  timestamp = {00,300}
}

@INPROCEEDINGS{2006_CNF_TRKhuman_Brostow,
  author = {Brostow, G.J. and Cipolla, R.},
  title = {Unsupervised Bayesian Detection of Independent Motion in Crowds},
  booktitle = {Computer Vision and Pattern Recognition, 2006 IEEE Computer Society
	Conference on},
  year = {2006},
  volume = {1},
  pages = { 594 - 601},
  abstract = { While crowds of various subjects may offer applicationspecific cues
	to detect individuals, we demonstrate that for the general case,
	motion itself contains more information than previously exploited.
	This paper describes an unsupervised data driven Bayesian clustering
	algorithm which has detection of individual entities as its primary
	goal. We track simple image features and probabilistically group
	them into clusters representing independently moving entities. The
	numbers of clusters and the grouping of constituent features are
	determined without supervised learning or any subject-specific model.
	The new approach is instead, that space-time proximity and trajectory
	coherence through image space are used as the only probabilistic
	criteria for clustering. An important contribution of this work is
	how these criteria are used to perform a one-shot data association
	without iterating through combinatorial hypotheses of cluster assignments.
	Our proposed general detection algorithm can be augmented with subject-specific
	filtering, but is shown to already be effective at detecting individual
	entities in crowds of people, insects, and animals. This paper and
	the associated video examine the implementation and experiments of
	our motion clustering framework.},
  comment = {trk_people},
  file = {:papers\\2006 CNF, Unsupervised Bayesian Detection of Independent Motion in Crowds (Brostow, Cipolla).pdf:PDF},
  issn = {1063-6919},
  timestamp = {00,060}
}

@ARTICLE{1992_JNL_SURVEYreg_Brown,
  author = {Brown, L.G.},
  title = {A survey of image registration techniques},
  journal = {Computing Surveys},
  year = {1992},
  volume = { 24},
  pages = {325 - 76},
  number = {4},
  abstract = {Registration is a fundamental task in image processing used to match
	two or more pictures taken, for example, at different times, from
	different sensors, or from different viewpoints. Over the years,
	a broad range of techniques has been developed for various types
	of data and problems. These techniques have been independently studied
	for several different applications, resulting in a large body of
	research. The author organizes this material by establishing the
	relationship between the variations in the images and the type of
	registration techniques which can most appropriately be applied.
	Three major types of variations are distinguished. The first type
	are the variations due to the differences in acquisition which cause
	the images to be misaligned. To register images, a spatial transformation
	is found which will remove these variations. The class of transformations
	which must be searched to find the optimal transformation is determined
	by knowledge about the variations of this type. The transformation
	class in turn influences the general technique that should be taken.
	The second type of variations are those which are also due to differences
	in acquisition, but cannot be modeled easily such as lighting and
	atmospheric conditions. This type usually effects intensity values,
	but they may also be spatial, such as perspective distortions. The
	third type of variations are differences in the images that are of
	interest such as object movements, growths, or other scene changes.
	Knowledge about the characteristics of each type of variation affect
	the choice of feature space, similarity measure, search space, and
	search strategy which will make up the final technique},
  address = {USA},
  comment = {survey},
  copyright = {Copyright 1993, IEE},
  file = {:papers\\1992 JNL, A survey of image registration techniques (Brown).pdf:PDF},
  issn = {0360-0300},
  keywords = {image processing;image recognition;},
  language = {English},
  timestamp = {02,500},
  url = {http://dx.doi.org/10.1145/146370.146374}
}

@ARTICLE{2003_JNL_SurveyStereo_Brown,
  author = {Brown, M.Z. and Burschka, D. and Hager, G.D.},
  title = {Advances in computational stereo},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2003},
  volume = {25},
  pages = { 993 - 1008},
  number = {8},
  month = {aug.},
  abstract = { Extraction of three-dimensional structure of a scene from stereo
	images is a problem that has been studied by the computer vision
	community for decades. Early work focused on the fundamentals of
	image correspondence and stereo geometry. Stereo research has matured
	significantly throughout the years and many advances in computational
	stereo continue to be made, allowing stereo to be applied to new
	and more demanding problems. We review recent advances in computational
	stereo, focusing primarily on three important topics: correspondence
	methods, methods for occlusion, and real-time implementations. Throughout,
	we present tables that summarize and draw distinctions among key
	ideas and approaches. Where available, we provide comparative analyses
	and we make suggestions for analyses yet to be done.},
  comment = {survey, 3D_stereo},
  doi = {10.1109/TPAMI.2003.1217603},
  file = {:papers\\2003 JNL, Advances in computational stereo (SURVEY, Brown, Burschka, Hager).pdf:PDF},
  issn = {0162-8828},
  keywords = { computational stereo; computer vision; image correspondence; occlusion;
	real-time implementations; stereo correspondence; stereo geometry;
	stereo images; three-dimensional structure extraction; computer vision;
	image matching; real-time systems; stereo image processing;},
  timestamp = {00,450}
}

@ARTICLE{2010_JNL_RegionMotion3Dtracking_Brox,
  author = {Brox, T. and Rosenhahn, B. and Gall, J. and Cremers, D.},
  title = {Combined Region and Motion-Based 3D Tracking of Rigid and Articulated
	Objects},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2010},
  volume = {32},
  pages = {402 -415},
  number = {3},
  abstract = {In this paper, we propose the combined use of complementary concepts
	for 3D tracking: region fitting on one side and dense optical flow
	as well as tracked SIFT features on the other. Both concepts are
	chosen such that they can compensate for the shortcomings of each
	other. While tracking by the object region can prevent the accumulation
	of errors, optical flow and SIFT can handle larger transformations.
	Whereas segmentation works best in case of homogeneous objects, optical
	flow computation and SIFT tracking rely on sufficiently structured
	objects. We show that a sensible combination yields a general tracking
	system that can be applied in a large variety of scenarios without
	the need to manually adjust weighting parameters.},
  comment = {trk_region_motion},
  doi = {10.1109/TPAMI.2009.32},
  file = {:papers\\2010 JNL, Combined Region and Motion-Based 3D Tracking of Rigid and Articulated Objects.pdf:PDF},
  issn = {0162-8828},
  keywords = {SIFT feature;SIFT tracking;articulated object;motion-based 3D tracking;object
	region tracking;object segmentation;optical flow;region fitting;rigid
	object;image motion analysis;image segmentation;image sequences;object
	detection;optical tracking;},
  timestamp = {-}
}

@ARTICLE{1997_JNL_ComparisonBayesianDS_Buede,
  author = {Buede, D. M. and Girardi, P.},
  title = {A target identification comparison of Bayesian and Dempster-Shafer
	multisensor fusion},
  journal = {Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions
	on},
  year = {1997},
  volume = {27},
  pages = {569-577},
  number = {5},
  comment = {PRML_inference},
  file = {:papers\\1997 JNL, A target identification comparison of Bayesian and Dempster-Shafer multisensor (Buede, Girardi, TSMC, 62).pdf:PDF},
  keywords = {Bayes methods aircraft case-based reasoning convergence object recognition
	probability radar target recognition sensor fusion uncertainty handling
	Bayesian reasoning Dempster-Shafer theory aircraft recognition convergence
	time evidence theory evidential reasoning multisensor fusion target
	identification},
  owner = {salman},
  timestamp = {00,050}
}

@MISC{2009_WEB_CensusBureau,
  author = {Bureau, US Census},
  title = {US Census Bureau},
  year = {2009},
  comment = {misc},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{1998_JNL_SVM_Burges,
  author = {Burges, C. J. C.},
  title = {A tutorial on support vector machines for pattern recognition},
  journal = {Data Mining and Knowledge Discovery},
  year = {1998},
  volume = {2},
  pages = {121-67},
  abstract = {The article starts with an overview of the concepts of VC dimension
	and structural risk minimization. We then describe linear support
	vector machines (SVMs) for separable and non separable data, working
	through a non trivial example in detail. We describe a mechanical
	analogy, and discuss when SVM solutions are unique and when they
	are global. We describe how support vector training can be practically
	implemented, and discuss in detail the kernel mapping technique which
	is used to construct SVM solutions which are nonlinear in the data.
	We show how support vector machines can have very large (even infinite)
	VC dimension by computing the VC dimension for homogeneous polynomial
	and Gaussian radial basis function kernels. While very high VC dimension
	would normally bode ill for generalization performance, and while
	at present there exists no theory which shows that good generalization
	performance is guaranteed for SVMs, there are several arguments which
	support the observed high accuracy of SVMs, which we review. Results
	of some experiments which were inspired by these arguments are also
	presented. We give numerous examples and proofs of most of the key
	theorems},
  comment = {tutorial},
  file = {:papers\\1998 JNL, A tutorial on support vector machines for pattern recognition (C. J. C. Burges, DMKD, 6583).pdf:PDF},
  keywords = {learning (artificial intelligence) neural nets pattern recognition
	vector processor systems},
  owner = {salman},
  timestamp = {06,500}
}

@ARTICLE{2003_JNL_SURVEYiu_Buxton,
  author = {Buxton, H.},
  title = {Learning and understanding dynamic scene activity: a review},
  journal = {Image and Vision Computing},
  year = {2003},
  volume = {21},
  pages = {125-36},
  abstract = {We are entering an era of more intelligent cognitive vision systems.
	Such systems can analyse activity in dynamic scenes to compute conceptual
	descriptions from motion trajectories of moving people and the objects
	they interact with. Here we review progress in the development of
	flexible, generative models that can explain visual input as a combination
	of hidden variables and can adapt to new types of input. Such models
	are particularly appropriate for the tasks posed by cognitive vision
	as they incorporate learning as well as having sufficient structure
	to represent a general class of problems. In addition, generative
	models explain all aspects of the input rather than attempting to
	ignore irrelevant sources of variation as in exemplar-based learning.
	Applications of these models in visual interaction for education,
	smart rooms and cars, as well as surveillance systems is also briefly
	reviewed},
  comment = {survey},
  file = {:papers\\2003 JNL, Learning and understanding dynamic scene activity_ a review (SURVEY, Buxton, IVC, 86).pdf:PDF},
  keywords = {bibliographies cognitive systems computer graphics computer vision
	educational computing explanation learning systems motion estimation
	surveillance},
  owner = {salman},
  timestamp = {00,080}
}

@ARTICLE{1980_JNL_TSVQ_Buzo,
  author = {Buzo, A. and Gray, A., Jr. and Gray, R. and Markel, J.},
  title = {Speech coding based upon vector quantization},
  journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  year = {1980},
  volume = {28},
  pages = { 562 - 574},
  number = {5},
  month = {oct},
  abstract = { With rare exception, all presently available narrow-band speech coding
	systems implement scalar quantization (independent quantization)
	of the transmission parameters (such as reflection coefficients or
	transformed reflection coefficients in LPC systems). This paper presents
	a new approach called vector quantization. For very low data rates,
	realistic experiments have shown that vector quantization can achieve
	a given level of average distortion with 15 to 20 fewer bits/frame
	than that required for the optimized scalar quantizing approaches
	presently in use. The vector quantizing approach is shown to be a
	mathematically and computationally tractable method which builds
	upon knowledge obtained in linear prediction analysis studies. This
	paper introduces the theory in a nonrigorous form, along with practical
	results to date and an extensive list of research topics for this
	new area of speech coding.},
  comment = {VQ},
  file = {:papers\\1980 JNL, Speech coding based upon vector quantization (Buzo, Gray, Gray, Markel).pdf:PDF},
  issn = {0096-3518},
  review = {salman: Introduction of TSVQ},
  timestamp = {00,400}
}

@ARTICLE{1999_JNL_Tracking_StructuredEnvironmentsDistributed_CaiAggarwal,
  author = {Cai, Q. and Aggarwal, J.K.},
  title = {Tracking human motion in structured environments using a distributed-camera
	system},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1999},
  volume = {21},
  pages = {1241 -1247},
  number = {11},
  month = {nov},
  abstract = {This paper presents a comprehensive framework for tracking coarse
	human models from sequences of synchronized monocular grayscale images
	in multiple camera coordinates. It demonstrates the feasibility of
	an end-to-end person tracking system using a unique combination of
	motion analysis on 3D geometry in different camera coordinates and
	other existing techniques in motion detection, segmentation, and
	pattern recognition. The system starts with tracking from a single
	camera view. When the system predicts that the active camera will
	no longer have a good view of the subject of interest, tracking will
	be switched to another camera which provides a better view and requires
	the least switching to continue tracking. The nonrigidity of the
	human body is addressed by matching points of the middle line of
	the human image, spatially and temporally, using Bayesian classification
	schemes. Multivariate normal distributions are employed to model
	class-conditional densities of the features for tracking, such as
	location, intensity, and geometric features. Limited degrees of occlusion
	are tolerated within the system. Experimental results using a prototype
	system are presented and the performance of the algorithm is evaluated
	to demonstrate its feasibility for real time applications},
  comment = {trk_people},
  doi = {10.1109/34.809119},
  file = {:papers\\1999 JNL, Tracking human motion in structured environments using a distributed-camera system (Cai, Aggarwal, PAMI, 185).pdf:PDF},
  issn = {0162-8828},
  keywords = {3D geometry;Bayesian classification schemes;class-conditional densities;coarse
	human models;distributed-camera system;end-to-end person tracking
	system;geometric features;human body nonrigidity;human motion tracking;intensity;motion
	analysis;motion detection;multiple camera coordinates;multivariate
	normal distributions;occlusion;pattern recognition;real time applications;segmentation;spatial
	point matching;structured environments;synchronized monocular grayscale
	image sequences;temporal point matching;Bayes methods;computer vision;image
	matching;image motion analysis;object recognition;optical tracking;},
  timestamp = {00,200}
}

@ARTICLE{2010_JNL_SURVEY_beh_Candamo,
  author = {Candamo, J. and Shreve, M. and Goldgof, D.B. and Sapper, D.B. and
	Kasturi, R.},
  title = {Understanding Transit Scenes: A Survey on Human Behavior-Recognition
	Algorithms},
  journal = {Intelligent Transportation Systems, IEEE Transactions on},
  year = {2010},
  volume = {11},
  pages = {206 -224},
  number = {1},
  month = march,
  abstract = {Visual surveillance is an active research topic in image processing.
	Transit systems are actively seeking new or improved ways to use
	technology to deter and respond to accidents, crime, suspicious activities,
	terrorism, and vandalism. Human behavior-recognition algorithms can
	be used proactively for prevention of incidents or reactively for
	investigation after the fact. This paper describes the current state-of-the-art
	image-processing methods for automatic-behavior-recognition techniques,
	with focus on the surveillance of human activities in the context
	of transit applications. The main goal of this survey is to provide
	researchers in the field with a summary of progress achieved to date
	and to help identify areas where further research is needed. This
	paper provides a thorough description of the research on relevant
	human behavior-recognition methods for transit surveillance. Recognition
	methods include single person (e.g., loitering), multiple-person
	interactions (e.g., fighting and personal attacks), person-vehicle
	interactions (e.g., vehicle vandalism), and person-facility/location
	interactions (e.g., object left behind and trespassing). A list of
	relevant behavior-recognition papers is presented, including behaviors,
	data sets, implementation details, and results. In addition, algorithm's
	weaknesses, potential research directions, and contrast with commercial
	capabilities as advertised by manufacturers are discussed. This paper
	also provides a summary of literature surveys and developments of
	the core technologies (i.e., low-level processing techniques) used
	in visual surveillance systems, including motion detection, classification
	of moving objects, and tracking.},
  comment = {SURVEY_beh},
  doi = {10.1109/TITS.2009.2030963},
  issn = {1524-9050},
  keywords = {human activities surveillance;human behavior recognition algorithms;image
	processing;motion detection;moving objects classification;multiple-person
	interactions;person-facility-location interactions;person-vehicle
	interactions;single person;transit systems;visual surveillance;behavioural
	sciences computing;image recognition;traffic engineering computing;video
	surveillance;},
  timestamp = {-}
}

@TECHREPORT{2008_REP_SURVEYtrk_Cannons,
  author = {Cannons, K.},
  title = {A review of visual tracking},
  institution = {Technical Report CSE-2008-07, York University, Department of Computer
	Science and Engineering},
  year = {2008},
  comment = {TRK_survey},
  file = {:papers\\2008 TECH, A review of visual tracking (Cannons).pdf:PDF},
  timestamp = {-}
}

@ARTICLE{1986_JNL_Edges_Canny,
  author = {Canny, John},
  title = {A Computational Approach to Edge Detection},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1986},
  volume = {PAMI-8},
  pages = {679 -698},
  number = {6},
  month = {nov. },
  abstract = {This paper describes a computational approach to edge detection. The
	success of the approach depends on the definition of a comprehensive
	set of goals for the computation of edge points. These goals must
	be precise enough to delimit the desired behavior of the detector
	while making minimal assumptions about the form of the solution.
	We define detection and localization criteria for a class of edges,
	and present mathematical forms for these criteria as functionals
	on the operator impulse response. A third criterion is then added
	to ensure that the detector has only one response to a single edge.
	We use the criteria in numerical optimization to derive detectors
	for several common image features, including step edges. On specializing
	the analysis to step edges, we find that there is a natural uncertainty
	principle between detection and localization performance, which are
	the two main goals. With this principle we derive a single operator
	shape which is optimal at any scale. The optimal detector has a simple
	approximate implementation in which edges are marked at maxima in
	gradient magnitude of a Gaussian-smoothed image. We extend this simple
	detector using operators of several widths to cope with different
	signal-to-noise ratios in the image. We present a general method,
	called feature synthesis, for the fine-to-coarse integration of information
	from operators at different scales. Finally we show that step edge
	detector performance improves considerably as the operator point
	spread function is extended along the edge.},
  comment = {feature_edge},
  doi = {10.1109/TPAMI.1986.4767851},
  file = {:papers\\1986 JNL, A computational approach to edge detection (Canny, PAMI, 9666).pdf:PDF},
  issn = {0162-8828},
  timestamp = {10,000}
}

@ARTICLE{1992_JNL_MCMC_Carlin,
  author = {Carlin, B.P. and Polson, N.G. and Stoffer, D.S.},
  title = {A Monte Carlo approach to nonnormal and nonlinear state-space modeling},
  journal = {Journal of the American Statistical Association},
  year = {1992},
  volume = {87},
  pages = {493--500},
  number = {418},
  comment = {00,450},
  file = {:papers\\1992_JNL_MCMC_Carlin.pdf:PDF},
  publisher = {JSTOR},
  timestamp = {TRK_particle}
}

@BOOK{citeulike:1923961,
  title = {Riemannian Geometry},
  publisher = {Birkh\"{a}user Boston},
  year = {1992},
  author = {do Carmo, Manfredo P.},
  edition = {1},
  month = jan,
  abstract = {{This text has been adopted at: University of Pennsylvania, Philadelphia
	University of Connecticut, Storrs Duke University, Durham, NC California
	Institute of Technology, Pasadena University of Washington, Seattle
	Swarthmore College, Swarthmore, PA University of Chicago, IL University
	of Michigan, Ann Arbor "In the reviewer's opinion, this is a superb
	book which makes learning a real pleasure." ne de Mathematiques Pures
	et Appliquees "This main-stream presentation of differential geometry
	serves well for a course on Riemannian geometry, and it is complemented
	by many annotated exercises." F. Mathematik "This is one of the best
	(if even not just the best) book for those who want to get a good,
	smooth and quick, but yet thorough introduction to modern Riemannian
	geometry." es Mathematicae Contents: Differential Manifolds * Riemannian
	Metrics * Affine Connections; Riemannian Connections * Geodesics;
	Convex Neighborhoods * Curvature * Jacobi Fields * Isometric Immersions
	* Complete Manifolds; Hopf-Rinow and Hadamard Theorems * Spaces of
	Constant Curvature * Variations of Energy * The Rauch Comparison
	Theorem * The Morse Index Theorem * The Fundamental Group of Manifolds
	of Negative Curvature * The Sphere Theorem * Index Series: Mathematics:
	Theory and Applications}},
  citeulike-article-id = {1923961},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0817634908},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0817634908},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0817634908},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0817634908},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0817634908/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0817634908},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0817634908},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0817634908},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0817634908\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0817634908},
  comment = {math_book},
  day = {01},
  file = {:papers\\1992 BOOK, Riemannian Geometry (M. doCarmo).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0817634908},
  posted-at = {2011-03-10 14:56:56},
  priority = {2},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0817634908}
}

@INPROCEEDINGS{1999_CNF_PF_carpenter,
  author = {Carpenter, J. and Clifford, P. and Fearnhead, P.},
  title = {Improved particle filter for nonlinear problems},
  booktitle = {Radar, Sonar and Navigation, IEE Proceedings-},
  year = {1999},
  volume = {146},
  number = {1},
  pages = {2--7},
  organization = {IET}
}

@INPROCEEDINGS{1994_CNF_color_Carron,
  author = {Carron, T. and Lambert, P.},
  title = {Color edge detector using jointly hue, saturation and intensity},
  booktitle = {Image Processing, 1994. Proceedings. ICIP-94., IEEE International
	Conference},
  year = {1994},
  volume = {3},
  pages = {977 -981 vol.3},
  month = nov,
  abstract = {In the hue saturation intensity (HSI) space, a hue difference taking
	into account the hue relevance is defined. A hue gradient operator
	is build up with this hue difference. Two color edge detectors are
	then proposed. These detectors mix in a different way H,S,I gradients.
	Experimental results are presented},
  comment = {TRK_color},
  file = {:papers\\1994 CNF, Color edge detector using jointly hue, saturation and intensity (Carron).pdf:PDF},
  keywords = { color edge detector; experimental results; hue difference; hue gradient
	operator; hue relevance; hue saturation intensity space; intensity
	gradient; saturation gradient; edge detection; image colour analysis;
	image processing;},
  timestamp = {00,106}
}

@ARTICLE{1992_JNL_GibbsSampler_Casella,
  author = {Casella, George and George, Edward I.},
  title = {Explaining the Gibbs Sampler},
  journal = {The American Statistician},
  year = {1992},
  volume = {46},
  pages = {167--174},
  number = {3},
  abstract = {Computer-intensive algorithms, such as the Gibbs sampler, have become
	increasingly popular statistical tools, both in applied and theoretical
	work. The properties of such algorithms, however, may sometimes not
	be obvious. Here we give a simple explanation of how and why the
	Gibbs sampler works. We analytically establish its properties in
	a simple case and provide insight for more complicated cases. There
	are also a number of examples.},
  comment = {tutorial},
  copyright = {Copyright  1992 American Statistical Association},
  file = {:papers\\1992 JNL, Explaining the Gibbs Sampler (Casella, ASA, 1176).pdf:PDF},
  issn = {00031305},
  jstor_articletype = {primary_article},
  jstor_formatteddate = {Aug., 1992},
  publisher = {American Statistical Association},
  timestamp = {01,000},
  url = {http://www.jstor.org/stable/2685208}
}

@INPROCEEDINGS{1995_CNF_GeodesicActiveContours_Caselles,
  author = {Caselles, V. and Kimmel, R. and Sapiro, G.},
  title = {Geodesic active contours},
  booktitle = {Computer Vision, 1995. Proceedings., Fifth International Conference
	on},
  year = {1995},
  pages = {694 -699},
  abstract = {A novel scheme for the detection of object boundaries is presented.
	The technique is based on active contours deforming according to
	intrinsic geometric measures of the image. The evolving contours
	naturally split and merge, allowing the simultaneous detection of
	several objects and both interior and exterior boundaries. The proposed
	approach is based on the relation between active contours and the
	computation of geodesics or minimal distance curves. The minimal
	distance curve lays in a Riemannian space whose metric as defined
	by the image content. This geodesic approach for object segmentation
	allows to connect classical ldquo;snakes rdquo; based on energy minimization
	and geometric active contours based on the theory of curve evolution.
	Previous models of geometric active contours are improved as showed
	by a number of examples. Formal results concerning existence, uniqueness,
	stability, and correctness of the evolution are presented as well},
  comment = {feature_contour},
  doi = {10.1109/ICCV.1995.466871},
  file = {:papers\\1995 CNF, Geodesic active contours (Caselles, Kimmel, Sapiro, ICCV, 2333).pdf:PDF},
  keywords = { Riemannian space; active contour deformation; classical snakes; curve
	evolution; energy minimization; evolving contours; exterior boundaries;
	geodesic active contours; geodesics; geometric active contours; image;
	image content; interior boundaries; intrinsic geometric measures;
	metric; minimal distance curves; object boundary detection; object
	detection; object segmentation; active vision; computer vision; geodesy;
	image segmentation; minimisation; object detection;},
  timestamp = {02,500}
}

@ARTICLE{2000_JNL_MovementAndMind_Castelli,
  author = {Fulvia Castelli and Francesca Happ and Uta Frith and Chris Frith},
  title = {Movement and Mind: A Functional Imaging Study of Perception and Interpretation
	of Complex Intentional Movement Patterns},
  journal = {NeuroImage},
  year = {2000},
  volume = {12},
  pages = {314 - 325},
  number = {3},
  abstract = {We report a functional neuroimaging study with positron emission tomography
	(PET) in which six healthy adult volunteers were scanned while watching
	silent computer-presented animations. The characters in the animations
	were simple geometrical shapes whose movement patterns selectively
	evoked mental state attribution or simple action description. Results
	showed increased activation in association with mental state attribution
	in four main regions: medial prefrontal cortex, temporoparietal junction
	(superior temporal sulcus), basal temporal regions (fusiform gyrus
	and temporal poles adjacent to the amygdala), and extrastriate cortex
	(occipital gyrus). Previous imaging studies have implicated these
	regions in self-monitoring, in the perception of biological motion,
	and in the attribution of mental states using verbal stimuli or visual
	depictions of the human form. We suggest that these regions form
	a network for processing information about intentions, and speculate
	that the ability to make inferences about other people's mental states
	evolved from the ability to make inferences about other creatures'
	actions.},
  comment = {motion_biology},
  doi = {DOI: 10.1006/nimg.2000.0612},
  file = {:papers\\2000 JNL, Movement and mind_ a functional imaging study of perception and interpretation of complex intentional movement patterns (Castelli, Happe, Frith, Frith).pdf:PDF},
  issn = {1053-8119},
  keywords = {brain imaging; theory of mind; mentalizing; biological motion; autism},
  timestamp = {00,450},
  url = {http://www.sciencedirect.com/science/article/B6WNP-45C0TMS-1N/2/addcf23c2a404ce1d43e11b9cf4d23f0}
}

@ARTICLE{1995_JNL_SURVEYmotion_Cedras,
  author = {Cedras, Claudette and Shah, Mubarak},
  title = {Motion-based recognition: a survey},
  journal = {Image and Vision Computing},
  year = {1995},
  volume = {13},
  pages = {129-155},
  abstract = {Motion-based recognition deals with the recognition of an object or
	its motion based on motion in a sequence of images. In this approach,
	a sequence containing a large number of frames is used to extract
	motion information. The advantage is that a longer sequence leads
	to recognition of higher level motions, like walking or running,
	which consists of a complex and coordinated series of events that
	cannot be understood by looking at only a few frames. This paper
	provides a review of recent developments in the computer vision aspect
	of motion-based recognition. We will identify two main steps in motion-based
	recognition. The first step is the extraction of motion information
	and its organization into motion models. The second step consists
	of the matching of some unknown input with a constructed model. Several
	methods for the recognition of objects and motions will then be reported.
	They include methods such as cyclic motion detection and recognition,
	lipreading, hand gestures interpretation, motion verb recognition
	and temporal textures classification. Tracking and recognition of
	human motion, like walking, skipping and running will also be discussed.
	Finally, we will conclude the paper with some thoughts about future
	directions for motion-based recognition.},
  comment = {survey},
  file = {:papers\\1995 JNL, Motion-based recognition a survey (SURVEY, Cedras, Mubarak Shah, IVC, 281).pdf:PDF},
  keywords = {Computer vision Classification (of information) Feature extraction
	Image processing Object recognition Three dimensional},
  owner = {salman},
  timestamp = {00,300}
}

@ARTICLE{2002_JNL_ViewInvariantActions_Cen,
  author = {Cen, Rao and Yilmaz, A. and Shah, M.},
  title = {View-invariant representation and recognition of actions},
  journal = {International Journal of Computer Vision},
  year = {2002},
  volume = {50},
  pages = {203-26},
  abstract = {Analysis of human perception of motion shows that information for
	representing the motion is obtained from the dramatic changes in
	the speed and direction of the trajectory. In this paper, we present
	a computational representation of human action to capture these dramatic
	changes using spatio-temporal curvature of 2-D trajectory. This representation
	is compact, view-invariant, and is capable of explaining an action
	in terms of meaningful action units called dynamic instants and intervals.
	A dynamic instant is an instantaneous entity that occurs for only
	one frame, and represents an important change in the motion characteristics.
	An interval represents the time period between two dynamic instants
	during which the motion characteristics do not change. Starting without
	a model, we use this representation for recognition and incremental
	learning of human actions. The proposed method can discover instances
	of the same action performed by different people from different view
	points. Experiments on 47 actions performed by 7 individuals in an
	environment with no constraints shows the robustness of the proposed
	method},
  comment = {recog_action},
  file = {:papers\\2002 JNL, View-invariant representation and recognition of actions (Mubark Shah, IJCV, 180).pdf:PDF},
  keywords = {gesture recognition image matching image representation motion estimation},
  owner = {salman},
  timestamp = {00,200}
}

@INPROCEEDINGS{1999_CNF_MultipleHypothesisFigureTracking_Cham,
  author = {Cham, Tat-Jen and Rehg, J. M.},
  title = {A multiple hypothesis approach to figure tracking},
  booktitle = {Computer Vision and Pattern Recognition, 1999. IEEE Computer Society
	Conference on.},
  year = {1999},
  volume = {2},
  comment = {trk_people},
  file = {:papers\\1999 CNF, A multiple hypothesis approach to figure tracking (Cham, Rehg, CVPR, 299).pdf:PDF},
  keywords = {computer vision object recognition optimisation probability state-space
	methods tracking figure tracking highly articulated objects tracking
	local optimization movie dance sequence multiple hypothesis approach
	piecewise Gaussians probabilistic multiple-hypothesis framework probability
	density state-space search temporal evolution},
  owner = {salman},
  timestamp = {00,300}
}

@ARTICLE{2011_JNL_libSVM_Chang,
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  title = {{LIBSVM}: A library for support vector machines},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  year = {2011},
  volume = {2},
  pages = {27:1--27:27},
  note = {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}},
  issue = {3}
}

@ARTICLE{1998_JNL_VideoQ_Chang,
  author = {Chang, Shih-Fu and Chen, W. and Meng, H.J. and Sundaram, H. and Di
	Zhong},
  title = {A fully automated content-based video search engine supporting spatiotemporal
	queries},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  year = {1998},
  volume = {8},
  pages = {602 -615},
  number = {5},
  month = {sep},
  abstract = {The rapidity with which digital information, particularly video, is
	being generated has necessitated the development of tools for efficient
	search of these media. Content-based visual queries have been primarily
	focused on still image retrieval. In this paper, we propose a novel,
	interactive system on the Web, based on the visual paradigm, with
	spatiotemporal attributes playing a key role in video retrieval.
	We have developed innovative algorithms for automated video object
	segmentation and tracking, and use real-time video editing techniques
	while responding to user queries. The resulting system, called VideoQ
	, is the first on-line video search engine supporting automatic object-based
	indexing and spatiotemporal queries. The system performs well, with
	the user being able to retrieve complex video clips such as those
	of skiers and baseball players with ease},
  comment = {analysis_retrieval},
  doi = {10.1109/76.718507},
  file = {:papers\\1998 JNL, A fully automated content-based video search engine supporting spatiotemporal queries (Chang, Chen, Meng, Sundaram, Zhong).pdf:PDF},
  issn = {1051-8215},
  keywords = {VideoQ;World Wide Web;algorithms;automated content-based video search
	engine;automated video object segmentation;automated video object
	tracking;automatic object-based indexing;content-based visual queries;digital
	information;interactive system;on-line video search engine;real-time
	video editing;spatiotemporal queries;still image retrieval;user queries;video
	databases;video retrieval;visual paradigm;image segmentation;indexing;interactive
	video;online front-ends;query processing;tracking;video signal processing;visual
	databases;},
  timestamp = {00,730}
}

@INPROCEEDINGS{1998_CNF_VideoQ_Chang,
  author = {Chang, Shih-Fu and Chen, W. and Sundaram, H.},
  title = {VideoQ: a fully automated video retrieval system using motion sketches},
  booktitle = {Applications of Computer Vision, 1998. WACV '98. Proceedings., Fourth
	IEEE Workshop on},
  year = {1998},
  pages = {270 -271},
  month = {oct},
  abstract = {The rapidity with which digital information, particularly video, is
	being generated, has necessitated the development of tools for efficient
	search of these media. Content based visual queries have been primarily
	focused on still image retrieval. In this paper we propose a novel
	interactive system on the Web, based on the visual paradigm, with
	spatio-temporal attributes playing a key role in video retrieval.
	The resulting system VideoQ, is the first on-line video search engine
	supporting automatic object based indexing and spatio-temporal queries},
  comment = {analysis_retrieval},
  doi = {10.1109/ACV.1998.732901},
  file = {:papers\\1998 CNF, VideoQ_ a fully automated video retrieval system using motion sketches (Chang, Chen, Sundaram).pdf:PDF},
  keywords = {VideoQ;automatic object based indexing;motion sketches;spatio-temporal
	attributes;spatio-temporal queries;video retrieval;video retrieval
	system;video search engine;image retrieval;search engines;video databases;},
  timestamp = {00,350}
}

@INPROCEEDINGS{1991_CNF_3DreconstructionDetectionEstimation_Chang,
  author = {Chang, Y.-L. and Aggarwal, J.K.},
  title = {3D structure reconstruction from an ego motion sequence using statistical
	estimation and detection theory},
  booktitle = {Visual Motion, 1991., Proceedings of the IEEE Workshop on},
  year = {1991},
  pages = {268 -273},
  month = {oct},
  abstract = {The paper discusses the problem of estimating 3D structures from an
	extended sequence of 2D images taken by a moving camera with known
	motion. The work is mainly concerned with sparse line features and
	thus is a natural extension of the feature-based motion analysis
	paradigm. Usually such a paradigm involves several separate operations:
	feature detection, feature matching, structure/motion estimation,
	and higher level processing, such as feature grouping. The authors
	propose to integrate the different phases based on the statistical
	estimation and detection theory. They show how each operation can
	be formalized and, in particular, consider the structure parameter
	estimation and the feature matching together as the combined estimation-decision
	problem. The proposed algorithm is tested with both synthetic and
	real data},
  comment = {3D},
  doi = {10.1109/WVM.1991.212797},
  file = {:papers\\1991 CNF, 3D structure reconstruction from an ego motion sequence using statistical estimation and detection theory (Chang, Aggarwal, WVM, 25).pdf:PDF},
  keywords = {detection theory;ego motion sequence;feature detection;feature grouping;feature
	matching;feature-based motion analysis;moving camera;sparse line
	features;statistical estimation;structure from motion;structure parameter
	estimation;feature extraction;image reconstruction;image sequences;motion
	estimation;parameter estimation;statistical analysis;},
  timestamp = {-}
}

@INPROCEEDINGS{2007_CNF_DMcolorSegmentation_Chang,
  author = {Chang, Yu-Lin and Fang, Chih-Ying and Ding, Li-Fu and Chen, Shao-Yi
	and Chen, Liang-Gee},
  title = {Depth Map Generation for 2D-to-3D Conversion by Short-Term Motion
	Assisted Color Segmentation},
  booktitle = {Multimedia and Expo, 2007 IEEE International Conference on},
  year = {2007},
  pages = {1958 -1961},
  month = {2-5},
  comment = {3D_stereo},
  doi = {10.1109/ICME.2007.4285061},
  file = {:papers\\2007 CNF, Depth Map Generation for 2D-to-3D Conversion by Short-Term Motion Assisted Color Segmentation (Chang, Fang, Ding, Chen, Chen).pdf:PDF},
  keywords = {2D-to-3D conversion;binocular depth cues;connected component algorithm;depth
	map generation;edge registration technique;human vision;monocular
	depth cues;motion jitter error;motion registration technique;motion/image
	segment adaptation algorithm;pictorial depth cues;short-term motion
	assisted color segmentation;static scene;edge detection;image colour
	analysis;image motion analysis;image registration;image segmentation;},
  timestamp = {-}
}

@ARTICLE{1991_JNL_BNwoTears_Charniak,
  author = {Charniak, E.},
  title = {Bayesian networks without tears},
  journal = {AI Magazine},
  year = {1991},
  volume = {12},
  pages = {50-63},
  abstract = {The article is an introduction to Bayesian networks for AI researchers
	with a limited grounding in probability theory. This method of reasoning
	using probabilities has become popular within the AI probability
	and uncertainty community, rather as resolution theorem proving is
	in the AI-logic community. Nevertheless, despite what seems to be
	their obvious importance, the ideas and techniques have not spread
	much beyond the research community responsible for them, probably
	because the ideas and techniques are easy to understand. The article
	is aimed at overcoming this difficulty},
  comment = {tutorial},
  file = {:papers\\1991 JNL, Bayesian networks without tears (Charniak, AImag, 612).pdf:PDF},
  keywords = {artificial intelligence Bayes methods probability},
  owner = {salman},
  timestamp = {00,600}
}

@ARTICLE{1995_JNL_SURVEYfaces_Chellappa,
  author = {Chellappa, Rama and Wilson, Charles L. and Sirohey, Saad},
  title = {Human and machine recognition of faces: a survey},
  journal = {Proceedings of the IEEE},
  year = {1995},
  volume = {83},
  pages = {705-740},
  abstract = {In order to address the critical issues involved in face recognition
	by human and machine as well, a survey of existing literature on
	human and machine recognition of faces is presented. Among the issues
	discussed include data collection and performance evaluation. Followingly,
	several techniques and recognition systems considered in most engineering
	literature are described.},
  comment = {survey},
  file = {:papers\\1995 JNL, Human and machine recognition of faces_ a survey (SURVEY, Chellappa, ProcIEEE, 1893).pdf:PDF},
  keywords = {Pattern recognition Algorithms Data acquisition Engineering research
	Feature extraction Image segmentation Image understanding Performance
	Psychophysiology Statistics Technology},
  owner = {salman},
  timestamp = {02,000}
}

@ARTICLE{JNL_TRKhuman_Chen,
  author = {Duan-Yu Chen and Cannons, K. and Hsiao-Rong Tyan and Sheng-Wen Shih
	and Liao, H.-Y.M.},
  title = {Spatiotemporal Motion Analysis for the Detection and Classification
	of Moving Targets},
  journal = {Multimedia, IEEE Transactions on},
  year = {2008},
  volume = {10},
  pages = {1578 -1591},
  number = {8},
  month = dec.,
  abstract = {This paper presents a video surveillance system in the environment
	of a stationary camera that can extract moving targets from a video
	stream in real time and classify them into predefined categories
	according to their spatiotemporal properties. Targets are detected
	by computing the pixel-wise difference between consecutive frames,
	and then classified with a temporally boosted classifier and ldquospatiotemporal-oriented
	energyrdquo analysis. We demonstrate that the proposed classifier
	can successfully recognize five types of objects: a person, a bicycle,
	a motorcycle, a vehicle, and a person with an umbrella. In addition,
	we process targets that do not match any of the AdaBoost-based classifier's
	categories by using a secondary classification module that categorizes
	such targets as crowds of individuals or non-crowds. We show that
	the above classification task can be performed effectively by analyzing
	a target's spatiotemporal-oriented energies, which provide a rich
	description of the target's spatial and dynamic features. Our experiment
	results demonstrate that the proposed system is extremely effective
	in recognizing all predefined object classes.},
  comment = {TRKhuman},
  doi = {10.1109/TMM.2008.2007289},
  file = {:c\:\\salman\\work\\writing\\papers\\2008 JNL, Spatiotemporal Motion Analysis for the Detection and Classification of Moving Targets (Chen).pdf:PDF},
  issn = {1520-9210},
  keywords = {moving target classification;moving target detection;object recognition;spatiotemporal
	motion analysis;spatiotemporal-oriented energy analysis;video stream;video
	surveillance;feature extraction;image classification;image motion
	analysis;object recognition;video streaming;video surveillance;}
}

@INPROCEEDINGS{2001_CNF_JPDAFhmmContourTracking_Chen,
  author = {Chen, Yunqiang and Rui, Yong and Huang, T. S.},
  title = {JPDAF based HMM for real-time contour tracking},
  booktitle = {Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings
	of the 2001 IEEE Computer Society Conference on},
  year = {2001},
  volume = {1},
  pages = {I-543-I-550 vol.1},
  abstract = {Tracking objects using multiple cues yields more robust results. The
	well-known hidden Markov model (HMM) provides a powerful framework
	to incorporate multiple cues by expanding its observation. However,
	a plain HMM does not capture the inter-correlation between measurements
	of neighboring states when computing the transition probabilities.
	This can seriously damage the tracking performance. To overcome this
	difficulty, we propose a novel HMM framework targeted at contour-based
	object tracking. A joint probability data association filter (JPDAF)
	is used to compute the HMM's transition probabilities, taking into
	account the intercorrelated neighboring measurements. To ensure real-time
	performance, we have further developed an efficient method to calculate
	the data association probability via dynamic programming, which allows
	the proposed JPDAF-HMM to run comfortably at 30 frames/sec. This
	new tracking framework can easily incorporate various image cues
	(e.g., edge intensity, foreground region color and background region
	color), and also offers an online learning process to adapt to changes
	in the scene. To evaluate its tracking performance, we have applied
	the proposed JPDAF-HMM in various real-world video sequences. We
	report promising tracking results in complex environments.},
  comment = {trk_contour},
  file = {:papers\\2001 CNF, JPDAF Based HMM or Real-Time Contour Tracking (Chen, Rui, Thomas Huang, CVPR, 71).pdf:PDF},
  keywords = {dynamic programming edge detection hidden Markov models image sequences
	probability real-time systems tracking JPDAF based HMM background
	region color contour-based object tracking data association probability
	edge intensity foreground region color hidden Markov model image
	cues intercorrelated neighboring measurements joint probability data
	association filter multiple cues neighboring states object tracking
	online learning process plain HMM real-time contour tracking real-time
	performance real-world video sequences tracking framework tracking
	performance transition probabilities},
  owner = {salman},
  timestamp = {00,070}
}

@ARTICLE{2008_CNF_SnakeSegmentation_Cheng,
  author = {Jinyong Cheng and Caixia Liu},
  title = {Image Segmentation with GVF Snake and Corner Detection},
  journal = {Computer Science and Software Engineering, 2008 International Conference
	on},
  year = {2008},
  volume = {1},
  pages = {1017-1020},
  month = {Dec.},
  abstract = {Gradient vector flow (GVF) snake model is used widely in image segmentation
	and computer vision. GVF snake has larger capture range and stronger
	convergence ability to boundary concavities than traditional snake.
	However in the energy minimization process some corner points canpsilat
	be found. Because of this the object boundary is not accurate. In
	this paper, a new image segmentation algorithm based on Susan approach
	and GVF Snake model is proposed. First we check corner points at
	the edge using Susan approach and mark them as energy minimization
	points, then use GVF Snake model to capture object boundary after
	set initial snake curve. Experiments indicate that the new algorithm
	can improve GVF snake modelpsilas precision to capture the boundary
	with sharp-angled corner.},
  doi = {10.1109/CSSE.2008.1156},
  keywords = {computer vision, curve fitting, edge detection, gradient methods,
	image segmentationSUSAN approach, boundary concavity, computer vision,
	convergence ability, corner detection, edge detection, energy minimization
	process, gradient vector flow snake model, image segmentation, initial
	snake curve}
}

@ARTICLE{1995_JNL_MeanShiftModeClustering_Cheng,
  author = {Yizong Cheng},
  title = {Mean shift, mode seeking, and clustering},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1995},
  volume = {17},
  pages = {790 -799},
  number = {8},
  month = {aug},
  abstract = {Mean shift, a simple interactive procedure that shifts each data point
	to the average of data points in its neighborhood is generalized
	and analyzed in the paper. This generalization makes some k-means
	like clustering algorithms its special cases. It is shown that mean
	shift is a mode-seeking process on the surface constructed with a
	ldquo;shadow rdquo; kernal. For Gaussian kernels, mean shift is a
	gradient mapping. Convergence is studied for mean shift iterations.
	Cluster analysis if treated as a deterministic problem of finding
	a fixed point of mean shift that characterizes the data. Applications
	in clustering and Hough transform are demonstrated. Mean shift is
	also considered as an evolutionary strategy that performs multistart
	global optimization},
  comment = {feature},
  doi = {10.1109/34.400568},
  file = {:papers\\1995 JNL, Mean shift, mode seeking, and clustering (Cheng, PAMI, 945).pdf:PDF},
  issn = {0162-8828},
  keywords = {Gaussian kernels;Hough transform;cluster analysis;convergence;gradient
	mapping;iterations;k-means like clustering algorithms;mean shift;mode
	seeking;multistart global optimization;shadow kernal;volutionary
	strategy;Hough transforms;convergence;optimisation;pattern recognition;},
  timestamp = {01,000}
}

@INPROCEEDINGS{2003_CNF_ShapeFromSilhouette_Cheung,
  author = {Cheung, K. M. G. and Baker, S. and Kanade, T.},
  title = {Shape-from-silhouette of articulated objects and its use for human
	body kinematics estimation and motion capture},
  booktitle = {Computer Vision and Pattern Recognition, 2003. Proceedings. 2003
	IEEE Computer Society Conference on},
  year = {2003},
  abstract = {Shape-from-silhouette (SFS), also known as visual hull (VH) construction,
	is a popular 3D reconstruction method, which estimates the shape
	of an object from multiple silhouette images. The original SFS formulation
	assumes that the entire silhouette images are captured either at
	the same time or while the object is static. This assumption is violated
	when the object moves or changes shape. Hence the use of SFS with
	moving objects has been restricted to treating each time instant
	sequentially and independently. Recently we have successfully extended
	the traditional SFS formulation to refine the shape of a rigidly
	moving object over time. We further extend SFS to apply to dynamic
	articulated objects. Given silhouettes of a moving articulated object,
	the process of recovering the shape and motion requires two steps:
	(1) correctly segmenting (points on the boundary of) the silhouettes
	to each articulated part of the object, (2) estimating the motion
	of each individual part using the segmented silhouette. In this paper,
	we propose an iterative algorithm to solve this simultaneous assignment
	and alignment problem. Once we have estimated the shape and motion
	of each part of the object, the articulation points between each
	pair of rigid parts are obtained by solving a simple motion constraint
	between the connected parts. To validate our algorithm, we first
	apply it to segment the different body parts and estimate the joint
	positions of a person. The acquired kinematic (shape and joint) information
	is then used to track the motion of the person in new video sequences.},
  comment = {recog_action_shape},
  file = {:papers\\2003 CNF, Shape-from-silhouette of articulated objects and its use for human body kinematics estimation and motion capture (Kanade, CVPR, 188).pdf:PDF},
  keywords = {edge detection image reconstruction image segmentation image sequences
	iterative methods kinematics motion estimation object detection optical
	tracking stereo image processing video signal processing 3D reconstruction
	dynamic articulated object human body kinematics estimation iterative
	algorithm joint position estimation motion capture motion recovery
	motion tracking moving articulated object multiple silhouette images
	object shape estimation shape recovery shape-from-silhouette silhouette
	image segmentation video sequence visual hull construction},
  owner = {salman},
  timestamp = {00,200}
}

@ARTICLE{2003_JNL_TRK_matching_Chui,
  author = {Chui, H. and Rangarajan, A.},
  title = {A new point matching algorithm for non-rigid registration},
  journal = {Computer Vision and Image Understanding},
  year = {2003},
  volume = {89},
  pages = {114--141},
  number = {2-3},
  comment = {TRK_match},
  file = {:papers\\2003 JNL, A new point matching algorithm for non-rigid registration (Chui).pdf:PDF},
  issn = {1077-3142},
  publisher = {Elsevier},
  timestamp = {00,400}
}

@ARTICLE{2001_JNL_AlgorithmsCooperativeMultisensorSurveillance_Collins,
  author = {Collins, R.T. and Lipton, A.J. and Fujiyoshi, H. and Kanade, T.},
  title = {Algorithms for cooperative multisensor surveillance},
  journal = {Proceedings of the IEEE},
  year = {2001},
  volume = {89},
  pages = {1456 -1477},
  number = {10},
  month = {oct},
  abstract = {The Video Surveillance and Monitoring (VSAM) team at Carnegie Mellon
	University (CMU) has developed an end-to-end, multicamera surveillance
	system that allows a single human operator to monitor activities
	in a cluttered environment using a distributed network of active
	video sensors. Video understanding algorithms have been developed
	to automatically detect people and vehicles, seamlessly track them
	using a network of cooperating active sensors, determine their three-dimensional
	locations with respect to a geospatial site model, and present this
	information to a human operator who controls the system through a
	graphical user interface. The goal is to automatically collect and
	disseminate real-time information to improve the situational awareness
	of security providers and decision makers. The feasibility of real-time
	video surveillance has been demonstrated within a multicamera testbed
	system developed on the campus of CMU. This paper presents an overview
	of the issues and algorithms involved in creating this semiautonomous,
	multicamera surveillance system},
  comment = {?},
  doi = {10.1109/5.959341},
  file = {:papers\\2001 JNL, Algorithms for cooperative multisensor surveillance (Collins, Fujiyoshi, Kanade, ProcIEEE, 367).pdf:PDF},
  issn = {0018-9219},
  keywords = {GUI;active video sensors;active vision;cluttered environment;cooperative
	multisensor surveillance;decision makers;distributed network;geolocation
	multisensor systems;geospatial site model;graphical user interface;human
	operator;multicamera surveillance system;people detection;real-time
	information;real-time video surveillance;security providers;site
	security monitoring;situational awareness;smart sensors;three-dimensional
	locations;vehicle detection;video surveillance;video understanding
	algorithms;active vision;clutter;distributed tracking;graphical user
	interfaces;image classification;image motion analysis;intelligent
	sensors;monitoring;object detection;real-time systems;security;sensor
	fusion;surveillance;video signal processing;},
  timestamp = {00,400}
}

@ARTICLE{2007_JNL_TRK_Colombari,
  author = {A. Colombari and A. Fusiello and V. Murino},
  title = {Segmentation and tracking of multiple video objects},
  journal = {Pattern Recognition},
  year = {2007},
  volume = {40},
  pages = {1307 - 1317},
  number = {4},
  abstract = {This paper describes a technique that produces a content-based representation
	of a video shot composed by a background (still) mosaic and one or
	more foreground moving objects. Segmentation of moving objects is
	based on ego-motion compensation and on background modelling using
	tools from robust statistics. Region matching is carried out by an
	algorithm that operates on the Mahalanobis distance between region
	descriptors in two subsequent frames and uses singular value decomposition
	to compute a set of correspondences satisfying both the principle
	of proximity and the principle of exclusion. The sequence is represented
	as a layered graph, and specific techniques are introduced to cope
	with crossing and occlusion. Examples of MPEG-4 (main profile) encoding
	are reported.},
  comment = {TRK},
  doi = {DOI: 10.1016/j.patcog.2006.07.008},
  file = {:C\:\\salman\\work\\writing\\papers\\2007 JNL, Segmentation and tracking of multiple video objects (Colombari).pdf:PDF},
  issn = {0031-3203},
  keywords = {Content-based representation},
  timestamp = {00,026},
  url = {http://www.sciencedirect.com/science/article/B6V14-4M0J4C1-1/2/fdbc31a159e5776791b5ea1e8702ff48}
}

@ARTICLE{2002_JNL_MeanShift_Comaniciu,
  author = {Comaniciu, D.},
  title = {Bayesian kernel tracking},
  journal = {Pattern Recognition. 24th DAGM Symposium. Proceedings (Lecture Notes
	in Computr Science)},
  year = {2002},
  volume = {2449},
  pages = {438 - 45},
  abstract = {We present a Bayesian approach to real-time object tracking using
	nonparametric density estimation. The target model and candidates
	are represented by probability densities in the joint spatial-intensity
	domain. The new location and appearance of the target are jointly
	derived by computing the maximum likelihood estimate of the parameter
	vector that characterizes the transformation from the candidate to
	the model. This probabilistic formulation accommodates variations
	in the target appearance, while being robust to outliers represented
	by partial occlusions. In this paper we analyze the simplest parameterization
	represented by translation in both domains and present a gradient-based
	iterative solution. Various tracking sequences demonstrate the superior
	behavior of the method},
  address = {Berlin, Germany},
  comment = {trk},
  copyright = {Copyright 2003, IEE},
  file = {:papers\\2002 JNL, Bayesian kernel tracking (Comaniciu, PR, 21).pdf:PDF},
  keywords = {Bayes methods;maximum likelihood estimation;object recognition;tracking;},
  language = {English},
  timestamp = {-}
}

@ARTICLE{2002_JNL_MeanShiftFeatureSpaceAnalysis_Comaniciu,
  author = {Comaniciu, D. and Meer, P.},
  title = {Mean shift: a robust approach toward feature space analysis},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2002},
  volume = {24},
  pages = {603-619},
  number = {5},
  abstract = {A general non-parametric technique is proposed for the analysis of
	a complex multimodal feature space and to delineate arbitrarily shaped
	clusters in it. The basic computational module of the technique is
	an old pattern recognition procedure: the mean shift. For discrete
	data, we prove the convergence of a recursive mean shift procedure
	to the nearest stationary point of the underlying density function
	and, thus, its utility in detecting the modes of the density. The
	relation of the mean shift procedure to the Nadaraya-Watson estimator
	from kernel regression and the robust M-estimators; of location is
	also established. Algorithms for two low-level vision tasks discontinuity-preserving
	smoothing and image segmentation - are described as applications.
	In these algorithms, the only user-set parameter is the resolution
	of the analysis, and either gray-level or color images are accepted
	as input. Extensive experimental results illustrate their excellent
	performance},
  comment = {feature},
  file = {:papers\\2002 JNL, Mean shift_ a robust approach toward feature space analysis (Comaniciu, Meer, PAMI, 2588).pdf:PDF},
  keywords = {computer vision estimation theory image segmentation nonparametric
	statistics pattern clustering smoothing methods Nadaraya-Watson estimator
	algorithm performance analysis resolution arbitrarily shaped cluster
	delineation color images complex multimodal feature space computational
	module convergence density function density modes detection discontinuity-preserving
	image smoothing discrete data gray-level images kernel regression
	location estimation low-level vision algorithms mean shift nearest
	stationary point nonparametric technique pattern recognition procedure
	recursive mean shift procedure robust M-estimators robust feature
	space analysis user-set parameter},
  owner = {salman},
  timestamp = {02,600}
}

@INPROCEEDINGS{1999_CNF_MeanShiftAnalysisAndApplications_Comaniciu,
  author = {Comaniciu, D. and Meer, P.},
  title = {Mean shift analysis and applications},
  booktitle = {Computer Vision, 1999. The Proceedings of the Seventh IEEE International
	Conference on},
  year = {1999},
  volume = {2},
  pages = {1197-1203 vol.2},
  abstract = {A nonparametric estimator of density gradient, the mean shift, is
	employed in the joint, spatial-range (value) domain of gray level
	and color images for discontinuity preserving filtering and image
	segmentation. Properties of the mean shift are reviewed and its convergence
	on lattices is proven. The proposed filtering method associates with
	each pixel in the image the closest local mode in the density distribution
	of the joint domain. Segmentation into a piecewise constant structure
	requires only one more step, fusion of the regions associated with
	nearby modes. The proposed technique has two parameters controlling
	the resolution in the spatial and range domains. Since convergence
	is guaranteed, the technique does not require the intervention of
	the user to stop the filtering at the desired image quality. Several
	examples, for gray and color images, show the versatility of the
	method and compare favorably with results described in the literature
	for the same images},
  comment = {feature},
  file = {:papers\\1999 CNF, Mean shift analysis and applications (Comaniciu, Meer, ICCV, 471).pdf:PDF},
  keywords = {computer vision image segmentation color images density gradient discontinuity
	preserving filtering gray level lattices mean shift analysis nonparametric
	estimator spatial-range domain},
  owner = {salman},
  timestamp = {00,500}
}

@ARTICLE{2003_JNL_TRKkernel_Comaniciu,
  author = {Comaniciu, D. and Ramesh, V. and Meer, P.},
  title = {Kernel-based object tracking},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2003},
  volume = {25},
  pages = {564-577},
  number = {5},
  abstract = {A new approach toward target representation and localization, the
	central component in visual tracking of nonrigid objects, is proposed.
	The feature histogram-based target representations are regularized
	by spatial masking with an isotropic kernel. The masking induces
	spatially-smooth similarity functions suitable for gradient-based
	optimization, hence, the target localization problem can be formulated
	using the basin of attraction of the local maxima. We employ a metric
	derived from the Bhattacharyya coefficient as similarity measure,
	and use the mean shift procedure to perform the optimization. In
	the presented tracking examples, the new method successfully coped
	with camera motion, partial occlusions, clutter, and target scale
	variations. Integration with motion filters and data association
	techniques is also discussed. We describe only a few of the potential
	applications: exploitation of background information, Kalman tracking
	using motion models, and face tracking.},
  comment = {trk},
  file = {:papers\\2003 JNL, Kernel-Based Object Tracking (Comaniciu, Ramesh, Meer, PAMI, 1368).pdf:PDF},
  keywords = {computer vision image motion analysis image representation object
	recognition optimisation tracking Bhattacharyya coefficient Kalman
	tracking background information basin of attraction camera motion
	clutter data association techniques face tracking feature histogram-based
	target representation gradient-based optimization isotropic kernel
	kernel-based object tracking local maxima mean shift procedure partial
	occlusion spatial masking spatially-smooth similarity functions target
	localization visual tracking},
  owner = {salman},
  timestamp = {01,400}
}

@INPROCEEDINGS{2000_CNF_RealTimeTrackingMeanShift_Comaniciu,
  author = {Comaniciu, D. and Ramesh, V. and Meer, P.},
  title = {Real-time tracking of non-rigid objects using mean shift},
  booktitle = {Computer Vision and Pattern Recognition, 2000. Proceedings. IEEE
	Conference on},
  year = {2000},
  volume = {2},
  pages = {142 -149 vol.2},
  abstract = {A new method for real time tracking of non-rigid objects seen from
	a moving camera is proposed. The central computational module is
	based on the mean shift iterations and finds the most probable target
	position in the current frame. The dissimilarity between the target
	model (its color distribution) and the target candidates is expressed
	by a metric derived from the Bhattacharyya coefficient. The theoretical
	analysis of the approach shows that it relates to the Bayesian framework
	while providing a practical, fast and efficient solution. The capability
	of the tracker to handle in real time partial occlusions, significant
	clutter, and target scale variations, is demonstrated for several
	image sequences},
  comment = {trk},
  file = {:papers\\2000 CNF, Real-time tracking of non-rigid objects using mean shift (Comaniciu, Ramesh, Meer).pdf:PDF},
  keywords = {Bayesian framework;Bhattacharyya coefficient;clutter;color distribution;computational
	module;image sequences;mean shift iterations;most probable target
	position;moving camera;non-rigid object tracking;partial occlusions;real
	time tracking;target candidate;target model;target scale variations;Bayes
	methods;computer vision;image sequences;iterative methods;optical
	tracking;real-time systems;},
  timestamp = {01,400}
}

@BOOK{BOOK_spheres_Conway,
  title = {Sphere Packings, Lattices and Groups},
  publisher = {Springer},
  year = {1998},
  author = {Conway, John H. and Sloane, Neil J. A.},
  edition = {3rd},
  month = Dec,
  day = {07},
  howpublished = {Hardcover},
  isbn = {0387985859},
  posted-at = {2011-04-12 03:29:12},
  priority = {2},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387985859}
}

@ARTICLE{1995_JNL_ActiveModels_Cootes,
  author = {Cootes, T.F. and Taylor, C.J. and Cooper, D.H. and Graham, J.},
  title = {Active shape models-their training and application},
  journal = {Computer Vision and Image Understanding},
  year = {1995},
  volume = { 61},
  pages = {38 - 59},
  number = {1},
  abstract = {Model-based vision is firmly established as a robust approach to recognizing
	and locating known rigid objects in the presence of noise, clutter,
	and occlusion. It is more problematic to apply model-based methods
	to images of objects whose appearance can vary, though a number of
	approaches based on the use of flexible templates have been proposed.
	The problem with existing methods is that they sacrifice model specificity
	in order to accommodate variability, thereby compromising robustness
	during image interpretation. We argue that a model should only be
	able to deform in ways characteristic of the class of objects it
	represents. We describe a method for building models by learning
	patterns of variability from a training set of correctly annotated
	images. These models can be used for image search in an iterative
	refinement algorithm analogous to that employed by Active Contour
	Models (Snakes). The key difference is that our Active Shape Models
	can only deform to fit the data in ways consistent with the training
	set. We show several practical examples where we have built such
	models and used them to locate partially occluded objects in noisy,
	cluttered images},
  address = {USA},
  comment = {feature_contour},
  copyright = {Copyright 1995, IEE},
  file = {:papers\\1995 JNL, Active shape models-their training and application (Cootes, Taylor, Cooper, Graham, CVIU, 2827).pdf:PDF},
  issn = {1077-3142},
  keywords = {clutter;computer vision;},
  language = {English},
  timestamp = {03,000},
  url = {http://dx.doi.org/10.1006/cviu.1995.1004}
}

@ARTICLE{1994_JNL_ASMmedical_Cootes,
  author = {Cootes, T. F. and Hill, A. and Taylor, C.J. and Haslam, J.},
  title = {The use of active shape models for locating structures in medical
	images},
  journal = {Image and Vision Computing},
  year = {1994},
  volume = {12},
  pages = {355-366},
  comment = {trk_contour},
  file = {:papers\\1994 JNL, The use of active shape models for locating structures in medical images (Cootes, Hill, Taylor, Haslam).pdf:PDF},
  timestamp = {00,800}
}

@BOOK{2001_BOOK_Algorithms_Cormen,
  title = {Introduction to Algorithms, Second Edition},
  publisher = {The MIT Press},
  year = {2001},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L.
	and Stein, Clifford},
  edition = {2nd},
  month = {September},
  abstract = {{Aimed at any serious programmer or computer science student, the
	new second edition of <I>Introduction to Algorithms</I> builds on
	the tradition of the original with a truly magisterial guide to the
	world of algorithms. Clearly presented, mathematically rigorous,
	and yet approachable even for the math-averse, this title sets a
	high standard for a textbook and reference to the best algorithms
	for solving a wide range of computing problems.<p>With sample problems
	and mathematical proofs demonstrating the correctness of each algorithm,
	this book is ideal as a textbook for classroom study, but its reach
	doesn't end there. The authors do a fine job of explaining each algorithm.
	(Reference sections on basic mathematical notation will help readers
	bridge the gap, but it will help to have some math background to
	appreciate the full achievement of this handsome hardcover volume.)
	Every algorithm is presented in pseudo-code, which can be implemented
	in any computer language, including C/C++ and Java. This ecumenical
	approach is one of the book's strengths. When it comes to sorting
	and common data structures, from basic linked lists to trees (including
	binary trees, red-black, and B-trees), this title really shines,
	with clear diagrams that show algorithms in operation. Even if you
	just glance over the mathematical notation here, you can definitely
	benefit from this text in other ways.<p>The book moves forward with
	more advanced algorithms that implement strategies for solving more
	complicated problems (including dynamic programming techniques, greedy
	algorithms, and amortized analysis). Algorithms for graphing problems
	(used in such real-world business problems as optimizing flight schedules
	or flow through pipelines) come next. In each case, the authors provide
	the best from current research in each topic, along with sample solutions.<p>This
	text closes with a grab bag of useful algorithms including matrix
	operations and linear programming, evaluating polynomials, and the
	well-known Fast Fourier Transformation (FFT) (useful in signal processing
	and engineering). Final sections on "NP-complete" problems, like
	the well-known traveling salesman problem, show off that while not
	all problems have a demonstrably final and best answer, algorithms
	that generate acceptable approximate solutions can still be used
	to generate useful, real-world answers.<p>Throughout this text, the
	authors anchor their discussion of algorithms with current examples
	drawn from molecular biology (like the Human Genome Project), business,
	and engineering. Each section ends with short discussions of related
	historical material, often discussing original research in each area
	of algorithms. On the whole, they argue successfully that algorithms
	are a "technology" just like hardware and software that can be used
	to write better software that does more, with better performance.
	Along with classic books on algorithms (like Donald Knuth's three-volume
	set, <I>The Art of Computer Programming</I>), this title sets a new
	standard for compiling the best research in algorithms. For any experienced
	developer, regardless of their chosen language, this text deserves
	a close look for extending the range and performance of real-world
	software. <I>--Richard Dragan</I> <p> <B>Topics covered:</B> Overview
	of algorithms (including algorithms as a technology); designing and
	analyzing algorithms; asymptotic notation; recurrences and recursion;
	probabilistic analysis and randomized algorithms; heapsort algorithms;
	priority queues; quicksort algorithms; linear time sorting (including
	radix and bucket sort); medians and order statistics (including minimum
	and maximum); introduction to data structures (stacks, queues, linked
	lists, and rooted trees); hash tables (including hash functions);
	binary search trees; red-black trees; augmenting data structures
	for custom applications; dynamic programming explained (including
	assembly-line scheduling, matrix-chain multiplication, and optimal
	binary search trees); greedy algorithms (including Huffman codes
	and task-scheduling problems); amortized analysis (the accounting
	and potential methods); advanced data structures (including B-trees,
	binomial and Fibonacci heaps, representing disjoint sets in data
	structures); graph algorithms (representing graphs, minimum spanning
	trees, single-source shortest paths, all-pairs shortest paths, and
	maximum flow algorithms); sorting networks; matrix operations; linear
	programming (standard and slack forms); polynomials and the Fast
	Fourier Transformation (FFT); number theoretic algorithms (including
	greatest common divisor, modular arithmetic, the Chinese remainder
	theorem, RSA public-key encryption, primality testing, integer factorization);
	string matching; computational geometry (including finding the convex
	hull); NP-completeness (including sample real-world NP-complete problems
	and their insolvability); approximation algorithms for NP-complete
	problems (including the traveling salesman problem); reference sections
	for summations and other mathematical notation, sets, relations,
	functions, graphs and trees, as well as counting and probability
	backgrounder (plus geometric and binomial distributions).}},
  comment = {Computer_science_book},
  day = {01},
  file = {:papers\\2001 BOOK, Introduction to Algorithms (Cormen, 2nd ed).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0262032937},
  posted-at = {2010-06-01 14:57:14},
  priority = {2},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0262032937}
}

@ARTICLE{1998_JNL_MultibodyFactorization_CosteiraKanade,
  author = {Costeira, Joao Paulo and Kanade, Takeo},
  title = {A multibody factorization method for independently moving objects},
  journal = {International Journal of Computer Vision},
  year = {1998},
  volume = {29},
  pages = {159-179},
  number = {Compendex},
  abstract = {The structure-from-motion problem has been extensively studied in
	the field of computer vision. Yet, the bulk of the existing work
	assumes that the scene contains only a single moving object. The
	more realistic case where an unknown number of objects move in the
	scene has received little attention, especially for its theoretical
	treatment. In this paper we present a new method for separating and
	recovering the motion and shape of multiple independently moving
	objects in a sequence of images. The method does not require prior
	knowledge of the number of objects, nor is dependent on any grouping
	of features into an object at the image level. For this purpose,
	we introduce a mathematical construct of object shapes, called the
	shape interaction matrix, which is invariant to both the object motions
	and the selection of coordinate systems. This invariant structure
	is computable solely from the observed trajectories of image features
	without grouping them into individual objects. Once the matrix is
	computed, it allows for segmenting features into objects by the process
	of transforming it into a canonical form, as well as recovering the
	shape and motion of each object. The theory works under a broad set
	of projection models (scaled orthography, paraperspective and affine)
	but they must be linear, so it excludes projective "cameras".},
  comment = {?},
  file = {:papers\\1998 JNL, A multibody factorization method for independently moving objects (Coseteira, Kanade, IJCV, 181).pdf:PDF},
  keywords = {Computer vision Fourier transforms Image segmentation Image understanding
	Mathematical models Matrix algebra Motion estimation Problem solving
	Three dimensional},
  owner = {salman},
  timestamp = {00,200}
}

@BOOK{2006_BOOK_InformationTheory_Cover,
  title = {Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications
	and Signal Processing)},
  publisher = {Wiley-Interscience},
  year = {2006},
  author = {Cover, Thomas and Thomas, Joy},
  abstract = {The latest edition of this classic is updated with new problem sets
	and material The Second Edition of this fundamental textbook maintains
	the book's tradition of clear, thought-provoking instruction. Readers
	are provided once again with an instructive mix of mathematics, physics,
	statistics, and information theory. All the essential topics in information
	theory are covered in detail, including entropy, data compression,
	channel capacity, rate distortion, network information theory, and
	hypothesis testing. The authors provide readers with a solid understanding
	of the underlying theory and applications. Problem sets and a telegraphic
	summary at the end of each chapter further assist readers. The historical
	notes that follow each chapter recap the main points. The Second
	Edition features: * Chapters reorganized to improve teaching * 200
	new problems * New material on source coding, portfolio theory, and
	feedback capacity * Updated references Now current and enhanced,
	the Second Edition of Elements of Information Theory remains the
	ideal textbook for upper-level undergraduate and graduate courses
	in electrical engineering, statistics, and telecommunications. An
	Instructor's Manual presenting detailed solutions to all the problems
	in the book is available from the Wiley editorial department.},
  comment = {IT_book},
  file = {:papers\\2006 BOOK, Elements Of Information Theory (Cover, Wiley 2nd ed).pdf:PDF},
  keywords = {1_signal_processing 2_information_theory},
  timestamp = {-}
}

@ARTICLE{1993_JNL_SURVEYcorresp_Cox,
  author = {Cox, I. J.},
  title = {A review of statistical data association techniques for motion correspondence},
  journal = {International Journal of Computer Vision},
  year = {1993},
  volume = {10},
  pages = {53-66},
  number = {Copyright 1993, IEE},
  abstract = {Motion correspondence is a fundamental problem in computer vision
	and many other disciplines. The article describes statistical data
	association techniques originally developed in the context of target
	tracking and surveillance and now beginning to be used in dynamic
	motion analysis by the computer vision community. The Mahalanobis
	distance measure is first introduced before discussing the limitations
	of nearest neighbor algorithms. Then, the track-splitting, joint
	likelihood, multiple hypothesis algorithms are described, each method
	solving an increasingly more complicated optimization. Real-time
	constraints may prohibit the application of these optimal methods.
	The suboptimal joint probabilistic data association algorithm is
	therefore described. The advantages, limitations, and relationships
	between the approaches are discussed},
  comment = {survey},
  file = {:papers\\1993 JNL, A review of statistical data association techniques for motion correspondence (Cox, IJCV, 178).pdf:PDF},
  keywords = {computer vision motion estimation reviews statistical analysis},
  owner = {salman},
  timestamp = {00,200}
}

@ARTICLE{1996_JNL_EfficientMHT_Cox,
  author = {Cox, I. J. and Hingorani, S. L.},
  title = {An efficient implementation of Reid's multiple hypothesis tracking
	algorithm and its evaluation for the purpose of visual tracking},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1996},
  volume = {18},
  pages = {138-150},
  number = {2},
  abstract = {An efficient implementation of Reid's multiple hypothesis tracking
	(MHT) algorithm is presented in which the k-best hypotheses are determined
	in polynomial time using an algorithm due to Murly (1968). The MHT
	algorithm is then applied to several motion sequences. The MHT capabilities
	of track initiation, termination, and continuation are demonstrated
	together with the latter's capability to provide low level support
	of temporary occlusion of tracks. Between 50 and 150 corner features
	are simultaneously tracked in the image plane over a sequence of
	up to 51 frames. Each corner is tracked using a simple linear Kalman
	filter and any data association uncertainty is resolved by the MHT.
	Kalman filter parameter estimation is discussed, and experimental
	results show that the algorithm is robust to errors in the motion
	model. An investigation of the performance of the algorithm as a
	function of look-ahead (tree depth) indicates that high accuracy
	can be obtained for tree depths as shallow as three. Experimental
	results suggest that a real-time MHT solution to the motion correspondence
	problem is possible for certain classes of scenes},
  comment = {trk_correspondence},
  file = {:papers\\1996 JNL, An efficient  implementation of  Reid's multiple hypothesis tracking algorithm and its evaluation for the purpose of visual tracking   (Cox, Hingorani, PAMI, 351).pdf:PDF},
  keywords = {Kalman filters image sequences motion estimation optical tracking
	parameter estimation Reid's multiple hypothesis tracking algorithm
	data association uncertainty k-best hypotheses linear Kalman filter
	motion model motion sequences temporary occlusion tree depths visual
	tracking},
  owner = {salman},
  timestamp = {00,350}
}

@INPROCEEDINGS{2002_CNF_NonlinearShapeMumfordShahSegmentation_CremersKohlbergerSchnorr,
  author = {Cremers, Daniel and Kohlberger, Timo and Schnorr, Christoph},
  title = {Nonlinear Shape Statistics in Mumford-Shah Based Segmentation},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year = {2002},
  comment = {seg},
  file = {:papers\\2002 CNF, Nonlinear Shape Statistics in Mumford-Shah Based Segmentation (Cremers, Kohlberger, Schnorr, ECCV, 112).pdf:PDF},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{2003_JNL_StatisticalShapeKnowledge_Cremers,
  author = {Cremers, Daniel and Schnorr, Christoph},
  title = {Statistical shape knowledge in variational motion segmentation},
  journal = {Image and Vision Computing},
  year = {2003},
  volume = {21},
  pages = {77 - 86},
  number = {1},
  abstract = {We present a generative approach to model-based motion segmentation
	by incorporating a statistical shape prior into a novel variational
	segmentation method. The shape prior statistically encodes a training
	set of object outlines presented in advance during a training phase.
	In a region competition manner the proposed variational approach
	maximizes the homogeneity of the motion vector field estimated on
	a set of regions, thus evolving the separating discontinuity set.
	Due to the shape prior, this discontinuity set is not only sensitive
	to motion boundaries but also favors shapes according to the statistical
	shape knowledge. In numerical examples we verify several properties
	of the proposed approach: for objects which cannot be easily discriminated
	from the background by their appearance, the desired motion segmentation
	is obtained, although the corresponding segmentation based on image
	intensities fails. The region-based formulation facilitates convergence
	of the contour from its initialization over fairly large distances,
	and the estimated flow field is progressively improved during the
	gradient descent minimization. Due to the shape prior, partial occlusions
	of the moving object by 'unfamiliar' objects are ignored, and the
	evolution of the motion boundary is effectively restricted to the
	subspace of familiar shapes. &copy; 2002 Elsevier Science B.V. All
	rights reserved.},
  comment = {motion},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2003 JNL, Statistical shape knowledge in variational motion segmentation (Cremers, Schnorr, IVC, 33).pdf:PDF},
  issn = {02628856},
  key = {Image segmentation},
  keywords = {Image coding;Statistical methods;Variational techniques;Vectors;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1016/S0262-8856(02)00128-2}
}

@ARTICLE{1999_JNL_PF_Crisan,
  author = {Crisan, D. and Del Moral, P. and Lyons, TJ},
  title = {Non linear filtering using branching and interacting particle systems},
  journal = {Markov Processes Related Fields},
  year = {1999},
  volume = {5},
  pages = {293--319},
  number = {3}
}

@ARTICLE{4430462,
  author = {Crisman, J.D. and Thorpe, C.E.},
  title = {SCARF: a color vision system that tracks roads and intersections},
  journal = {IEEE Transactions on Robotics and Automation},
  year = {1993},
  volume = { 9},
  pages = {49 - 58},
  number = { 1},
  note = {mobile robots;road tracking;image recognition;computer vision;color
	vision system;SCARF;navigation;Navlab;Bayesian classification;path-planning
	system;},
  abstract = {SCARF, a color vision system that recognizes difficult roads and intersections,
	is presented. It has been integrated into several navigation systems
	that drive a robot vehicle, the Navlab, on a variety of roads in
	many different weather conditions. SCARF recognizes roads that have
	degraded surfaces and edges with no lane markings in difficult shadow
	conditions. It also recognizes intersections with or without predictions
	from the navigation system. This is the first system that detects
	intersections in images without a priori knowledge of the intersection
	shape and location. SCARF uses Bayesian classification to determine
	a road-surface likelihood for each pixel in a reduced color image.
	It then evaluates a number of road and intersection candidates by
	matching an ideal road-surface likelihood image with the results
	from the Bayesian classification. The best matching candidate is
	passed to a path-planning system that navigates the robot vehicle
	on the road or intersection. The SCARF system is described in detail,
	results on a variety of images are presented, and Navlab test runs
	using SCARF are discussed},
  address = {USA},
  comment = {trk_color},
  copyright = {Copyright 1993, IEE},
  file = {:papers\\1993 JNL, SCARF_ a color vision system that tracks roads and intersections (Crisman, Thorpe).pdf:PDF},
  issn = {1042-296X},
  keywords = {Bayes methods;computer vision;computerised navigation;image recognition;mobile
	robots;path planning;},
  language = {English},
  review = {salman: 
	
	
	- class bayes rule applied here
	
	- if you want to learn about bayes rule, th},
  timestamp = {00,150},
  url = {http://dx.doi.org/10.1109/70.210794}
}

@ARTICLE{2000_JNL_PeriodicMotion_CutlerDavis,
  author = {Cutler, R. and Davis, L.S.},
  title = {Robust real-time periodic motion detection, analysis, and applications},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {781 -796},
  number = {8},
  month = {aug},
  abstract = {We describe new techniques to detect and analyze periodic motion as
	seen from both a static and a moving camera. By tracking objects
	of interest, we compute an object's self-similarity as it evolves
	in time. For periodic motion, the self-similarity measure is also
	periodic and we apply time-frequency analysis to detect and characterize
	the periodic motion. The periodicity is also analyzed robustly using
	the 2D lattice structures inherent in similarity matrices. A real-time
	system has been implemented to track and classify objects using periodicity.
	Examples of object classification (people, running dogs, vehicles),
	person counting, and nonstationary periodicity are provided},
  comment = {motion},
  doi = {10.1109/34.868681},
  file = {:papers\\2000 JNL, Robust real-time periodic motion detection (Cutler, Davis, PAMI, 447).pdf:PDF},
  issn = {0162-8828},
  keywords = {2D lattice structures;nonstationary periodicity;object classification;periodic
	motion analysis;periodicity;person counting;real-time system;robust
	real-time periodic motion detection;self-similarity;similarity matrices;time-frequency
	analysis;image classification;image motion analysis;matrix algebra;object
	recognition;real-time systems;stability;time-frequency analysis;},
  timestamp = {00,450}
}

@ARTICLE{9490210,
  author = {Czyz, J. and Ristic, B. and Macq, B.},
  title = {A particle filter for joint detection and tracking of color objects},
  journal = {Image and Vision Computing},
  year = {2007},
  volume = { 25},
  pages = {1271 - 81},
  number = { 8},
  note = {color particle filter;joint detection;color deformable object tracking
	algorithm;image sequences;hybrid valued sequential state estimation
	algorithm;target detection;external track initialization algorithm;external
	track cancellation algorithm;video sequences;visual multiple-target
	tracking;},
  abstract = {Color is a powerful feature for tracking deformable objects in image
	sequences with complex backgrounds. The color particle filter has
	proven to be an efficient, simple and robust tracking algorithm.
	In this paper, we present a hybrid valued sequential state estimation
	algorithm, and its particle filter-based implementation, that extends
	the standard color particle filter in two ways. First, target detection
	and deletion are embedded in the particle filter without relying
	on an external track initialization and cancellation algorithm. Second,
	the algorithm is able to track multiple objects sharing the same
	color description while keeping the attractive properties of the
	original color particle filter. The performance of the proposed filter
	are evaluated qualitatively on various real-world video sequences
	with appearing and disappearing targets. [All rights reserved Elsevier]},
  address = {Netherlands},
  comment = {trk_color},
  copyright = {Copyright 2007, The Institution of Engineering and Technology},
  file = {:papers\\2007 JNL, A particle filter for joint detection and tracking of color objects (Czyz, Ristic, Macq).pdf:PDF},
  issn = {0262-8856},
  keywords = {computer vision;image colour analysis;image sequences;object detection;optical
	tracking;particle filtering (numerical methods);target tracking;video
	signal processing;},
  language = {English},
  timestamp = {00,030},
  url = {http://dx.doi.org/10.1016/j.imavis.2006.07.027}
}

@INPROCEEDINGS{2005_CNF_HOG_Dalal,
  author = {Dalal, N. and Triggs, B.},
  title = {Histograms of oriented gradients for human detection},
  booktitle = {Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recognition
	CVPR 2005},
  year = {2005},
  volume = {1},
  pages = {886--893},
  comment = {TRK_ped},
  doi = {10.1109/CVPR.2005.177},
  file = {:papers\\2005 CNF, Histograms of oriented gradients for human detection (Dalal).pdf:PDF},
  timestamp = {01,700}
}

@ARTICLE{2000_JNL_PersonTracking_Darrell,
  author = {Darrell, T. and Gordon, G. and Harville, M. and Woodfill, J.},
  title = {Integrated person tracking using stereo, color, and pattern detection},
  journal = {International Journal of Computer Vision},
  year = {2000},
  volume = { 37},
  pages = {175 - 85},
  abstract = {We present an approach to real-time person tracking in crowded and/or
	unknown environments using integration of multiple visual modalities.
	We combine stereo, color, and face detection modules into a single
	robust system, and show an initial application in an interactive,
	face-responsive display. Dense, real-time stereo processing is used
	to isolate users from other objects and people in the background.
	Skin-hue classification identifies and tracks likely body parts within
	the silhouette of a user. Face pattern detection discriminates and
	localizes the face within the identified body parts. Faces and bodies
	of users are tracked over several temporal scales: short-term (user
	stays within the field of view), medium-term (user exits/reenters
	within minutes), and long term (user returns after hours or days).
	Short-term tracking is performed using simple region position and
	size correspondences, while medium and long-term tracking are based
	on statistics of user appearance. We discuss the failure modes of
	each individual module, describe our integration method, and report
	results with the complete system in trials with thousands of users},
  address = {Netherlands},
  comment = {trk_color, trk_people},
  copyright = {Copyright 2000, IEE},
  file = {:papers\\2000 JNL, Integrated person tracking using stereo, color, and pattern detection (Darrell, Gordon, Harville, Woodfill, IJCV, 374).pdf:PDF},
  issn = {0920-5691},
  keywords = {computer vision;face recognition;feature extraction;image colour analysis;stereo
	image processing;user interfaces;},
  language = {English},
  timestamp = {00,400},
  url = {http://dx.doi.org/10.1023/A:1008103604354}
}

@INPROCEEDINGS{1993_CNF_Gestures_Darrell,
  author = {Darrell, T. and Pentland, A.},
  title = {Space-time gestures},
  booktitle = {Computer Vision and Pattern Recognition, 1993. Proceedings CVPR '93.,
	1993 IEEE Computer Society Conference on},
  year = {1993},
  pages = {335-340},
  abstract = {A method for learning, tracking, and recognizing human gestures using
	a view-based approach to model articulated objects is presented.
	Objects are represented using sets of view models, rather than single
	templates. Stereotypical space-time patterns, i.e., gestures, are
	then matched to stored gesture patterns using dynamic time warping.
	Real-time performance is achieved by using special purpose correlation
	hardware and view prediction to prune as much of the search space
	as possible. Both view models and view predictions are learned from
	examples. Results showing tracking and recognition of human hand
	gestures at over 10 Hz are presented},
  comment = {recog_action_appearance},
  file = {:papers\\1993 CNF, Space-time gestures (Darrell, Pentland, CVPR, 302).pdf:PDF},
  keywords = {correlators human factors image recognition image sequences motion
	estimation real-time systems user interfaces 10 Hz articulated objects
	correlation hardware dynamic time warping gesture learning gesture
	recognition gesture tracking human gestures real-time performance
	search space pruning space-time gestures stereotypical space-time
	patterns view prediction view-based approach},
  timestamp = {00,300}
}

@ARTICLE{2005_JNL_SpaceTimeStereo_Davis,
  author = {Davis, J. and Nehab, D. and Ramamoorthi, R. and Rusinkiewicz, S.},
  title = {Spacetime stereo: a unifying framework for depth from triangulation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2005},
  volume = {27},
  pages = {296 -302},
  number = {2},
  month = {feb. },
  abstract = {Depth from triangulation has traditionally been investigated in a
	number of independent threads of research, with methods such as stereo,
	laser scanning, and coded structured light considered separately.
	We propose a common framework called spacetime stereo that unifies
	and generalizes many of these previous methods. To show the practical
	utility of the framework, we develop two new algorithms for depth
	estimation: depth from unstructured illumination change and depth
	estimation in dynamic scenes. Based on our analysis, we show that
	methods derived from the spacetime stereo framework can be used to
	recover depth in situations in which existing methods perform poorly.},
  comment = {3D_stereo},
  doi = {10.1109/TPAMI.2005.37},
  file = {:papers\\2005 JNL, Spacetime stereo_ a unifying framework for depth from triangulation (Davis, Ramamoorthi, Rusinkiewicz).pdf:PDF},
  issn = {0162-8828},
  keywords = {depth estimation;depth from triangulation algorithm;dynamic scenes;spacetime
	stereo;mesh generation;stereo image processing;Algorithms;Artificial
	Intelligence;Computer Simulation;Image Enhancement;Image Interpretation,
	Computer-Assisted;Imaging, Three-Dimensional;Information Storage
	and Retrieval;Pattern Recognition, Automated;Photogrammetry;Reproducibility
	of Results;Sensitivity and Specificity;Subtraction Technique;Video
	Recording;},
  timestamp = {00,150}
}

@MASTERSTHESIS{2004_MAS_SURVEYtrk_Dedeoglu,
  author = {Dedeoglu, Yigithan},
  title = {Moving object detection, tracking and classification for smart video
	surveillance},
  school = {Bilkent University},
  year = {2004},
  comment = {survey},
  file = {:papers\\2004 MAS, Moving object detection, tracking and classification for smart video surveillance (Dedeoglu, Bilkent, 20).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@BOOK{1999_BK_PF_Moral,
  title = {Interacting particle filtering with discrete observations},
  publisher = {Univ. Paul Sabatier},
  year = {1999},
  author = {Del Moral, P. and Jacod, J.}
}

@ARTICLE{1977_JNL_MLEM_Dempster,
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  title = {Maximum likelihood from incomplete data via the EM algorithm},
  journal = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
  year = {1977},
  volume = {39},
  pages = {1-38},
  number = {1},
  abstract = {Your use of the JSTOR archive indicates your acceptance of JSTOR&#039;s
	Terms and Conditions of Use, available at http://www.jstor.org/about/terms.html.
	JSTOR&#039;s Terms and Conditions of Use provides, in part, that
	unless you have obtained prior permission, you may not download an
	entire issue of a journal or multiple copies of articles, and you
	may use content in the JSTOR archive only for your personal, non-commercial
	use. Please contact the publisher regarding any further use of this
	work. Publisher contact information may be obtained at},
  comment = {PRML},
  file = {:papers\\1977 JNL, Maximum Likelihood from Incomplete Data via the EM Algorithm (Dempster, JRS, 20831).pdf:PDF},
  owner = {salman},
  timestamp = {21,000}
}

@ARTICLE{2006_CNF_FourierSnakes_Derrode,
  author = {Derrode, S. and Charmi, M.A. and Ghorbel, F.},
  title = {Fourier-Based Invariant Shape Prior for Snakes},
  journal = {Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings.
	2006 IEEE International Conference on},
  year = {2006},
  volume = {2},
  pages = {II-II},
  month = {May},
  abstract = {A novel method of parametric active contours with geometric shape
	prior is presented in this paper. The main idea of the method consists
	in minimizing an energy functional that includes an additional information
	on a shape reference called "template" or "prototype". Prior shape
	knowledge is introduced throw a complete family of Euclidean invariants,
	computed from the Fourier descriptors of the evolving contour and
	the prototype. It enhances the model robustness to noise and occlusion,
	and allows it to evolve in highly concave boundaries. The variational
	formulation of the proposed approach is described in details. Experimental
	results on both synthetic and real images are presented and discussed},
  doi = {10.1109/ICASSP.2006.1660289},
  issn = {1520-6149},
  keywords = {Fourier analysis, image processingEuclidean invariants, Fourier-based
	invariant shape, geometric shape, parametric active contours, snakes}
}

@BOOK{1996_BOOK_PR_DevroyeGyorfiLugosi,
  title = {A Probabilistic Theory of Pattern Recognition (Stochastic Modelling
	and Applied Probability)},
  publisher = {Springer},
  year = {1996},
  author = {Devroye, Luc and Gyorfi, Laszlo and Lugosi, Gabor},
  edition = {Corrected},
  month = {April},
  abstract = {{Pattern recognition presents one of the most significant challenges
	for scientists and engineers, and many different approaches have
	been proposed. The aim of this book is to provide a self-contained
	account of probabilistic analysis of these approaches. The book includes
	a discussion of distance measures, nonparametric methods based on
	kernels or nearest neighbors, Vapnik-Chervonenkis theory, epsilon
	entropy, parametric classification, error estimation, free classifiers,
	and neural networks. Wherever possible, distribution-free properties
	and inequalities are derived. A substantial portion of the results
	or the analysis is new. Over 430 problems and exercises complement
	the material.}},
  comment = {PRML_book},
  day = {04},
  file = {:papers\\1996 BOOK, A probabilistic theory of pattern recognition (Devroye, Gyorfi, Lugosi, Springer).djvu:Djvu},
  howpublished = {Hardcover},
  isbn = {0387946187},
  posted-at = {2010-04-15 15:52:21},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387946187}
}

@ARTICLE{1989_JNL_StructureFromStereo_Dhond,
  author = {Dhond, U.R. and Aggarwal, J.K.},
  title = {Structure from stereo-a review},
  journal = {Systems, Man and Cybernetics, IEEE Transactions on},
  year = {1989},
  volume = {19},
  pages = {1489 -1510},
  number = {6},
  month = {nov/dec},
  abstract = {The authors review major recent developments in establishing stereo
	correspondence for the extraction of the 3D structure of a scene.
	Broad categories of stereo algorithms are identified on the basis
	of differences in imaging geometry, matching primitives, and the
	computational structure used. The performance of these stereo techniques
	on various classes of test images is reviewed, and possible directions
	of future research are indicated},
  comment = {3D_stereo},
  doi = {10.1109/21.44067},
  file = {:papers\\1989 JNL, Structure from stereo-a review (SURVEY, Dhond, Aggarwal).pdf:PDF},
  issn = {0018-9472},
  keywords = {3D structure recovery;computational structure;computerised picture
	processing;imaging geometry;matching primitives;pattern recognition;stereo
	correspondence;structure extraction;computerised pattern recognition;computerised
	picture processing;reviews;},
  timestamp = {00,750}
}

@TECHREPORT{1994_TechRep_TRKocclusion_Koller,
  author = {Dieter Koller, Joseph Weber, Jitendra Malik},
  title = {Robust Multiple Car Tracking with Occlusion Reasoning},
  institution = {California Path Program, Institute of Transportation Studies, University
	of California, Berkeley},
  year = {1994},
  comment = {TRK_occlusion},
  file = {:papers\\1994 REP, Robust Multiple Car Tracking with Occlusion Reasoning (Koller).pdf:PDF},
  timestamp = {00,400}
}

@ARTICLE{2002_JNL_VideoContentAnalysis_Dimitrova,
  author = {Dimitrova, N. and Hong-Jiang, Zhang and Shahraray, B. and Sezan,
	I. and Huang, T. and Zakhor, A.},
  title = {Applications of video-content analysis and retrieval},
  journal = {IEEE Multimedia},
  year = {2002},
  volume = {9},
  pages = {42-55},
  abstract = {Managing multimedia data requires more than collecting the data into
	storage archives and delivering it via networks to homes or offices.
	We survey technologies and applications for video-content analysis
	and retrieval. We also give specific examples},
  comment = {survey},
  file = {:papers\\2002 JNL, Applications of video-content analysis and retrieval (SURVEY, Zakhor, MMUL, 172).pdf:PDF},
  keywords = {content-based retrieval multimedia databases video databases},
  owner = {salman},
  timestamp = {00,180}
}

@INPROCEEDINGS{2004_CNF_ClusterStructure_DingHe,
  author = {Ding, C. and He, X.},
  title = {Cluster structure of K-means clustering via principal component analysis},
  year = {2004},
  pages = {414 - 18},
  address = {Berlin, Germany},
  note = {K-means clustering;principal component analysis;data clustering algorithm;statistical
	technique;eigenvalue;data covariance matrix;},
  abstract = {K-means clustering is a popular data clustering algorithm. Principal
	component analysis (PCA) is a widely used statistical technique for
	dimension reduction. Here we prove that principal components are
	the continuous solutions to the discrete cluster membership indicators
	for K-means clustering, with a clear simplex cluster structure. Our
	results prove that PCA-based dimension reductions are particularly
	effective for K-means clustering. New lower bounds for K-means objective
	function are derived, which is the total variance minus the eigenvalues
	of the data covariance matrix},
  comment = {TRK_subspace},
  copyright = {Copyright 2004, IEE},
  file = {:papers\\2004 CNF, Cluster structure of K-means clustering via principal component analysis (Ding, He).pdf:PDF},
  journal = {Advances in Knowledge Discovery and Data Mining. 8th Pacific-Asia
	Conference, PAKDD 2004. Proceedings (Lecture Notes in Artificial
	Intelligence Vol.3056)},
  keywords = {covariance matrices;data analysis;data mining;eigenvalues and eigenfunctions;pattern
	clustering;principal component analysis;},
  language = {English}
}

@INPROCEEDINGS{2004_CNF_KmeansVsPCA_DingHe,
  author = {Ding, Chris and He, Xiaofeng},
  title = {K-means clustering via principal component analysis},
  booktitle = {ICML '04: Proceedings of the twenty-first international conference
	on Machine learning},
  year = {2004},
  pages = {29},
  address = {New York, NY, USA},
  publisher = {ACM},
  comment = {TRK_subspace},
  doi = {http://doi.acm.org/10.1145/1015330.1015408},
  file = {:papers\\2004 CNF, K-means Clustering via Principal Component Analysis (Ding, He).pdf:PDF},
  isbn = {1-58113-828-5},
  location = {Banff, Alberta, Canada}
}

@ARTICLE{1996_JNL_MPEG4RateControl_Wei,
  author = {Wei Ding and Bede Liu},
  title = {Rate control of MPEG video coding and recording by rate-quantization
	modeling},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  year = {1996},
  volume = {6},
  pages = {12-20},
  number = {1},
  month = {Feb},
  abstract = {For MPEG video coding and recording applications, it is important
	to select the quantization parameters at slice and macroblock levels
	to produce consistent quality image for a given bit budget. A well-designed
	rate control strategy can improve the overall image quality for video
	transmission over a constant-bit-rate channel and fulfil the editing
	requirement of video recording, where a certain number of new pictures
	are encoded to replace consecutive frames on the storage media using,
	at most, the same number of bits. We developed a feedback re-encoding
	method with a rate-quantization model, which can be adapted to changes
	in picture activities. The model is used for quantization parameter
	selection at the frame and slice level. The extra computations needed
	are modest. Experiments show the accuracy of the model and the effectiveness
	of the proposed rate control method. A new bit allocation algorithm
	is then proposed for MPEG video coding},
  doi = {10.1109/76.486416},
  issn = {1051-8215},
  keywords = {code standards, feedback, quantisation (signal), telecommunication
	standards, video coding, video recordingMPEG video coding, MPEG video
	recording, bit allocation algorithm, constant bit rate channel, editing,
	experiments, feedback reencoding method, frame level, image quality,
	macroblock level, quantization parameter selection, quantization
	parameters, rate control method, rate quantization modeling, slice
	level, storage media, video transmission}
}

@ARTICLE{2001_JNL_MultipleCameraTracking_DockstaderTekalp,
  author = {Dockstader, S. L. and Tekalp, A. M.},
  title = {Multiple camera tracking of interacting and occluded human motion},
  journal = {Proceedings of the IEEE},
  year = {2001},
  volume = {89},
  pages = {1441-1455},
  number = {10},
  abstract = {We propose a distributed, real-time computing platform for tracking
	multiple interacting persons in motion. To combat the negative effects
	of occlusion and articulated motion we use a multiview implementation,
	where each view is first independently processed on a dedicated processor.
	This monocular processing uses a predictor-corrector filter to weigh
	reprojections of three-dimensional (3-D) position estimates, obtained
	by the central processor, against observations of measurable image
	motion. The corrected state vectors from each view provide input
	observations to a Bayesian belief network, in the central processor,
	with a dynamic, multidimensional topology that varies as a function
	of scene content and feature confidence. The Bayesian net fuses independent
	observations from multiple cameras by iteratively resolving independency
	relationships and confidence levels within the graph, thereby producing
	the most likely vector of 3-D state estimates given the available
	data. To maintain temporal continuity, we follow the network with
	a layer of Kalman filtering that updates the 3-D state estimates.
	We demonstrate the efficacy of the proposed system using a multiview
	sequence of several people in motion. Our experiments suggest that,
	when compared with data fusion based on averaging, the proposed technique
	yields a noticeable improvement in tracking accuracy},
  comment = {trk_people},
  file = {:papers\\2001 JNL, Multiple camera tracking of interacting and occluded human (Dockstader, Tekalp, ProcIEEE, 116).pdf:PDF},
  keywords = {Kalman filters belief networks distributed processing distributed
	tracking filtering theory image motion analysis image sequences prediction
	theory real-time systems sensor fusion state estimation surveillance
	3D position estimates 3D state estimates Bayesian belief network
	Kalman filtering articulated motion central processor confidence
	levels dedicated processor distributed real-time computing platform
	dynamic multidimensional topology feature confidence graph image
	motion independency relationships input observations interacting
	human motion monocular processing multiple camera fusion multiple
	camera tracking multiple interacting persons multiview implementation
	multiview sequence occluded human motion predictor-corrector filter
	real-time tracking scene content state vectors tracking accuracy},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{2001_JNL_TrackingOccluded_DockstaderTekalp,
  author = {Dockstader, S. L. and Tekalp, A. M.},
  title = {On the tracking of articulated and occluded video object motion},
  journal = {Real-Time Imaging},
  year = {2001},
  volume = {7},
  pages = {415-432},
  number = {Compendex},
  abstract = {This paper presents a novel approach to the tracking of multiple articulate
	objects in the presence of occlusion in moderately complex scenes.
	Most conventional tracking algorithms work well when only one object
	is tracked at a time. However, when multiple objects must be tracked
	simultaneously, significant computation is often introduced in order
	to handle occlusion and to calculate the appropriate region correspondence
	between successive frames. We introduce a near real-time solution
	to this problem by using a probabilistic mixing of low-level features
	and components. The algorithm mixes coarse motion estimates, change
	detection information, and unobservable predictions to create accurate
	trajectories of moving objects. We implement this multi-feature mixing
	strategy using a modified Kalman filtering mechanism. The proposed
	technique is presented within the context of a video surveillance
	system, where we further introduce a simple color-based technique
	for automatically extracting facial regions of moving persons. The
	single, most descriptive, facial snapshot for each person is added
	to a database that, when used in conjunction with the tracking data,
	can be used as a compact, semantic representation for the video sequence.
	Experimental results demonstrate the efficacy of the proposed tracking
	and surveillance system. 2001 Academic Press.},
  comment = {trk_people},
  file = {:papers\\2001 JNL, On the Tracking of Articulated and Occluded Video Object Motion (Dockstader, RTI, 28).pdf:PDF},
  keywords = {Object recognition Algorithms Color image processing Computation theory
	Computer vision Kalman filtering Motion estimation Probabilistic
	logics Real time systems},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{2001_JNL_PFjumpMarkov_Doucet,
  author = {Doucet, A. and Gordon, N.J. and Krishnamurthy, V.},
  title = {Particle filters for state estimation of jump Markov linear systems
	},
  journal = {Signal Processing, IEEE Transactions on},
  year = {2001},
  volume = {49},
  pages = {613 -624},
  number = {3},
  month = {mar},
  abstract = {Jump Markov linear systems (JMLS) are linear systems whose parameters
	evolve with time according to a finite state Markov chain. In this
	paper, our aim is to recursively compute optimal state estimates
	for this class of systems. We present efficient simulation-based
	algorithms called particle filters to solve the optimal filtering
	problem as well as the optimal fixed-lag smoothing problem. Our algorithms
	combine sequential importance sampling, a selection scheme, and Markov
	chain Monte Carlo methods. They use several variance reduction methods
	to make the most of the statistical structure of JMLS. Computer simulations
	are carried out to evaluate the performance of the proposed algorithms.
	The problems of on-line deconvolution of impulsive processes and
	of tracking a maneuvering target are considered. It is shown that
	our algorithms outperform the current methods},
  comment = {trk_filtering},
  doi = {10.1109/78.905890},
  file = {:papers\\2001 JNL, Particle filters for state estimation of jump Markov linear systems (Doucet, Gordon, Krishnamurthy, TSP, 339).pdf:PDF},
  issn = {1053-587X},
  keywords = {Markov chain Monte Carlo methods;computer simulations;finite state
	Markov chain;impulsive processes;jump Markov linear systems;maneuvering
	target tracking;on-line deconvolution;optimal filtering;optimal fixed-lag
	smoothing;optimal state estimates;particle filters;performance evaluation;sequential
	importance sampling;simulation-based algorithms;statistical structure;variance
	reduction methods;Markov processes;deconvolution;digital simulation;filtering
	theory;importance sampling;linear systems;optimisation;state estimation;tracking;},
  timestamp = {00,350}
}

@INBOOK{2009_BOOK_PF_Doucet,
  chapter = {{A Tutorial on Particle Filtering and Smoothing: Fifteen Years Later}},
  title = {The Oxford Handbook of Nonlinear Filtering},
  publisher = {Oxford University Press},
  year = {2009},
  author = {Doucet, Arnaud and Johansen, Adam M.},
  citeulike-article-id = {4940198},
  citeulike-linkout-0 = {http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic/johansen/publications/DJ09.pdf},
  comment = {trk_filtering},
  file = {:papers\\2009 INBOOK, A Tutorial on Particle Filtering and Smoothing Fifteen years Later (Doucet, Johansen, 28).pdf:PDF},
  keywords = {file-import-09-06-24},
  posted-at = {2009-06-24 01:14:12},
  priority = {2},
  timestamp = {-},
  url = {http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic/johansen/publications/DJ09.pdf}
}

@BOOK{2005_BOOK_PR_Duda,
  title = {Pattern Classification 2nd Edition with Computer Manual 2nd Edition
	Set},
  publisher = {Wiley-Interscience},
  year = {2004},
  author = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
  edition = {2},
  month = {June},
  comment = {PRML_book},
  day = {04},
  file = {:papers\\2004 BOOK, Pattern Classification (Duda, 2nd ed).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0471703508},
  posted-at = {2010-04-19 19:48:54},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0471703508}
}

@INPROCEEDINGS{1998_CNF_FaceActiveAppearance_Edwards,
  author = {Edwards, G.J. and Taylor, C.J. and Cootes, T.F.},
  title = {Interpreting face images using active appearance models},
  booktitle = {Automatic Face and Gesture Recognition, 1998. Proceedings. Third
	IEEE International Conference on},
  year = {1998},
  pages = {300 -305},
  month = {apr},
  abstract = {We demonstrate a fast, robust method of interpreting face images using
	an Active Appearance Model (AAM). An AAM contains a statistical model
	of shape and grey level appearance which can generalise to almost
	any face. Matching to an image involves finding model parameters
	which minimise the difference between the image and a synthesised
	face. We observe that displacing each model parameter from the correct
	value induces a particular pattern in the residuals. In a training
	phase, the AAM learns a linear model of the correlation between parameter
	displacements and the induced residuals. During search it measures
	the residuals and uses this model to correct the current parameters,
	leading to a better fit. A good overall match is obtained in a few
	iterations, even from poor starting estimates. We describe the technique
	in detail and show it matching to new face images},
  comment = {det_face},
  doi = {10.1109/AFGR.1998.670965},
  file = {:papers\\1998 CNF, Interpreting face images using active appearance models (Edwards, Taylor, Cootes, AFGR, 247).pdf:PDF},
  keywords = {AAM;Active Appearance Model;face image interpretation;grey level appearance;image
	matching;linear model;model parameter;model parameters;parameter
	displacements;statistical model;synthesised face;training phase;face
	recognition;image matching;search problems;statistical analysis;},
  timestamp = {00,250}
}

@BOOK{1982_BOOK_Jackknife_Efron,
  title = {The Jackknife, the Bootstrap, and Other Resampling Plans (CBMS-NSF
	Regional Conference Series in Applied Mathematics)},
  publisher = {Society for Industrial Mathematics},
  year = {1982},
  author = {Efron, Bradley},
  month = {January},
  abstract = {{The jackknife and the bootstrap are nonparametric methods for assessing
	the errors in a statistical estimation problem. They provide several
	advantages over the traditional parametric approach: the methods
	are easy to describe and they apply to arbitrarily complicated situations;
	distribution assumptions, such as normality, are never made. <P>This
	monograph connects the jackknife, the bootstrap, and many other related
	ideas such as cross-validation, random subsampling, and balanced
	repeated replications into a unified exposition. The theoretical
	development is at an easy mathematical level and is supplemented
	by a large number of numerical examples. <P>The methods described
	in this monograph form a useful set of tools for the applied statistician.
	They are particularly useful in problem areas where complicated data
	structures are common, for example, in censoring, missing data, and
	highly multivariate situations.}},
  comment = {PRML_book},
  day = {01},
  file = {:papers\\1982 BOOK, The jackknife, the bootstrap and other resampling plans (Efron, SIAM).djvu:Djvu},
  howpublished = {Paperback},
  isbn = {0898711797},
  posted-at = {2010-04-22 00:30:59},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0898711797}
}

@INPROCEEDINGS{2003_CNF_ActionRecognition_Efros,
  author = {Efros, A. A. and Berg, A. C. and Mori, G. and Malik, J.},
  title = {Recognizing action at a distance},
  booktitle = {Computer Vision, 2003. Proceedings. Ninth IEEE International Conference
	on},
  year = {2003},
  abstract = {Our goal is to recognize human action at a distance, at resolutions
	where a whole person may be, say, 30 pixels tall. We introduce a
	novel motion descriptor based on optical flow measurements in a spatiotemporal
	volume for each stabilized human figure, and an associated similarity
	measure to be used in a nearest-neighbor framework. Making use of
	noisy optical flow measurements is the key challenge, which is addressed
	by treating optical flow not as precise pixel displacements, but
	rather as a spatial pattern of noisy measurements which are carefully
	smoothed and aggregated to form our spatiotemporal motion descriptor.
	To classify the action being performed by a human figure in a query
	sequence, we retrieve nearest neighbor(s) from a database of stored,
	annotated video sequences. We can also use these retrieved exemplars
	to transfer 2D/3D skeletons onto the figures in the query sequence,
	as well as two forms of data-based action synthesis "do as I do"
	and "do as I say". Results are demonstrated on ballet, tennis as
	well as football datasets.},
  comment = {recog_action_opticalFlow},
  file = {:papers\\2003 CNF, Recognizing action at a distance (Efros, Mori, Malik, ICCV, 407).pdf:PDF},
  keywords = {computer vision image motion analysis image retrieval image sequences
	video signal processing 2D skeletons 3D skeletons action classification
	ballet dataset data-based action synthesis distant action recognition
	football dataset human action motion descriptor nearest neighbor
	retrieval nearest-neighbor framework noisy optical flow optical flow
	measurements query sequence retrieved exemplars similarity measure
	spatial pattern spatiotemporal volume stabilized human figure tennis
	dataset video sequences database},
  owner = {salman},
  timestamp = {00,400}
}

@ARTICLE{7778583,
  author = {Elgammal, A. and Duraiswami, R. and Davis, L.S.},
  title = {Efficient kernel density estimation using the fast gauss transform
	with applications to color modeling and tracking},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2003},
  volume = { 25},
  pages = {1499 - 504},
  number = { 11},
  note = {vision algorithms;density function;kernel density estimation;fast
	Gauss transform;color modeling;tracking;image segmentation;image
	tracking;Gauss transform;},
  abstract = {Many vision algorithms depend on the estimation of a probability density
	function from observations. Kernel density estimation techniques
	are quite general and powerful methods for this problem, but have
	a significant disadvantage in that they are computationally intensive.
	In this paper, we explore the use of kernel density estimation with
	the fast Gauss transform (FGT) for problems in vision. The FGT allows
	the summation of a mixture of ill Gaussians at N evaluation points
	in O(M+N) time, as opposed to O(MN) time for a naive evaluation and
	can be used to considerably speed up kernel density estimation. We
	present applications of the technique to problems from image segmentation
	and tracking and show that the algorithm allows application of advanced
	statistical techniques to solve practical vision problems in real-time
	with today's computers},
  address = {USA},
  comment = {trk_color},
  copyright = {Copyright 2003, IEE},
  file = {:papers\\2003 JNL, Efficient kernel density estimation using the fast gauss transform with applications to color modeling and tracking (Elgammal, Duraiswami, Davis).pdf:PDF},
  issn = {0162-8828},
  keywords = {computer vision;image segmentation;transforms;},
  language = {English},
  timestamp = {00,100},
  url = {http://dx.doi.org/10.1109/TPAMI.2003.1240123}
}

@ARTICLE{2002_JNL_BGFGkernelDensityEstimation_Elgammal,
  author = {Elgammal, A. and Duraiswami, R. and Harwood, D. and Davis, L. S.},
  title = {Background and foreground modeling using nonparametric kernel density
	estimation for visual surveillance},
  journal = {Proceedings of the IEEE},
  year = {2002},
  volume = {90},
  pages = {1151-1163},
  number = {7},
  abstract = {Automatic understanding of events happening at a site is the ultimate
	goal for many visual surveillance systems. Higher level understanding
	of events requires that certain lower level computer vision tasks
	be performed. These may include detection of unusual motion, tracking
	targets, labeling body parts, and understanding the interactions
	between people. To achieve many of these tasks, it is necessary to
	build representations of the appearance of objects in the scene.
	This paper focuses on two issues related to this problem. First,
	we construct a statistical representation of the scene background
	that supports sensitive detection of moving objects in the scene,
	but is robust to clutter arising out of natural scene variations.
	Second, we build statistical representations of the foreground regions
	(moving objects) that support their tracking and support occlusion
	reasoning. The probability density functions (pdfs) associated with
	the background and foreground are likely to vary from image to image
	and will not in general have a known parametric form. We accordingly
	utilize general nonparametric kernel density estimation techniques
	for building these statistical representations of the background
	and the foreground. These techniques estimate the pdf directly from
	the data without any assumptions about the underlying distributions.
	Example results from applications are presented.},
  comment = {trk_BG},
  file = {:papers\\2002 JNL, Background and foreground modeling  using nonparametric  kernel density  estimation for  visual surveillance (Elgammal, Duraiswami, David Harwood, Larry Davis, ProcIEEE, 505).pdf:PDF},
  keywords = {computer vision image representation nonparametric statistics parameter
	estimation surveillance automatic understanding kernel density estimation
	nonparametric occlusion modeling probability density functions scene
	background statistical representation visual surveillance},
  owner = {salman},
  timestamp = {00,500}
}

@INPROCEEDINGS{2000_CNF_BackgroundSubtraction_Elgammal,
  author = {Elgammal, Ahmed and Harwood, David and Davis, Larry},
  title = {Non-parametric model for background subtraction},
  booktitle = {FRAME-RATE Workshop, IEEE},
  year = {2000},
  pages = {751-767},
  abstract = {Abstract. Background subtraction is a method typically used to segment
	moving regions in image sequences taken from a static camera by comparing
	each new frame to a model of the scene background. We present anovel
	non-parametric background model and a background subtraction approach.
	The model can handle situations where the background of the scene
	is cluttered and not completely static but contains small motions
	such as tree branches and bushes. The model estimates the probability
	of observing pixel intensity values based on a sample of intensity
	values for each pixel. The model adapts quickly to changes in the
	scene which enables very sensitive detection of moving targets. We
	also show how the model can use color information to suppress detection
	of shadows. The implementation of the model runs in real-time for
	both gray level and color imagery. Evaluation shows that this approach
	achieves very sensitive detection with very low false alarm rates.
	Key words: visual motion, active and real time vision, motion detection,
	non-parametric estimation, visual surveillance, shadow detection
	1},
  comment = {trk_BG},
  file = {:papers\\2000 CNF, Non-parametric model for background subtraction (Elgammal, Harwood, Davis, ECCV, 808).pdf:PDF},
  owner = {salman},
  timestamp = {00,800}
}

@ARTICLE{20100612698097,
  author = {Elmezain, Mahmoud and Al-Hamadi, Ayoub and Niese, Robert and Michaelis,
	Bernd},
  title = {A robust method for hand tracking using mean-shift algorithm and
	Kalman filter in stereo color image sequences},
  journal = {Proceedings of World Academy of Science, Engineering and Technology},
  year = {2009},
  volume = {59},
  pages = {283 - 287},
  abstract = {Real-time hand tracking is a challenging task in many computer vision
	applications such as gesture recognition. This paper proposes a robust
	method for hand tracking in a complex environment using Mean-shift
	analysis and Kalman filter in conjunction with 3D depth map. The
	depth information solve the overlapping problem between hands and
	face, which is obtained by passive stereo mea- suring based on cross
	correlation and the known calibration data of the cameras. Mean-shift
	analysis uses the gradient of Bhattacharyya coefficient as a similarity
	function to derive the candidate of the hand that is most similar
	to a given hand target model. And then, Kalman filter is used to
	estimate the position of the hand target. The results of hand tracking,
	tested on various video sequences, are robust to changes in shape
	as well as partial occlusion.},
  address = {United States},
  comment = {trk_color},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2009 JNL, A robust method for hand tracking using mean-shift algorithm and Kalman filter in stereo color image sequences (Elmezain et al.).pdf:PDF},
  issn = {20703740},
  key = {Gesture recognition},
  keywords = {Computer applications;Computer vision;Image analysis;Kalman filters;Three
	dimensional;Video recording;},
  language = {English},
  timestamp = {-}
}

@ARTICLE{1999_JNL_stab_Engelsberg,
  author = {Engelsberg, A. and Schmidt, G.},
  title = {A comparative review of digital image stabilising algorithms for
	mobile video communications},
  journal = {Consumer Electronics, IEEE Transactions on},
  year = {1999},
  volume = {45},
  pages = {591--597},
  number = {3},
  publisher = {IEEE}
}

@ARTICLE{2009_JNL_SurveyPedestrianDetection,
  author = {Enzweiler, M. and Gavrila, D.M.},
  title = {Monocular Pedestrian Detection: Survey and Experiments},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2009},
  volume = {31},
  pages = {2179 -2195},
  number = {12},
  month = {dec. },
  abstract = {Pedestrian detection is a rapidly evolving area in computer vision
	with key applications in intelligent vehicles, surveillance, and
	advanced robotics. The objective of this paper is to provide an overview
	of the current state of the art from both methodological and experimental
	perspectives. The first part of the paper consists of a survey. We
	cover the main components of a pedestrian detection system and the
	underlying models. The second (and larger) part of the paper contains
	a corresponding experimental study. We consider a diverse set of
	state-of-the-art systems: wavelet-based AdaBoost cascade, HOG/linSVM,
	NN/LRF, and combined shape-texture detection. Experiments are performed
	on an extensive data set captured onboard a vehicle driving through
	urban environment. The data set includes many thousands of training
	samples as well as a 27-minute test sequence involving more than
	20,000 images with annotated pedestrian locations. We consider a
	generic evaluation setting and one specific to pedestrian detection
	onboard a vehicle. Results indicate a clear advantage of HOG/linSVM
	at higher image resolutions and lower processing speeds, and a superiority
	of the wavelet-based AdaBoost cascade approach at lower image resolutions
	and (near) real-time processing speeds. The data set (8.5 GB) is
	made public for benchmarking purposes.},
  comment = {survey},
  doi = {10.1109/TPAMI.2008.260},
  file = {:papers\\2009 JNL, Monocular Pedestrian Detection_ Survey and Experiments (SURVEY, Enzweiler, Gavrila, PAMI, 7).pdf:PDF},
  issn = {0162-8828},
  keywords = {HOG/linSVM;NN/LRF;advanced robotics;annotated pedestrian location;combined
	shape-texture detection;computer vision;image resolutions;intelligent
	vehicles;monocular pedestrian detection;surveillance;wavelet-based
	AdaBoost cascade;computer vision;image resolution;image texture;learning
	(artificial intelligence);object detection;shape recognition;traffic
	engineering computing;wavelet transforms;},
  timestamp = {-}
}

@ARTICLE{2009_JNL_MPTmobilePlatform_Ess,
  author = {Ess, A. and Leibe, B. and Schindler, K. and van Gool, L.},
  title = {Robust Multiperson Tracking from a Mobile Platform},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2009},
  volume = {31},
  pages = {1831 -1846},
  number = {10},
  month = {oct. },
  abstract = {In this paper, we address the problem of multiperson tracking in busy
	pedestrian zones using a stereo rig mounted on a mobile platform.
	The complexity of the problem calls for an integrated solution that
	extracts as much visual information as possible and combines it through
	cognitive feedback cycles. We propose such an approach, which jointly
	estimates camera position, stereo depth, object detection, and tracking.
	The interplay between those components is represented by a graphical
	model. Since the model has to incorporate object-object interactions
	and temporal links to past frames, direct inference is intractable.
	We, therefore, propose a two-stage procedure: for each frame, we
	first solve a simplified version of the model (disregarding interactions
	and temporal continuity) to estimate the scene geometry and an overcomplete
	set of object detections. Conditioned on these results, we then address
	object interactions, tracking, and prediction in a second step. The
	approach is experimentally evaluated on several long and difficult
	video sequences from busy inner-city locations. Our results show
	that the proposed integration makes it possible to deliver robust
	tracking performance in scenes of realistic complexity.},
  comment = {trk_people},
  doi = {10.1109/TPAMI.2009.109},
  file = {:papers\\2009 JNL, Robust Multiperson Tracking from a Mobile Platform (Ess, Liebe, Schindler, vanGool, PAMI).pdf:PDF},
  issn = {0162-8828},
  keywords = {camera position;cognitive feedback cycle;graphical model;mobile platform;object
	detection;object-object interaction;pedestrian zones;robust muitiperson
	tracking;stereo rig;video sequences;visual information;computer vision;mobile
	computing;object detection;tracking;},
  timestamp = {-}
}

@INPROCEEDINGS{2010_CNF_camera_Evans,
  author = {Evans, M. and Ferryman, J.},
  title = {Surveillance Camera Calibration from Observations of a Pedestrian},
  booktitle = {Advanced Video and Signal Based Surveillance (AVSS), 2010 Seventh
	IEEE International Conference on},
  year = {2010},
  pages = {64 -71},
  month = {292010-sept.1},
  abstract = {Calibrated cameras are an extremely useful resource for computer vision
	scenarios. Typically, cameras are calibrated through calibration
	targets, measurements of the observed scene, or self-calibrated through
	features matched between cameras with overlapping fields of view.
	This paper considers an approach to camera calibration based on observations
	of a pedestrian and compares the resulting calibration to a commonly
	used approach requiring that measurements be made of the scene.},
  comment = {camera},
  doi = {10.1109/AVSS.2010.32},
  file = {:papers\\2010 CNF, Surveillance Camera Calibration from Observations of a Pedestrian (Evans).pdf:PDF},
  keywords = {calibrated cameras;calibration targets;computer vision scenarios;pedestrian
	observation;surveillance camera calibration;calibration;cameras;computer
	vision;surveillance;},
  timestamp = {-}
}

@ARTICLE{fan1949theorem,
  author = {Fan, K.},
  title = {On a theorem of Weyl concerning eigenvalues of linear transformations},
  journal = {Proceedings of the National Academy of Sciences of the United States
	of America},
  year = {1949},
  volume = {35},
  pages = {652},
  number = {11},
  comment = {PRML},
  file = {:papers\\1949 JNL, On a theorem of Weyl concerning eigenvalues of linear transformations.pdf:PDF},
  publisher = {National Academy of Sciences},
  timestamp = {00,250}
}

@INPROCEEDINGS{1997_CNF_ColorHeadTracking_Fieguth,
  author = {Fieguth, P. and Terzopoulos, D.},
  title = {Color-based tracking of heads and other mobile objects at video frame
	rates},
  booktitle = {Computer Vision and Pattern Recognition. Proceedings., IEEE Computer
	Society Conference on},
  year = {1997},
  pages = {21-27},
  abstract = {We develop a simple and very fast method for object tracking based
	exclusively on color information in digitized video images. Running
	on a Silicon Graphics R4600 Indy system with an IndyCam, our algorithm
	is capable of simultaneously tracking objects at full frame size
	(640&times;480 pixels) and video frame rate (30 fps). Robustness
	with respect to occlusion is achieved via can explicit hypothesis-tree
	model of the occlusion process. We demonstrate the efficacy of our
	technique in the challenging task of tracking people, especially
	tracking human heads and hands},
  comment = {trk_people, trk_color},
  file = {:papers\\1997 CNF, Color-Based Tracking of Heads and Other Mobile Objects at Video (Fieguth, Terzopoulos, CVPR, 192).pdf:PDF},
  keywords = {colour vision computer vision motion estimation tracking Indy system
	IndyCam Silicon Graphics R4600 color information digitized video
	images object tracking occlusion tracking people video frame rates},
  owner = {salman},
  timestamp = {00,200}
}

@ARTICLE{1964_JNL_DigComm_Fine,
  author = {Fine, T.},
  title = {Properties of an optimum digital system and applications},
  journal = {Information Theory, IEEE Transactions on},
  year = {1964},
  volume = {10},
  pages = { 287 - 296},
  number = {4},
  month = oct,
  abstract = { A model representative of any time-sampled, real-valued input and
	output digital system without feedback from system input to output
	(there may be internal feedback loops) is presented, and properties
	that are necessary for an optimum (nonlinear) digital system are
	developed. The model itself consists of a cascade of three mapping
	operations that successively transform the real-valued input sequence
	into an M-ary valued digital sequence, thence into anN-ary valued
	digital sequence, and finally into a real-valued output sequence.
	The operating routine and optimality criterion function chosen permit
	us to consider the problems of prediction, interpolation or synchronous
	operation as well as a wide variety of loss functions. The derived
	properties are discussed with particular regard to the quadratic
	loss function and synchronous or interpolatory operation and then
	applied to the determination of several optimum systems. Optimum
	nonlinear amplitude quantizers, pulse code and delta modulation systems,
	and stepped controllers are described. In particular, conditions
	are given under which delta modulation is superior to pulse code
	modulation and some remarks are made concerning suboptimal delta
	modulation systems and their relation to previous work.},
  comment = {Comm},
  doi = {10.1109/TIT.1964.1053707},
  file = {:papers\\1964 JNL, Properties of an optimum digital system and applications (Fine).pdf:PDF},
  issn = {0018-9448},
  keywords = { Digital communications; Optimization methods;},
  timestamp = {?}
}

@ARTICLE{1981_JNL_RANSAC_Fischler,
  author = {Fischler, M.A. and Bolles, R.C.},
  title = {Random sample consensus: a paradigm for model fitting with applications
	to image analysis and automated cartography},
  journal = {Communications of the ACM},
  year = {1981},
  volume = { 24},
  pages = {381 - 95},
  number = {6},
  abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model
	to experimental data is introduced. RANSAC is capable of interpreting/smoothing
	data containing a significant percentage of gross errors, and is
	thus ideally suited for applications in automated image analysis
	where interpretation is based on the data provided by error-prone
	feature detectors. The authors describe the application of RANSAC
	to the Location Determination Problem (LDP): given an image depicting
	a set of landmarks with known locations, determine that point in
	space from which the image was obtained. In response to a RANSAC
	requirement, new results are derived on the minimum number of landmarks
	needed to obtain a solution, and algorithms are presented for computing
	these minimum-landmark solutions in closed form. These results provide
	the basis for an automatic system that can solve the LDP under difficult
	viewing and analysis conditions. Implementation details and computational
	examples are also presented},
  address = {USA},
  comment = {PRML},
  copyright = {Copyright 1981, IEE},
  file = {:papers\\1981 JNL, Random sample consensus_ A paradigm for model fitting with application to image analysis and automated cartography (Fischler, Bolles, CACM. 4389).pdf:PDF},
  issn = {0001-0782},
  keywords = {computerised picture processing;},
  language = {English},
  timestamp = {04,000},
  url = {http://dx.doi.org/10.1145/358669.358692}
}

@ARTICLE{1973_JNL_MatchingPictorialStructures_Fischler,
  author = {Fischler, M.A. and Elschlager, R.A.},
  title = {The Representation and Matching of Pictorial Structures},
  journal = {Computers, IEEE Transactions on},
  year = {1973},
  volume = {C-22},
  pages = { 67 - 92},
  number = {1},
  month = {jan.},
  abstract = {The primary problem dealt with in this paper is the following. Given
	some description of a visual object, find that object in an actual
	photograph. Part of the solution to this problem is the specification
	of a descriptive scheme, and a metric on which to base the decision
	of "goodness" of matching or detection.},
  comment = {?},
  file = {:papers\\1973 JNL, The representation and matching of pictorial structures (Fischler, Elschlager, TC, 450).pdf:PDF},
  issn = {0018-9340},
  keywords = { Dynamic programming, heuristic optimization, picture description,
	picture matching, picture processing, representation.;},
  timestamp = {00,500}
}

@BOOK{2002_BOOK_CV_Forsyth,
  title = {Computer Vision: A Modern Approach},
  publisher = {Prentice Hall},
  year = {2002},
  author = {Forsyth, David A. and Ponce, Jean},
  abstract = {{The accessible presentation of this book gives both a general view
	of the entire computer vision enterprise and also offers sufficient
	detail to be able to build useful applications. Users learn techniques
	that have proven to be useful by first-hand experience and a wide
	range of mathematical methods. A <B>CD-ROM with every copy of the
	text</I></B> contains source code for programming practice, color
	images, and illustrative movies. Comprehensive and up-to-date, this
	book includes essential topics that either reflect practical significance
	or are of theoretical importance. Topics are discussed in substantial
	and increasing depth. <B>Application surveys</I></B> describe numerous
	important application areas such as image based rendering and digital
	libraries. Many important algorithms broken down and illustrated
	in pseudo code. Appropriate for use by engineers as a comprehensive
	reference to the computer vision enterprise.}},
  comment = {CV_book},
  day = {24},
  file = {:papers\\2002 BOOK, Computer Vision - A Modern Approach (Forsyth, Ponce).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0130851981},
  keywords = {machine\_learning, overview, vision},
  posted-at = {2007-10-31 23:10:48},
  priority = {0},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0130851981}
}

@ARTICLE{1983_JNL_JPDAF_Fortmann,
  author = {Fortmann, T. and Bar-Shalom, Y. and Scheffe, M.},
  title = {Sonar tracking of multiple targets using joint probabilistic data
	association},
  journal = {Oceanic Engineering, IEEE Journal of},
  year = {1983},
  volume = {8},
  pages = { 173 - 184},
  number = {3},
  month = {jul},
  abstract = { The problem of associating data with targets in a cluttered multi-target
	environment is discussed and applied to passive sonar tracking. The
	probabilistic data association (PDA) method, which is based on computing
	the posterior probability of each candidate measurement found in
	a validation gate, assumes that only one real target is present and
	all other measurements are Poisson-distributed clutter. In this paper,
	a new theoretical result is presented: the joint probabilistic data
	association (JPDA) algorithm, in which joint posterior association
	probabilities are computed for multiple targets (or multiple discrete
	interfering sources) in Poisson clutter. The algorithm is applied
	to a passive sonar tracking problem with multiple sensors and targets,
	in which a target is not fully observable from a single sensor. Targets
	are modeled with four geographic states, two or more acoustic states,
	and realistic (i.e., low) probabilities of detection at each sample
	time. A simulation result is presented for two heavily interfering
	targets illustrating the dramatic tracking improvements obtained
	by estimating the targets' states using joint association probabilities.},
  comment = {trk_correspondence},
  file = {:papers\\1983 JNL, Sonar tracking of multiple targets using joint probabilistic data association (Fortmann, JOE, 8).pdf:PDF},
  issn = {0364-9059},
  keywords = { Kalman filtering; Poisson distributions; Sonar tracking;},
  timestamp = {00,400}
}

@ARTICLE{1998_JNL_AutonomousDriving_Franke,
  author = {Franke, U. and Gavrila, D. and Gorzig, S. and Lindner, F. and Puetzold,
	F. and Wohler, C.},
  title = {Autonomous driving goes downtown},
  journal = {Intelligent Systems and their Applications, IEEE},
  year = {1998},
  volume = {13},
  pages = {40 -48},
  number = {6},
  month = {nov/dec},
  abstract = {Most computer-vision systems for vehicle guidance are for highway
	scenarios. Developing autonomous or driver-assistance systems for
	complex urban traffic poses new algorithmic and system-architecture
	challenges. To address these issues, the authors introduce their
	intelligent Stop amp;Go system and discuss appropriate algorithms
	and approaches for vision-module control},
  comment = {problem},
  doi = {10.1109/5254.736001},
  file = {:papers\\1999 JNL, Autonomous driving goes downtown (Franke, Gavrila, Gorzig, Lindner, Paetzold, Wohler, IS, 51).pdf:PDF},
  issn = {1094-7167},
  keywords = {complex urban traffic;computer-vision systems;driver-assistance systems;highway
	scenarios;intelligent Stop amp;Go system;vehicle guidance;vision-module
	control;computer vision;driver information systems;},
  timestamp = {00,050}
}

@ARTICLE{1991_JNL_SteerableFilters_Freeman,
  author = {Freeman, W.T. and Adelson, E.H.},
  title = {The design and use of steerable filters},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1991},
  volume = {13},
  pages = {891 -906},
  number = {9},
  month = {sep},
  abstract = {The authors present an efficient architecture to synthesize filters
	of arbitrary orientations from linear combinations of basis filters,
	allowing one to adaptively steer a filter to any orientation, and
	to determine analytically the filter output as a function of orientation.
	Steerable filters may be designed in quadrature pairs to allow adaptive
	control over phase as well as orientation. The authors show how to
	design and steer the filters and present examples of their use in
	the analysis of orientation and phase, angularly adaptive filtering,
	edge detection, and shape from shading. One can also build a self-similar
	steerable pyramid representation. The same concepts can be generalized
	to the design of 3-D steerable filters},
  comment = {SP},
  doi = {10.1109/34.93808},
  file = {:papers\\1991 JNL, The design  and use of steerable filters (Freeman, Adelson, PAMI, 1623).pdf:PDF},
  issn = {0162-8828},
  keywords = {adaptive filtering;design;edge detection;picture processing;shape
	from shading;steerable filters;adaptive filters;filtering and prediction
	theory;picture processing;},
  timestamp = {01,600}
}

@ARTICLE{1997_JNL_GeneralizationofLearningBoosting_Freund,
  author = {Freund, Y. and Schapire, R.E.},
  title = {A decision-theoretic generalization of on-line learning and an application
	to boosting},
  journal = {Journal of Computer and System Sciences},
  year = {1997},
  volume = {55},
  pages = {119 - 39},
  number = { 1},
  abstract = {In the first part of the paper we consider the problem of dynamically
	apportioning resources among a set of options in a worst-case on-line
	framework. The model we study can be interpreted as a broad, abstract
	extension of the well-studied on-line prediction model to a general
	decision-theoretic setting. We show that the multiplicative weight-update
	Littlestone-Warmuth rule can be adapted to this model, yielding bounds
	that are slightly weaker in some cases, but applicable to a considerably
	more general class of learning problems. We show how the resulting
	learning algorithm can be applied to a variety of problems, including
	gambling, multiple-outcome prediction, repeated games, and prediction
	of points in <b>R</b><sup>n</sup>. In the second part of the paper
	we apply the multiplicative weight-update technique to derive a new
	boosting algorithm. This boosting algorithm does not require any
	prior knowledge about the performance of the weak learning algorithm.
	We also study generalizations of the new boosting algorithm to the
	problem of learning functions whose range, rather than being binary,
	is an arbitrary finite set or a bounded segment of the real line},
  address = {USA},
  comment = {PRML},
  copyright = {Copyright 1997, IEE},
  file = {:papers\\1997 JNL, A decision-theoretic generalization of on-line learning and an application to boosting (Freund, Schapire, JCSS, 4469).pdf:PDF},
  issn = {0022-0000},
  keywords = {decision theory;learning (artificial intelligence);resource allocation;},
  language = {English},
  timestamp = {04,500},
  url = {http://dx.doi.org/10.1006/jcss.1997.1504}
}

@ARTICLE{1998_JNL_AdditiveLogisticRegression,
  author = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
  title = {Additive Logistic Regression: a Statistical View of Boosting},
  journal = {Annals of Statistics},
  year = {1998},
  volume = {28},
  pages = {2000},
  comment = {PRML},
  file = {:papers\\1998 JNL, Additive logistic regression_ A statistical view of boosting (Friedman, Hastie, Tibshirani, AS, 1952).pdf:PDF},
  timestamp = {02,000}
}

@INPROCEEDINGS{1991_CNF_DesignPerformanceRVQ_Frost,
  author = {Frost, R.L. and Barnes, C.F. and Xu, F.},
  title = {Design and performance of residual quantizers},
  booktitle = {Data Compression Conference, 1991. DCC '91.},
  year = {1991},
  abstract = {This paper shows that tree search encoders are ineffective when used
	to determine the closest code vector in residual quantizer (RQ) alphabets.
	In particular, the equivalent cell boundaries are poorly chosen and
	the labelling of equivalent code vectors produced by the decoder
	and by a tree-structured encoder are inconsistent. This problem does
	not arise when two-level RQ alphabets are used in trellis coded vector
	quantizers. Trellis-coded RQs are designed for the memoryless Gaussian,
	Laplacian, and Gauss-Markov sources at a rate of R=1 bit per sample
	with encouraging results; a SQNR of 5.92 dB has been achieved on
	the Gaussian source},
  comment = {VQ_RVQ},
  doi = {10.1109/DCC.1991.213380},
  file = {:papers\\1991 CNF, Design and performance of residual quantizers (Frost, Barnes, Xu) .pdf:PDF},
  keywords = { Gauss-Markov sources; Gaussian source; Laplacian source; equivalent
	cell boundaries; performance; residual quantizer; tree search encoders;
	trellis coded vector quantizers; data compression; encoding;},
  timestamp = {-}
}

@INPROCEEDINGS{2010_CNF_SURVEY_traffic_Fu,
  author = {Meng-Yin Fu and Yuan-Shui Huang},
  title = {A survey of traffic sign recognition},
  booktitle = {Wavelet Analysis and Pattern Recognition (ICWAPR), 2010 International
	Conference on},
  year = {2010},
  pages = {119 -124},
  month = july,
  abstract = {Advanced Driver Assistance Systems (ADAS) refer to various high-tech
	in-vehicle systems that are designed to increase road traffic safety
	by helping drivers gain better awareness of the road and its potential
	hazards as well as other drivers around them. The design of traffic
	sign recognition, one important subsystem of ADAS, has been a challenge
	problem for many years and hence become an important and active research
	topic in the area of intelligent transport systems. The realization
	of a real-time traffic sign recognition system is usually divided
	into three stages: detection, tracking and classification. This paper
	introduces the main difficulties in road sign recognition and briefly
	surveys the state-of-the-art technologies in this field with further
	discussions on the potential trend of development of road sign recognition.},
  comment = {SURVEY_traffic},
  doi = {10.1109/ICWAPR.2010.5576425},
  file = {:c\:\\salman\\work\\writing\\papers\\2010 CNF, A survey of traffic sign recognition (Fu).pdf:PDF},
  keywords = {advanced driver assistance systems;high-tech in-vehicle systems;intelligent
	transport systems;road traffic safety;traffic sign recognition;driver
	information systems;image recognition;road safety;road traffic;},
  timestamp = {-}
}

@INPROCEEDINGS{1998_CNF_HumanMotionAnalysis_Fujiyoshi,
  author = {Fujiyoshi, H. and Lipton, A. J.},
  title = {Real-time human motion analysis by image skeletonization},
  booktitle = {Proceedings of WACV98 - Computer Vision, 19-21 Oct. 1998},
  year = {1998},
  abstract = {In this paper a process is described for analysing the motion of a
	human target in a video stream. Moving targets are detected and their
	boundaries extracted. From these, a star skeleton is produced. Two
	motion cues are determined from this skeletonization: body posture,
	and cyclic motion of skeleton segments. These cues are used to determine
	human activities such as walking or running, and even potentially,
	the target's gait. Unlike other methods, this does not require an
	a priori human model, or a large number of pixels on target. Furthermore,
	it is computationally inexpensive, and thus ideal for real-world
	video applications such as outdoor video surveillance},
  comment = {recog_action_periodicity},
  file = {:papers\\1998 CNF, Real-time human motion analysis by image skeletonization (Lipton, WACV, 293).pdf:PDF},
  keywords = {image processing signal thinning video *file-import-10-03-10},
  timestamp = {00,300}
}

@ARTICLE{1975_JNL_GradientOfPDF_Fukunaga,
  author = { Fukunaga, K. and Hostetler, L.},
  title = {The estimation of the gradient of a density function, with applications
	in pattern recognition},
  journal = {Information Theory, IEEE Transactions on},
  year = {1975},
  volume = {21},
  pages = { 32 - 40},
  number = {1},
  month = {jan},
  abstract = { Nonparametric density gradient estimation using a generalized kernel
	approach is investigated. Conditions on the kernel functions are
	derived to guarantee asymptotic unbiasedness, consistency, and uniform
	consistency of the estimates. The results are generalized to obtain
	a simple mcan-shift estimate that can be extended in ak-nearest-neighbor
	approach. Applications of gradient estimation to pattern recognition
	are presented using clustering and intrinsic dimensionality problems,
	with the ultimate goal of providing further understanding of these
	problems in terms of density gradients.},
  comment = {feature},
  file = {:papers\\1975 JNL, The estimation of the gradient of a density function, with applications in pattern recognition (Fukunaga, Hostetler, TIT, 768).pdf:PDF},
  issn = {0018-9448},
  keywords = { Nonparametric estimation; Pattern recognition; Probability functions;},
  timestamp = {00,800}
}

@INPROCEEDINGS{2006_JNL_TRKsubs_Fussenegger,
  author = {Fussenegger, M. and Roth, P.M. and Bischof, H. and Pinz, A.},
  title = {On-line, incremental learning of a robust active shape model},
  year = {2006},
  pages = {122 - 31},
  address = {Berlin, Germany},
  note = {online incremental learning;robust active shape model;principal component
	analysis;training data;partial occlusions;image segmentation;},
  abstract = {Active shape models are commonly used to recognize and locate different
	aspects of known rigid objects. However, they require an off-line
	learning stage, such that the extension of an existing model requires
	a complete new retraining phase. Furthermore, learning is based on
	principal component analysis and requires perfect training data that
	is not corrupted by partial occlusions or imperfect segmentation.
	The contribution of this paper is twofold: First, we present a novel
	robust active shape model that can handle corrupted shape data. Second,
	this model can be created on-line through the use of a robust incremental
	PCA algorithm. Thus, an already partially learned active shape model
	can be used for segmentation of a new image in a level set framework
	and the result of this segmentation process can be used for an on-line
	update of the robust model. Our experimental results demonstrate
	the robustness and the flexibility of this new model, which is at
	the same time computationally much more efficient than previous ASMs
	using batch or iterated batch PCA},
  comment = {TRK_subspace},
  copyright = {Copyright 2007, The Institution of Engineering and Technology},
  file = {:papers\\2006 JNL, On incremental and robust learning (Fussenegger).pdf:PDF},
  journal = {Pattern Recognition. 28th DAGM Symposium. Proceedings (Lecture Notes
	in Computer Science Vol.4174)},
  keywords = {image segmentation;learning (artificial intelligence);principal component
	analysis;},
  language = {English}
}

@INPROCEEDINGS{2000_CNF_ErrorBackgroundAdaption_Gao,
  author = {Xiang Gao and Boult, T.E. and Coetzee, F. and Ramesh, V.},
  title = {Error analysis of background adaption},
  booktitle = {Computer Vision and Pattern Recognition, 2000. Proceedings. IEEE
	Conference on},
  year = {2000},
  volume = {1},
  pages = {503 -510 vol.1},
  abstract = {Background modeling is a common component in video surveillance systems
	and is used to quickly identify regions of interest. To increase
	the robustness of background subtraction techniques, researchers
	have developed techniques to update the background model and also
	developed probabilistic/statistical approaches for thresholding the
	difference. This paper presents an error analysis of this type of
	background modeling and pixel labeling, providing both theoretical
	analysis and experimental validation. Evaluation is centered around
	the tradeoff of probability of false alarm and probability of miss
	detection, and this paper shows how to efficiently compute these
	probabilities front simpler values that are more easily measured.
	It includes an analysis for both static and dynamic background modeling.
	The paper also examines the assumptions of Gaussian and mixture of
	Gaussian models for a pixel},
  comment = {trk_BG},
  doi = {10.1109/CVPR.2000.855861},
  file = {:papers\\2000 CNF, Error analysis of background adaption (Gao, Boult, Coetzee, Ramesh, CVPR, 70).pdf:PDF},
  keywords = {EM algorithm;Markov chain;Mixture Gaussian;ROC curve;background adaption;background
	modeling;background subtraction;equilibrium;error analysis;pixel
	labeling;surveillance;video surveillance;Markov processes;error analysis;surveillance;video
	signal processing;},
  timestamp = {00,070}
}

@INPROCEEDINGS{2002_CNF_TRKped_Gavrila,
  author = {Gavrila, D.M. and Giebel, J.},
  title = {Shape-based pedestrian detection and tracking},
  booktitle = {Intelligent Vehicle Symposium, 2002. IEEE},
  year = {2002},
  volume = {1},
  pages = { 8 - 14 vol.1},
  month = june,
  abstract = {This paper presents a large scale experimental study on pedestrian
	detection. The focus of the study is the Chamfer System, a generic
	system for shape-based object recognition. Matching involves a simultaneous
	coarse-to-fine approach over a template hierarchy and over the transformation
	parameters based on correlation with (chamfer) distance-transformed
	images. Candidate solutions are verified by a neural network with
	local receptive fields, using a richer set of texture features. Detection
	is supplemented by an alpha-beta tracker which integrates results
	over time; the tracker compensates for momentarily missing detections
	due to image noise or occlusions. For this study, an extensive database
	of 4762 pedestrian images was compiled with precise ground-truth
	data. System performance was analyzed by several ROC curves. Although
	not viable for real-world deployment yet, system performance is shown
	to be quite promising.},
  comment = {TRKped},
  doi = {10.1109/IVS.2002.1187920},
  file = {:papers\\2002 CNF, Shape-based pedestrian detection and tracking (Gavrila).pdf:PDF},
  issn = { },
  keywords = { Chamfer System; alpha-beta tracker; chamfer distance-transformed
	images; correlation; ground-truth data; image noise; large-scale
	experimental study; local receptive fields; neural network; occlusions;
	shape-based object recognition; shape-based pedestrian detection;
	shape-based pedestrian tracking; simultaneous coarse-to-fine approach;
	template hierarchy; texture features; compensation; correlation methods;
	image recognition; image texture; neural nets; object detection;
	optical tracking; road vehicles;},
  timestamp = {00,080}
}

@ARTICLE{1999_JNL_SURVEYmotion_Gavrila,
  author = {Gavrila, D. M.},
  title = {The visual analysis of human movement: a survey},
  journal = {Computer Vision and Image Understanding},
  year = {1999},
  volume = {73},
  pages = {82-98},
  number = {Copyright 1999, IEE},
  abstract = {The ability to recognize humans and their activities by vision is
	key for a machine to interact intelligently and effortlessly with
	a human-inhabited environment. Because of many potentially important
	applications, looking at people is currently one of the most active
	application domains in computer vision. This survey identifies a
	number of promising applications and provides an overview of recent
	developments in this domain. The scope of this survey is limited
	to work on whole-body or hand motion; it does not include work on
	human faces. The emphasis is on discussing the various methodologies;
	they are grouped in 2-D approaches with or without explicit shape
	models and 3-D approaches. Where appropriate, systems are reviewed.
	We conclude with some thoughts about future directions},
  comment = {survey, VQ},
  file = {:papers\\1999 JNL, Visual analysis of human movement_ A survey (SURVEY, Gavrila, CVIU, 1250).pdf:PDF},
  keywords = {computer vision},
  owner = {salman},
  review = {salman: PCA picture quite close to RVQ codebooks},
  timestamp = {01,300}
}

@INPROCEEDINGS{1996_CNF_3DmodelHumanActionTracking_GavrilaDavis,
  author = {Gavrila, D. M. and Davis, L. S.},
  title = {3-D model-based tracking of humans in action: a multi-view approach},
  booktitle = {Computer Vision and Pattern Recognition, 1996. Proceedings CVPR '96,
	1996 IEEE Computer Society Conference on},
  year = {1996},
  comment = {trk_people},
  file = {:papers\\1996 CNF, 3-D  model-based tracking of humans in action_ a multi-view approach  (Gavrila, Davis, CVPR, 600).pdf:PDF},
  keywords = {computational complexity computer vision image sequences visual databases
	3-D body pose 3-D model-based tracking best-first technique chamfer
	matching database decomposition approach fast similarity measure
	graphical human model humans in action multi-view approach pose-recovery
	problem search problem unconstrained human movement vision system},
  owner = {salman},
  timestamp = {00,600}
}

@INPROCEEDINGS{2009_CNF+CrowdCounting_Ge,
  author = {Ge, W. and Collins, R. T.},
  title = {Marked point processes for crowd counting},
  booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference
	on},
  year = {2009},
  pages = {2913-2920},
  abstract = {A Bayesian marked point process (MPP) model is developed to detect
	and count people in crowded scenes. The model couples a spatial stochastic
	process governing number and placement of individuals with a conditional
	mark process for selecting body shape. We automatically learn the
	mark (shape) process from training video by estimating a mixture
	of Bernoulli shape prototypes along with an extrinsic shape distribution
	describing the orientation and scaling of these shapes for any given
	image location. The reversible jump Markov Chain Monte Carlo framework
	is used to efficiently search for the maximum a posteriori configuration
	of shapes, leading to an estimate of the count, location and pose
	of each person in the scene. Quantitative results of crowd counting
	are presented for two publicly available datasets with known ground
	truth.},
  comment = {trk_crowd},
  file = {:papers\\2009 CNF, Marked point processes for crowd counting (Ge, R.T. Collins PSU, CVPR).pdf:PDF},
  keywords = {Bayes methods Markov processes Monte Carlo methods object detection
	video signal processing Bayesian marked point process Bernoulli shape
	prototypes Markov chain Monte Carlo framework crowd counting crowd
	detection shape distribution stochastic process video detection},
  timestamp = {-}
}

@ARTICLE{1998_JNL_MultibodyGrouping_Gear,
  author = {Gear, C. W.},
  title = {Multibody Grouping from Motion Images},
  journal = {International Journal of Computer Vision},
  year = {1998},
  volume = {29},
  pages = {133-150},
  number = {2},
  abstract = {We want to deduce, from a sequence of noisy two-dimensional images
	of a scene of several rigid bodies moving independently in three
	dimensions, the number of bodies and the grouping of given feature
	points in the images to the bodies. Prior processing is assumed to
	have identified features or points common to all frames and the images
	are assumed to be created by orthographic projection (i.e., perspective
	effects are minimal). We describe a computationally inexpensive algorithm
	that can determine which points or features belong to which rigid
	body using the fact that, with exact observations in orthographic
	projection, points on a single body lie in a three or less dimensional
	linear manifold of frame space. If there are enough observations
	and independent motions, these manifolds can be viewed as a set linearly
	independent, four or less dimensional subspaces. We show that the
	row echelon canonical form provides direct information on the grouping
	of points to these subspaces. Treatment of the noise is the most
	difficult part of the problem. This paper uses a statistical approach
	to estimate the grouping of points to subspaces in the presence of
	noise by computing which partition has the maximum likelihood. The
	input data is assumed to be contaminated with independent Gaussian
	noise. The algorithm can base its estimates on a user-supplied standard
	deviation of the noise, or it can estimate the noise from the data.
	The algorithm can also be used to estimate the probability of a user-specified
	partition so that the hypothesis can be combined with others using
	Bayesian statistics.},
  comment = {?},
  file = {:papers\\1998 JNL, Multibody Grouping from Motion Images (Gear, IJCV, 81).pdf:PDF},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{1984_JNL_StochasticRelaxation_Geman,
  author = {Geman, Stuart and Geman, Donald},
  title = {Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration
	of Images},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1984},
  volume = {PAMI-6},
  pages = {721 -741},
  number = {6},
  month = {nov. },
  abstract = {We make an analogy between images and statistical mechanics systems.
	Pixel gray levels and the presence and orientation of edges are viewed
	as states of atoms or molecules in a lattice-like physical system.
	The assignment of an energy function in the physical system determines
	its Gibbs distribution. Because of the Gibbs distribution, Markov
	random field (MRF) equivalence, this assignment also determines an
	MRF image model. The energy function is a more convenient and natural
	mechanism for embodying picture attributes than are the local characteristics
	of the MRF. For a range of degradation mechanisms, including blurring,
	nonlinear deformations, and multiplicative or additive noise, the
	posterior distribution is an MRF with a structure akin to the image
	model. By the analogy, the posterior distribution defines another
	(imaginary) physical system. Gradual temperature reduction in the
	physical system isolates low energy states (``annealing''), or what
	is the same thing, the most probable states under the Gibbs distribution.
	The analogous operation under the posterior distribution yields the
	maximum a posteriori (MAP) estimate of the image given the degraded
	observations. The result is a highly parallel ``relaxation'' algorithm
	for MAP estimation. We establish convergence properties of the algorithm
	and we experiment with some simple pictures, for which good restorations
	are obtained at low signal-to-noise ratios.},
  comment = {seg},
  file = {:papers\\1984 JNL, Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images (Geman, Geman).pdf:PDF},
  issn = {0162-8828},
  timestamp = {10,000}
}

@ARTICLE{2010_JNL_TRK_ped_Andnimo,
  author = {Gero andnimo, D. and Lo andpez, A.M. and Sappa, A.D. and Graf, T.},
  title = {Survey of Pedestrian Detection for Advanced Driver Assistance Systems},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2010},
  volume = {32},
  pages = {1239 -1258},
  number = {7},
  month = july,
  abstract = {Advanced driver assistance systems (ADASs), and particularly pedestrian
	protection systems (PPSs), have become an active research area aimed
	at improving traffic safety. The major challenge of PPSs is the development
	of reliable on-board pedestrian detection systems. Due to the varying
	appearance of pedestrians (e.g., different clothes, changing size,
	aspect ratio, and dynamic shape) and the unstructured environment,
	it is very difficult to cope with the demanded robustness of this
	kind of system. Two problems arising in this research area are the
	lack of public benchmarks and the difficulty in reproducing many
	of the proposed methods, which makes it difficult to compare the
	approaches. As a result, surveying the literature by enumerating
	the proposals one--after-another is not the most useful way to provide
	a comparative point of view. Accordingly, we present a more convenient
	strategy to survey the different approaches. We divide the problem
	of detecting pedestrians from images into different processing steps,
	each with attached responsibilities. Then, the different proposed
	methods are analyzed and classified with respect to each processing
	stage, favoring a comparative viewpoint. Finally, discussion of the
	important topics is presented, putting special emphasis on the future
	needs and challenges.},
  comment = {TRK_ped},
  doi = {10.1109/TPAMI.2009.122},
  file = {:papers\\2010 JNL, Survey of Pedestrian Detection for Advanced Driver Assistance Systems (Geronimo).pdf:PDF},
  issn = {0162-8828},
  keywords = {advanced driver assistance systems;pedestrian detection;pedestrian
	protection systems;traffic safety;driver information systems;Accident
	Prevention;Accidents, Traffic;Automobile Driving;Humans;Image Processing,
	Computer-Assisted;Pattern Recognition, Automated;Walking;},
  timestamp = {-}
}

@ARTICLE{1982_JNL_VQ_Gersho,
  author = {Gersho, A.},
  title = {On the structure of vector quantizers},
  journal = {Information Theory, IEEE Transactions on},
  year = {1982},
  volume = {28},
  pages = { 157 - 166},
  number = {2},
  month = {mar},
  abstract = { Vector quantization is intrinsically superior to predictive coding,
	transform coding, and other suboptimal and {em ad hoc} procedures
	since it achieves optimal rate distortion performance subject only
	to a constraint on memory or block length of the observable signal
	segment being encoded. The key limitation of existing techniques
	is the very large randomly generated code books which must be stored,
	and the computational complexity of the associated encoding procedures.
	The quantization operation is decomposed into its rudimentary structural
	components. This leads to a simple and elegant approach to derive
	analytical properties of optimal quantizers. Some useful properties
	of quantizers and algorithmic approaches are given, which are relevant
	to the complexity of both storage and processing in the encoding
	operation. Highly disordered quantizers, which have been designed
	using a clustering algorithm, are considered. Finally, lattice quantizers
	are examined which circumvent the need for a code book by using a
	highly structured code based on lattices. The code vectors are algorithmically
	generated in a simple manner rather than stored in a code book, and
	fast algorithms perform the encoding algorithm with negligible complexity.},
  comment = {survey, VQ},
  file = {:papers\\1982 JNL, On the structure of vector quantizers (Gersho).pdf:PDF},
  issn = {0018-9448},
  keywords = { Quantization (signal); Signal quantization;},
  timestamp = {00,250}
}

@ARTICLE{1978_JNL_Quantization_Gersho,
  author = {Gersho, A.},
  title = {Principles of quantization},
  journal = {Circuits and Systems, IEEE Transactions on},
  year = {1978},
  volume = {25},
  pages = { 427 - 436},
  number = {7},
  abstract = { Quantization is the process of replacing analog samples with approximate
	values taken from a finite set of allowed values. The approximate
	values corresponding to a sequence of analog samples can then be
	specified by a digital signal for transmission, storage, or other
	digital processing. In this expository paper, the basic ideas of
	uniform quantization, companding, robustness to input power level,
	and optimal quantization are reviewed and explained. The performance
	of various schemes are compared using the ratio of signal power to
	mean-square quantizing noise as a criterion. Entropy coding and the
	ultimate theoretical bound on block quantizer performance are also
	compared with the simpler zero-memory quantizer.},
  comment = {VQ},
  file = {:papers\\1978 JNL, Principles of quantization (Gersho).pdf:PDF},
  issn = {0098-4094 },
  keywords = { General topics and reviews; Quantization (signal); Signal quantization;},
  timestamp = {00,100}
}

@ARTICLE{1983_JNL_VQ_Gersho,
  author = {Gersho, A. and Cuperman, V.},
  title = {Vector quantization: A pattern-matching technique for speech coding},
  journal = {IEEE Communications Magazine},
  year = {1983},
  volume = {21},
  pages = {15--21},
  number = {9},
  comment = {survey, VQ},
  file = {:papers\\1983 JNL, Vector quantization_ A pattern-matching technique for speech coding.pdf:PDF},
  owner = {salman},
  timestamp = {00,100}
}

@BOOK{1991_BOOK_VQ_GershoGray,
  title = {Vector Quantization and Signal Compression (The Springer International
	Series in Engineering and Computer Science)},
  publisher = {Springer},
  year = {1991},
  author = {Gersho, Allen and Gray, Robert},
  comment = {VQ_book},
  timestamp = {-}
}

@INPROCEEDINGS{1982_ImageCodingVQ_Gersho,
  author = {Gersho, A. and Ramamurthi, B.},
  title = {Image coding using vector quantization},
  booktitle = {Acoustics, Speech, and Signal Processing, IEEE International Conference
	on ICASSP '82.},
  year = {1982},
  abstract = { An image is partitioned into cells of pxp pixels. Each cell is regarded
	as a vector of dimension p2and is encoded by searching through a
	codebook for a nearest matching representative vector. A binary word
	identifying the selected representative vector is assigned as the
	codeword to describe the original cell. The decoder uses this codeword
	to address a codebook. Each entry of the codebook contains a full
	precision digital representation of one of the N representative vectors.
	The codebook design is based on a clustering technique for vector
	quantizer design preceded by a classification of training cells into
	edge or shade cells. Results for coding rates from 0.5 to 1.5 bits/pixel
	are discussed. Vector quantization appears to be a powerful and promising
	technique for image coding.},
  comment = {VQ},
  file = {:papers\\1982 CNF, Image coding using vector quantization (Gersho, Ramamurthi).pdf:PDF},
  timestamp = {00,100}
}

@ARTICLE{2004278246540,
  author = {Gevers, Theo},
  title = {Robust segmentation and tracking of colored objects in video},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  year = {2004},
  volume = {14},
  pages = {776 - 781},
  number = {6},
  note = {Color constancy;Deformable models;Multivalued gradients;Noise models;Object
	segmentation;Object tracking;Video;},
  abstract = {Segmenting and tracking of objects in video is of great importance
	for video-based encoding, surveillance, and retrieval. However, the
	inherent difficulty of object segmentation and tracking is to distinguish
	changes in the displacement of objects from disturbing effects such
	as noise and illumination changes. Therefore, in this paper, we formulate
	a color-based deformable model which is robust against noisy data
	and changing illumination. Computational methods are presented to
	measure color constant gradients. Further, a model is given to estimate
	the amount of sensor noise through these color constant gradients.
	The obtained uncertainty is subsequently used as a weighting term
	in the deformation process. Experiments are conducted on image sequences
	recorded from three-dimensional scenes. From the experimental results,
	it is shown that the proposed color constant deformable method successfully
	finds object contours robust against illumination, and noisy, but
	homogeneous regions.},
  comment = {trk_color},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2004 JNL, Robust segmentation and tracking of colored objects in video  (Gevers).pdf:PDF},
  issn = {10518215},
  key = {Image segmentation},
  keywords = {Color image processing;Computational methods;Geometry;Mathematical
	models;Signal encoding;Spurious signal noise;Tracking (position);Video
	signal processing;},
  language = {English},
  timestamp = {00,020},
  url = {http://dx.doi.org/10.1109/TCSVT.2004.828347}
}

@ARTICLE{1994_JNL_SupervisedLearningEM_GhahramaniJordan,
  author = {Ghahramani, Zoubin and Jordan, Michael},
  title = {Supervised learning from incomplete data via an EM approach},
  journal = {Advances in Neural Information Processing Systems 6},
  year = {1994},
  volume = {6},
  pages = {120-127},
  abstract = {Real-world learning tasks may involve high-dimensional data sets with
	arbitrary patterns of missing data. In this paper we present a framework
	based on maximum likelihood density estimation for learning from
	such data sets. We use mixture models for the density estimates and
	make two distinct appeals to the ExpectationMaximization (EM) principle
	(Dempster et al., 1977) in deriving a learning algorithm---EM is
	used both for the estimation of mixture components and for coping
	with missing data. The resulting algorithm is applicable to a wide
	range of supervised as well as unsupervised learning problems. Results
	from a classification benchmark---the iris data set---are presented.
	1 Introduction Adaptive systems generally operate in environments
	that are fraught with imperfections; nonetheless they must cope with
	these imperfections and learn to extract as much relevant information
	as needed for their particular goals. One form of imperfection is
	incompleteness in sensing information. Inc...},
  comment = {PRML},
  file = {:papers\\1994 JNL, Supervised learning from incomplete data via an EM approach (Ghahramani, Jordan, ANI, 292).pdf:PDF},
  owner = {salman},
  timestamp = {00,300}
}

@ARTICLE{1993_JNL_ParticleFilter_Gordon,
  author = {Gordon, N.J. and Salmond, D.J. and Smith, A.F.M.},
  title = {Novel approach to nonlinear/non-Gaussian Bayesian state estimation
	},
  journal = {Radar and Signal Processing, IEE Proceedings F},
  year = {1993},
  volume = {140},
  pages = {107 -113},
  number = {2},
  month = {apr},
  abstract = {An algorithm, the bootstrap filter, is proposed for implementing recursive
	Bayesian filters. The required density of the state vector is represented
	as a set of random samples, which are updated and propagated by the
	algorithm. The method is not restricted by assumptions of linearity
	or Gaussian noise: it may be applied to any state transition or measurement
	model. A simulation example of the bearings only tracking problem
	is presented. This simulation includes schemes for improving the
	efficiency of the basic algorithm. For this example, the performance
	of the bootstrap filter is greatly superior to the standard extended
	Kalman filter},
  comment = {trk},
  file = {:papers\\1993 JNL, Novel approach to nonlinear_non-Gaussian Bayesian state estimation (Gordon, Salmond, Smith, ProcRSP, 2556).pdf:PDF},
  issn = {0956-375X},
  keywords = {Gaussian noise;algorithm;bearings only tracking problem;bootstrap
	filter;extended Kalman filter;measurement model;nonGaussian Bayesian
	state estimation;nonlinear Bayesian state estimation;random samples;recursive
	Bayesian filters;simulation;state transition model;state vector density;Bayes
	methods;Kalman filters;filtering and prediction theory;state estimation;tracking;},
  timestamp = {02,500}
}

@INPROCEEDINGS{2002_JNL_JointTrackingClassification_NGordonSMaskell,
  author = {Gordon, N. J. and Maskell, S. and Kirubarajan, T.},
  title = {Efficient particle filters for joint tracking and classification},
  booktitle = {Signal and Data Processing of Small Targets 2002, 2-4 April 2002},
  year = {2002},
  abstract = {Target tracking is usually performed using data from sensors such
	as radar, whilst the target identification task usually relies on
	information from sensors such as IFF, ESM or imagery. The differing
	nature of the data from these sensors has generally led to these
	two vital tasks being performed separately. However, it is clear
	that an experienced operator can observe behavior characteristics
	of targets and, in combination with knowledge and expectations of
	target type and likely activity, can more knowledgeably identify
	the target and robustly predict its track than any automatic process
	yet defined. Most trackers are designed to follow targets within
	a wide envelope of trajectories and are not designed to derive behavior
	characteristics or include them as part of their output. Thus, there
	is potential scope for both applying target type knowledge to improve
	the reliability of the tracking process, and to derive behavioral
	characteristics which may enhance knowledge about target identity
	and/or activity. We introduce a Bayesian framework for joint tracking
	and identification and give a robust and computationally efficient
	particle filter based algorithm for numerical implementation of the
	resulting recursions. Simulation results illustrating algorithm performance
	are presented},
  comment = {trk_filtering},
  file = {:papers\\2002 JNL, Efficient particle filters for joint tracking and classification (Neil Gordon, Simon Maskell, SPIE, 52).pdf:PDF},
  keywords = {Bayes methods filtering theory military systems pattern classification
	signal classification target tracking tracking filters},
  owner = {salman},
  timestamp = {00,050}
}

@ARTICLE{2007_JNL_SpaceTimeShapes_Gorelick,
  author = {Gorelick, L. and Blank, M. and Shechtman, E. and Irani, M. and Basri,
	R.},
  title = {Actions as Space-Time Shapes},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2007},
  volume = {29},
  pages = {2247-2253},
  number = {12},
  abstract = {Human action in video sequences can be seen as silhouettes of a moving
	torso and protruding limbs undergoing articulated motion. We regard
	human actions as three-dimensional shapes induced by the silhouettes
	in the space-time volume. We adopt a recent approach [14] for analyzing
	2D shapes and generalize it to deal with volumetric space-time action
	shapes. Our method utilizes properties of the solution to the Poisson
	equation to extract space-time features such as local space-time
	saliency, action dynamics, shape structure, and orientation. We show
	that these features are useful for action recognition, detection,
	and clustering. The method is fast, does not require video alignment,
	and is applicable in (but not limited to) many scenarios where the
	background is known. Moreover, we demonstrate the robustness of our
	method to partial occlusions, nonrigid deformations, significant
	changes in scale and viewpoint, high irregularities in the performance
	of an action, and low-quality video.},
  comment = {recog_action_volume},
  file = {:papers\\2007 JNL, Actions as Space-Time Shapes (Gorelick, PAMI, 400).pdf:PDF},
  keywords = {Poisson equation feature extraction image sequences articulated motion
	human action clustering human action detection human action recognition
	nonrigid deformation partial occlusion space-time feature extraction
	video sequences volumetric space-time three-dimensional shape Action
	representation action recognition shape analysis space-time analysis},
  owner = {salman},
  review = {citations are from the conference and journal paper combined, which
	is why they're so high only a few years after the paper. nevertheless,
	a good paper.},
  timestamp = {00,400}
}

@ARTICLE{2006_JNL_Poisson_Gorelick,
  author = {Gorelick, L. and Galun, M. and Sharon, E. and Basri, R. and Brandt,
	A.},
  title = {Shape Representation and Classification Using the Poisson Equation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2006},
  volume = {28},
  pages = {1991-2005},
  number = {12},
  abstract = {We present a novel approach that allows us to reliably compute many
	useful properties of a silhouette. Our approach assigns, for every
	internal point of the silhouette, a value reflecting the mean time
	required for a random walk beginning at the point to hit the boundaries.
	This function can be computed by solving Poisson's equation, with
	the silhouette contours providing boundary conditions. We show how
	this function can be used to reliably extract various shape properties
	including part structure and rough skeleton, local orientation and
	aspect ratio of different parts, and convex and concave sections
	of the boundaries. In addition to this, we discuss properties of
	the solution and show how to efficiently compute this solution using
	multigrid algorithms. We demonstrate the utility of the extracted
	properties by using them for shape classification and retrieval},
  comment = {recog_action_shape},
  file = {:papers\\2006 JNL, Shape representation and classification using the Poisson equation (Gorelick, PAMI, 73).pdf:PDF},
  keywords = {Poisson equation computational geometry image classification image
	representation boundary conditions computer vision multigrid algorithms
	random walk shape classification shape representation silhouette
	classification silhouette contours shape silhouette classification.},
  owner = {salman},
  timestamp = {00,070}
}

@MISC{2009_WEB_EarliestHumanPaintings_Gray,
  author = {Richard Gray},
  title = {Prehistoric cave paintings took up to 20,000 years to complete},
  howpublished = {\url{http://www.telegraph.co.uk/earth/3352850/Prehistoric-cave-paintings-took-up-to-20000-years-to-complete.html}},
  year = {2008},
  comment = {misc},
  file = {:papers\\2008 WEB, Prehistoric cave paintings took up to 20,000 years to complete (Gray, Telegraph).pdf:PDF},
  timestamp = {-}
}

@ARTICLE{1984_JNL_VQ_Gray,
  author = {Gray, R.},
  title = {Vector quantization},
  journal = {ASSP Magazine, IEEE},
  year = {1984},
  volume = {1},
  pages = { 4 - 29},
  number = {2},
  month = {apr},
  abstract = { A vector quantizer is a system for mapping a sequence of continuous
	or discrete vectors into a digital sequence suitable for communication
	over or storage in a digital channel. The goal of such a system is
	data compression: to reduce the bit rate so as to minimize communication
	channel capacity or digital storage memory requirements while maintaining
	the necessary fidelity of the data. The mapping for each vector may
	or may not have memory in the sense of depending on past actions
	of the coder, just as in well established scalar techniques such
	as PCM, which has no memory, and predictive quantization, which does.
	Even though information theory implies that one can always obtain
	better performance by coding vectors instead of scalars, scalar quantizers
	have remained by far the most common data compression system because
	of their simplicity and good performance when the communication rate
	is sufficiently large. In addition, relatively few design techniques
	have existed for vector quantizers. During the past few years several
	design algorithms have been developed for a variety of vector quantizers
	and the performance of these codes has been studied for speech waveforms,
	speech linear predictive parameter vectors, images, and several simulated
	random processes. It is the purpose of this article to survey some
	of these design techniques and their applications.},
  comment = {survey, VQ},
  file = {:papers\\1984 JNL, Vector Quantization (Gray).pdf:PDF},
  issn = {0740-7467},
  timestamp = {02,000}
}

@INPROCEEDINGS{1994_CNF_SteerablePyramidFiltersRotationInvariance_Greenspan,
  author = {Greenspan, H. and Belongie, S. and Goodman, R. and Perona, P. and
	Rakshit, S. and Anderson, C.H.},
  title = {Overcomplete steerable pyramid filters and rotation invariance},
  booktitle = {Computer Vision and Pattern Recognition, 1994. Proceedings CVPR '94.,
	1994 IEEE Computer Society Conference on},
  year = {1994},
  pages = {222 -228},
  month = {jun},
  abstract = {A given (overcomplete) discrete oriented pyramid may be converted
	into a steerable pyramid by interpolation. We present a technique
	for deriving the optimal interpolation functions (otherwise called
	`steering coefficients'). The proposed scheme is demonstrated on
	a computationally efficient oriented pyramid, which is a variation
	on the Burt and Adelson (1983) pyramid. We apply the generated steerable
	pyramid to orientation-invariant texture analysis in order to demonstrate
	its excellent rotational isotropy. High classification rates and
	precise rotation identification are demonstrated},
  comment = {SP},
  doi = {10.1109/CVPR.1994.323833},
  file = {:papers\\1994 CNF, Overcomplete steerable pyramid filters and rotation invariance (Greenspan, Belongie, Goodman, Perona, Rakshit, Anderson, CVPR, 112).pdf:PDF},
  keywords = { classification rates; computational efficiency; discrete oriented
	pyramid; optimal interpolation functions; orientation-invariant texture
	analysis; overcomplete steerable pyramid filters; precise rotation
	identification; rotation invariance; rotational isotropy; steering
	coefficients; computer vision; filtering and prediction theory; image
	texture;},
  timestamp = {00,100}
}

@ARTICLE{1995_JNL_HashTableClassifier_Grewe,
  author = {Grewe, Lynne and Kak, Avinash C.},
  title = {Interactive Learning of a Multiple-Attribute Hash Table Classifier
	for Fast Object Recognition},
  journal = {Computer Vision and Image Understanding},
  year = {1995},
  volume = {61},
  pages = {387-416},
  number = {3},
  abstract = {Multiple-attribute hashing is now considered to be a powerful approach
	for the recognition and localization of 3D objects on the basis of
	their invariant properties. In the systems developed to date, the
	structure of the hash table is fixed and must be created by the system
	developer--an onerous task especially when the number of attributes
	is large, as it must in systems that use both geometric and nongeometric
	attributes. Another deficiency of previous systems is that uncertainty
	is treated as a fixed value and not modeled. In this paper, we will
	present a system, named MULTI-HASH, which uses the tools of decision
	trees and uncertainty modeling for the automatic construction of
	hash tables. The decision-tree framework in MULTI-HASH is based on
	a hybrid method that uses both qualitative attributes, such as the
	shape of a surface, and quantitative attributes such as color, dihedral
	angles, etc. The human trainer shows objects to the vision system
	and, in an interactive mode, tells the system the model identities
	of the various segmented regions, etc. Subsequently, the decision-tree-based
	framework learns the structure of the hash table.},
  comment = {PRML},
  file = {:papers\\1995 JNL, Interactive Learning of a Multiple-Attribute Hash Table Classifier for Fast Object Recognition (Grewe, Kak, CVIU, 28).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{1998_CNF_Tracking_Grimson,
  author = {Grimson, W.E.L. and Stauffer, C. and Romano, R. and Lee, L.},
  title = {Using adaptive tracking to classify and monitor activities in a site},
  booktitle = {Computer Vision and Pattern Recognition, 1998. Proceedings. 1998
	IEEE Computer Society Conference on},
  year = {1998},
  pages = {22 -29},
  month = {jun},
  abstract = {We describe a vision system that monitors activity in a site over
	extended periods of time. The system uses a distributed set of sensors
	to cover the site, and an adaptive tracker detects multiple moving
	objects in the sensors. Our hypothesis is that motion tracking is
	sufficient to support a range of computations about site activities.
	We demonstrate using the tracked motion data to calibrate the distributed
	sensors, to construct rough site models, to classify detected objects,
	to learn common patterns of activity for different object classes,
	and to detect unusual activities},
  comment = {trk},
  doi = {10.1109/CVPR.1998.698583},
  file = {:papers\\1998 CNF, Using adaptive tracking to classify and monitor activities in a site (Grimson, Stauffer, Romano, Lee).pdf:PDF},
  issn = {1063-6919},
  keywords = {adaptive tracker;adaptive tracking;distributed sensors;motion tracking;multiple
	moving objects;rough site models;site activities;tracked motion data;vision
	system;adaptive estimation;computer vision;motion estimation;security;surveillance;},
  timestamp = {00,450}
}

@ARTICLE{2000_JNL_BrainPartsForMotion_Grossman,
  author = {Grossman, E. and Donnelly, M. and Price, R. and Pickens, D. and Morgan,
	V. and Neighbor, G. and Blake, R.},
  title = {Brain Areas Involved in Perception of Biological Motion},
  journal = {Journal of Cognitive Neuroscience},
  year = {2000},
  volume = {12},
  pages = {711-720},
  number = {5},
  comment = {motion},
  doi = {10.1162/089892900562417},
  eprint = {http://www.mitpressjournals.org/doi/pdf/10.1162/089892900562417},
  file = {:papers\\2000 JNL, Brain areas involved in perception of biological motion (Grossman, Donnelly, Price, Pickens, Morgan, Neighbor, Blake).pdf:PDF},
  timestamp = {00,350},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/089892900562417}
}

@ARTICLE{2003_JNL_HighRateVQDetection_Hero,
  author = {Gupta, Riten and Hero, Alfred and Alfred and Hero, O.},
  title = {High Rate Vector Quantization for Detection},
  journal = {IEEE Transactions on Information Theory},
  year = {2003},
  volume = {49},
  pages = {1951-1969},
  abstract = {We investigate high rate quantization for various detection and reconstruction
	loss critera. A new distortion measure...},
  comment = {VQ_RVQ},
  file = {:papers\\2003 JNL, High-rate vector quantization for detection (Gupta, Hero, TIT, 15).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{1996_TRK_region_Hager,
  author = {Hager, G.D. and Belhumeur, P.N.},
  title = {Real-time tracking of image regions with changes in geometry and
	illumination},
  booktitle = {Computer Vision and Pattern Recognition, 1996. Proceedings CVPR '96,
	1996 IEEE Computer Society Conference on},
  year = {1996},
  pages = {403 -410},
  month = jun,
  abstract = {Historically, SSD or correlation-based visual tracking algorithms
	have been sensitive to changes in illumination and shading across
	the target region. This paper describes methods for implementing
	SSD tracking that is both insensitive to illumination variations
	and computationally efficient. We first describe a vector-space formulation
	of the tracking problem, showing how to recover geometric deformations.
	We then show that the same vector space formulation can be used to
	account for changes in illumination. We combine geometry and illumination
	into an algorithm that tracks large image regions on live video sequences
	using no more computation than would be required to trade with no
	accommodation for illumination changes. We present experimental results
	which compare the performance of SSD tracking with and without illumination
	compensation},
  comment = {TRK_region},
  file = {:papers\\1996 CNF, Real-time tracking of image regions with changes in geometry and illumination (Hager).pdf:PDF},
  keywords = {SSD tracking;geometric deformations;geometry;illumination;image regions;live
	video sequences;visual tracking;image segmentation;image sequences;real-time
	systems;},
  timestamp = {00,200}
}

@ARTICLE{1998_JNL_ModelsGeometryIllumination_Hager,
  author = {Hager, G. D. and Belhumeur, P. N.},
  title = {Efficient region tracking with parametric models of geometry and
	illumination},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1998},
  volume = {20},
  pages = {1025-1039},
  number = {10},
  abstract = {As an object moves through the field of view of a camera, the images
	of the object may change dramatically. This is not simply due to
	the translation of the object across the image plane; complications
	arise due to the fact that the object undergoes changes in pose relative
	to the viewing camera, in illumination relative to light sources,
	and may even become partially or fully occluded. We develop an efficient
	general framework for object tracking, which addresses each of these
	complications. We first develop a computationally efficient method
	for handling the geometric distortions produced by changes in pose.
	We then combine geometry and illumination into an algorithm that
	tracks large image regions using no more computation than would be
	required to track with no accommodation for illumination changes.
	Finally, we augment these methods with techniques from robust statistics
	and treat occluded regions on the object as statistical outliers.
	Experimental results are given to demonstrate the effectiveness of
	our methods},
  comment = {trk_region},
  file = {:papers\\1998 JNL, Efficient region tracking with parametric models of geometry and illumination (Hager, PAMI, 734).pdf:PDF},
  keywords = {computational geometry computer vision image sequences lighting motion
	estimation optical tracking statistical analysis geometric distortions
	illumination image regions light sources machine vision object tracking
	parametric models region tracking statistical outliers},
  owner = {salman},
  timestamp = {00,750}
}

@ARTICLE{1997_JNL_IntroFusion_Hall,
  author = {Hall, D. L. and Llinas, J.},
  title = {An introduction to multisensor data fusion},
  journal = {Proceedings of the IEEE},
  year = {1997},
  volume = {85},
  pages = {6-23},
  number = {1},
  abstract = {Multisensor data fusion is an emerging technology applied to Department
	of Defense (DoD) areas such as automated target recognition, battlefield
	surveillance, and guidance and control of autonomous vehicles, and
	to non-DoD applications such as monitoring of complex machinery,
	medical diagnosis, and smart buildings. Techniques for multisensor
	data fusion are drawn from a wide range of areas including artificial
	intelligence, pattern recognition, statistical estimation and other
	areas. This paper provides a tutorial on data fusion, introducing
	data fusion applications, process models, and identification of applicable
	techniques. Comments are made on the state-of-the-art in data fusion},
  comment = {survey},
  file = {:papers\\1997 JNL,  An introduction  to multisensor data fusion (TUTORIAL, Hall, LLinas, ProcIEEE).pdf:PDF},
  keywords = {aerospace computing artificial intelligence computerised instrumentation
	knowledge based systems military computing military systems pattern
	recognition sensor fusion Department of Defense DoD automated target
	recognition autonomous vehicles battlefield surveillance complex
	machinery control emerging technology guidance identification knowledge
	based methods medical diagnosis multisensor data fusion nonlinearities
	process models smart buildings statistical estimation},
  owner = {salman},
  timestamp = {00,800}
}

@INPROCEEDINGS{1994_TRK_subspace_Hallinan,
  author = {Hallinan, P.W.},
  title = {A low-dimensional representation of human faces for arbitrary lighting
	conditions},
  booktitle = {Computer Vision and Pattern Recognition, 1994. Proceedings CVPR '94.,
	1994 IEEE Computer Society Conference on},
  year = {1994},
  pages = {995 -999},
  month = jun,
  abstract = {When recognizing a fixed object from a fixed viewpoint, the dominant
	source of variation in image intensity is lighting changes. We propose
	a low-dimensional model for human faces that can both synthesize
	a face image when given lighting conditions and can estimate lighting
	conditions when given a face image. The model can handle non-Lambertian
	and self-shadowing surfaces such as faces because it does not make
	any assumptions about either the surface geometry or bidirectional
	reflectance function. The model can be adapted to handle any arbitrary
	lighting condition, and is easily extendable to any other viewpoint
	or to any other object},
  comment = {TRK_subspace},
  doi = {10.1109/CVPR.1994.323941},
  file = {:papers\\1994 CNF, A low-dimensional representation of human faces for arbitrary lighting conditions (Hallinan).pdf:PDF},
  keywords = { face image; fixed object; fixed viewpoint; human faces; image intensity;
	lighting changes; lighting conditions; self-shadowing surfaces; computer
	vision; face recognition; lighting;},
  timestamp = {00,300}
}

@ARTICLE{1973_JNL_TextureClassification_Haralick,
  author = {Haralick, Robert M. and Shanmugam, K. and Dinstein, Its'Hak},
  title = {Textural Features for Image Classification},
  journal = {Systems, Man and Cybernetics, IEEE Transactions on},
  year = {1973},
  volume = {3},
  pages = {610 -621},
  number = {6},
  month = {nov.},
  abstract = {Texture is one of the important characteristics used in identifying
	objects or regions of interest in an image, whether the image be
	a photomicrograph, an aerial photograph, or a satellite image. This
	paper describes some easily computable textural features based on
	gray-tone spatial dependancies, and illustrates their application
	in category-identification tasks of three different kinds of image
	data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic
	aerial photographs of eight land-use categories, and Earth Resources
	Technology Satellite (ERTS) multispecial imagery containing seven
	land-use categories. We use two kinds of decision rules: one for
	which the decision regions are convex polyhedra (a piecewise linear
	decision rule), and one for which the decision regions are rectangular
	parallelpipeds (a min-max decision rule). In each experiment the
	data set was divided into two parts, a training set and a test set.
	Test set identification accuracy is 89 percent for the photomicrographs,
	82 percent for the aerial photographic imagery, and 83 percent for
	the satellite imagery. These results indicate that the easily computable
	textural features probably have a general applicability for a wide
	variety of image-classification applications.},
  comment = {?},
  doi = {10.1109/TSMC.1973.4309314},
  file = {:papers\\1973 JNL, Textural features for image classification (Haralick, Shanmugam, Dinstein, TSMC, 4739).pdf:PDF},
  issn = {0018-9472},
  timestamp = {05,000}
}

@ARTICLE{2005_JNL_TRKblk_Hari,
  author = {Hariharakrishnan, K. and Schonfeld, D.},
  title = {Fast object tracking using adaptive block matching},
  journal = {Multimedia, IEEE Transactions on},
  year = {2005},
  volume = {7},
  pages = {853--859},
  number = {5},
  comment = {TRKblk},
  file = {:C\:\\salman\\work\\writing\\papers\\2005 JNL, Fast Object Tracking Using Adaptive Block Matching (Hari).pdf:PDF},
  issn = {1520-9210},
  publisher = {IEEE},
  timestamp = {00,050}
}

@ARTICLE{2000_JNL_W4_Haritaoglu,
  author = {Haritaoglu, I. and Harwood, D. and Davis, L. S.},
  title = {W4: real-time surveillance of people and their activities},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {809-830},
  number = {8},
  abstract = {W<sup>4</sup> is a real time visual surveillance system for detecting
	and tracking multiple people and monitoring their activities in an
	outdoor environment. It operates on monocular gray-scale video imagery,
	or on video imagery from an infrared camera. W<sup>4</sup> employs
	a combination of shape analysis and tracking to locate people and
	their parts (head, hands, feet, torso) and to create models of people's
	appearance so that they can be tracked through interactions such
	as occlusions. It can determine whether a foreground region contains
	multiple people and can segment the region into its constituent people
	and track them. W<sup>4</sup> can also determine whether people are
	carrying objects, and can segment objects from their silhouettes,
	and construct appearance models for them so they can be identified
	in subsequent frames. W<sup>4</sup> can recognize events between
	people and objects, such as depositing an object, exchanging bags,
	or removing an object. It runs at 25 Hz for 320&times;240 resolution
	images on a 400 MHz dual-Pentium II PC},
  comment = {trk_people},
  file = {:papers\\2000 JNL, W4_ real-time surveillance of people and their activities (Haritaoglu, Harwood, Davis, PAMI, 1546).pdf:PDF},
  keywords = {computer vision computerised monitoring image segmentation object
	recognition real-time systems surveillance target tracking people
	activity monitoring shape analysis tracking visual surveillance system},
  owner = {salman},
  timestamp = {01,500}
}

@INPROCEEDINGS{1988_CNF_CombinedCornerEdgeDetector_Harris,
  author = {Harris, Chris and Stephens, Mike},
  title = {A Combined Corner and Edge Detector},
  booktitle = {4th Alvey Vision Conference},
  year = {1988},
  comment = {feature},
  file = {:papers\\1988 CNF, A Combined Corner and Edge Detector (Harris, Stephens, 4265).pdf:PDF},
  owner = {salman},
  timestamp = {04,000}
}

@BOOK{2004_BOOK_CG_Hartley,
  title = {Multiple View Geometry in Computer Vision},
  publisher = {Cambridge University Press, ISBN: 0521540518},
  year = {2004},
  author = {Hartley, R.~I. and Zisserman, A.},
  edition = {Second},
  comment = {CG_book},
  file = {:papers\\2004 BOOK, Multiple View Geometry In Computer Vision (Hartley, Zisserman, 2nd ed).pdf:PDF},
  timestamp = {-}
}

@BOOK{2009_BOOK_PRML_Hastie,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and
	Prediction, Second Edition (Springer Series in Statistics)},
  publisher = {Springer},
  year = {2009},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  edition = {2nd ed. 2009. Corr. 3rd printing5th Printing.},
  month = {February},
  abstract = {{During the past decade there has been an explosion in computation
	and information technology. With it have come vast amounts of data
	in a variety of fields such as medicine, biology, finance, and marketing.
	The challenge of understanding these data has led to the development
	of new tools in the field of statistics, and spawned new areas such
	as data mining, machine learning, and bioinformatics. Many of these
	tools have common underpinnings but are often expressed with different
	terminology. This book describes the important ideas in these areas
	in a common conceptual framework. While the approach is statistical,
	the emphasis is on concepts rather than mathematics. Many examples
	are given, with a liberal use of color graphics. It should be a valuable
	resource for statisticians and anyone interested in data mining in
	science or industry. The book's coverage is broad, from supervised
	learning (prediction) to unsupervised learning. The many topics include
	neural networks, support vector machines, classification trees and
	boosting---the first comprehensive treatment of this topic in any
	book.
	
	This major new edition features many topics not covered in the original,
	including graphical models, random forests, ensemble methods, least
	angle regression \& path algorithms for the lasso, non-negative matrix
	factorization, and spectral clustering. There is also a chapter on
	methods for ``wide'' data (p bigger than n), including multiple testing
	and false discovery rates.
	
	Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors
	of statistics at Stanford University. They are prominent researchers
	in this area: Hastie and Tibshirani developed generalized additive
	models and wrote a popular book of that title. Hastie co-developed
	much of the statistical modeling software and environment in R/S-PLUS
	and invented principal curves and surfaces. Tibshirani proposed the
	lasso and is co-author of the very successful An Introduction to
	the Bootstrap. Friedman is the co-inventor of many data-mining tools
	including CART, MARS, projection pursuit and gradient boosting.}},
  comment = {PRML},
  day = {09},
  file = {:papers\\2009 BOOK, The Elements of Statistical Learning (Springer).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0387848576},
  posted-at = {2011-02-08 02:05:06},
  priority = {2},
  timestamp = {-}
}

@BOOK{2005_BOOK_AdaptiveFilters_Haykin,
  title = {Adaptive Filter Theory (3rd Edition)},
  publisher = {Prentice Hall},
  year = {1995},
  author = {Haykin, Simon},
  edition = {3rd},
  month = {December},
  citeulike-article-id = {7027890},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/013322760X},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/013322760X},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/013322760X},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/013322760X},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/013322760X/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/013322760X},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/013322760X},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN013322760X},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=013322760X\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/013322760X},
  comment = {PRML_book},
  day = {27},
  file = {:papers\\1995 BOOK, Adaptive Filter Theory (Haykin, Prentice Hall 3rd ed).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {013322760X},
  posted-at = {2010-04-17 01:40:09},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/013322760X}
}

@TECHREPORT{1995_REP_BayesNet_Heckerman,
  author = {David Heckerman},
  title = {A Tutorial on Learning With Bayesian Networks},
  institution = {Microsoft Research},
  year = {1995},
  comment = {Tutorial},
  file = {:C\:\\salman\\work\\writing\\papers\\1995 REP, A Tutorial on Learning With Bayesian Networks (Heckerman).pdf:PDF},
  timestamp = {-}
}

@BOOK{2004_BOOK_PRmatlab_Heijden,
  title = {Classification, Parameter Estimation and State Estimation: An Engineering
	Approach Using MATLAB},
  publisher = {Wiley},
  year = {2004},
  author = {van der Heijden, Ferdinand and Duin, Robert and de Ridder, Dick and
	Tax, David M. J.},
  edition = {1},
  month = {November},
  abstract = {{<i>Classification, Parameter Estimation and State Estimation</i>
	is a practical guide for data analysts and designers of measurement
	systems and postgraduates students that are interested in advanced
	measurement systems using MatLab. 'Prtools' is a powerful MatLab
	toolbox for pattern recognition and is written and owned by one of
	the co-authors, B. Duin of the Delft University of Technology. <p>
	After an introductory chapter, the book provides the theoretical
	construction for classification, estimation and state estimation.
	The book also deals with the skills required to bring the theoretical
	concepts to practical systems, and how to evaluate these systems.
	Together with the many examples in the chapters, the book is accompanied
	by a MatLab toolbox for pattern recognition and classification. The
	appendix provides the necessary documentation for this toolbox as
	well as an overview of the most useful functions from these toolboxes.
	With its integrated and unified approach to classification, parameter
	estimation and state estimation, this book is a suitable practical
	supplement in existing university courses in pattern classification,
	optimal estimation and data analysis. <ul> <li>Covers all contemporary
	main methods for classification and estimation. <li>Integrated approach
	to classification, parameter estimation and state estimation <li>Highlights
	the practical deployment of theoretical issues. <li>Provides a concise
	and practical approach supported by Matlab toolbox. <li>Offers exercises
	at the end of each chapter and numerous worked out examples. <li>PRtools
	toolbox (MatLab) and code of worked out examples available from the
	internet <li>Many examples showing implementations in MatLab <li>Enables
	students to practice their skills using a MatLab environment </ul>
	}},
  citeulike-article-id = {162667},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0470090138},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0470090138},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0470090138},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0470090138},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0470090138/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0470090138},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0470090138},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0470090138},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0470090138\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0470090138},
  day = {19},
  howpublished = {Hardcover},
  isbn = {0470090138},
  posted-at = {2010-09-01 14:53:15},
  priority = {2},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0470090138}
}

@ARTICLE{6263747,
  author = {Herodotou, N. and Plataniotis, K.N. and Venetsanopoulos, A.N.},
  title = {Automatic location and tracking of the facial region in color video
	sequences},
  journal = {Signal Processing: Image Communication},
  year = {1999},
  volume = { 14},
  pages = {359 - 88},
  number = { 5},
  note = {automatic location;automatic tracking;facial region;color video sequences;videophone-type
	sequences;color processing unit;knowledge-based shape analysis;knowledge-based
	color analysis;skin-tones;HSV color space;segmentation scheme;fuzzy
	membership functions;aggregation operator;computational complexity;QCIF
	video sequences;CIF video sequences;},
  abstract = {A novel technique is introduced to locate and track the facial area
	in videophone-type sequences. The proposed method essentially consists
	of two components: (i) a color processing unit, and (ii) a knowledge-based
	shape and color analysis module. The color processing component utilizes
	the distribution of skin-tones in the HSV color space to obtain an
	initial set of candidate regions or objects. The second component
	in the segmentation scheme, that is, the shape and color analysis
	module is used to correctly identify and select the facial region
	in the case where more than one object has been extracted. A number
	of fuzzy membership functions are devised to provide information
	about each object's shape, orientation, location and average hue.
	An aggregation operator finally combines these measures and correctly
	selects the facial area. The suggested approach is robust with regard
	to different skin types, and various types of object or background
	motion within the scene. Furthermore, the algorithm can be implemented
	at a low computational complexity due to the binary nature of the
	operations involved. Experimental results are presented for a series
	of CIF and QCIF video sequences},
  address = {Netherlands},
  comment = {trk_color},
  copyright = {Copyright 1999, IEE},
  file = {:papers\\1999 JNL, Automatic location and tracking of the facial region in color video sequences (Herodotou, Plataiotis, Venetsanopoulos).pdf:PDF},
  issn = {0923-5965},
  keywords = {computational complexity;fuzzy logic;image colour analysis;image segmentation;image
	sequences;knowledge based systems;tracking;videotelephony;},
  language = {English},
  timestamp = {00,050},
  url = {http://dx.doi.org/10.1016/S0923-5965(98)00018-6}
}

@ARTICLE{2008_JNL_VideoRecognition_Hervieu,
  author = {Hervieu, A. and Bouthemy, P. and Le Cadre, J. P.},
  title = {A Statistical Video Content Recognition Method Using Invariant Features
	on Object Trajectories},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  year = {2008},
  volume = {18},
  pages = {1533-1543},
  number = {11},
  abstract = {This work is dedicated to a statistical trajectory-based approach
	addressing two issues related to dynamic video content understanding:
	recognition of events and detection of unexpected events. Appropriate
	local differential features combining curvature and motion magnitude
	are defined and robustly computed on the motion trajectories in the
	image sequence. These features are invariant to image translation,
	in-the-plane rotation and spatial scaling. The temporal causality
	of the features is then captured by hidden Markov models dedicated
	to trajectory description, whose states are properly quantized values.
	The similarity between trajectories is expressed by exploiting this
	quantization-based HMM framework. Moreover statistical techniques
	have been developed for parameter estimations. Evaluations of the
	method have been conducted on several data sets including real trajectories
	obtained from sport videos, especially Formula One and ski TV program.
	The novel method compares favorably with other methods including
	feature histogram comparisons, HMM/GMM modeling and SVM classification.},
  comment = {analysis},
  file = {:papers\\2008 JNL, A Statistical Video Content Recognition Method Using Invariant Features on Object Trajectories (Hervieu, TCSVT, 6).pdf:PDF},
  keywords = {hidden Markov models image motion analysis image recognition image
	sequences object detection parameter estimation statistical analysis
	support vector machines video signal processing SVM classification
	dynamic video content understanding event detection event recognition
	feature histogram comparisons image sequence image translation invariant
	features motion magnitude motion trajectories object trajectories
	parameter estimations statistical techniques statistical trajectory-based
	approach statistical video content recognition method},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{2007_CNF_HMMdynamicVideo_Hervieu,
  author = {Hervieu, A. and Bouthemy, P. and Le Cadre, J. P.},
  title = {A HMM-Based Method for Recognizing Dynamic Video Contents from Trajectories},
  booktitle = {Image Processing, 2007. ICIP 2007. IEEE International Conference
	on},
  year = {2007},
  volume = {4},
  comment = {recog_action},
  file = {:papers\\2007 CNF,  A HMM-Based Method for Recognizing Dynamic Video Contents from Trajectories (Hervieu, Bouthemy, Cadre, ICIP).pdf:PDF},
  keywords = {feature extraction hidden Markov models image motion analysis image
	recognition image sequences object recognition support vector machines
	target tracking video signal processing HMM-based method SVM classification
	dynamic video content recognition longest common subsequence distance
	classification object motion trajectory classification pattern classification
	synthetic trajectories video sequences},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{2004_CNF_L2tracking_Ho,
  author = {Ho, J. and Kuang-Chih Lee and Ming-Hsuan Yang and Kriegman, D.},
  title = {Visual tracking using learned linear subspaces},
  booktitle = {Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings
	of the 2004 IEEE Computer Society Conference on},
  year = {2004},
  volume = {1},
  pages = { I-782 - I-789 Vol.1},
  abstract = {This paper presents a simple but robust visual tracking algorithm
	based on representing the appearances of objects using affine warps
	of learned linear subspaces of the image space. The tracker adaptively
	updates this subspace while tracking by finding a linear subspace
	that best approximates the observations made in the previous frames.
	Instead of the traditional L2-reconstruction error norm which leads
	to subspace estimation using PCA or SVD, we argue that a variant
	of it, the uniform L2-reconstruction error norm, is the right one
	for tracking. Under this framework we provide a simple and a computationally
	inexpensive algorithm for finding a subspace whose uniform L2-reconstruction
	error norm for a given collection of data samples is below some threshold,
	and a simple tracking algorithm is an immediate consequence. We show
	experimental results on a variety of image sequences of people and
	man-made objects moving under challenging imaging conditions, which
	include drastic illumination variation, partial occlusion and extreme
	pose variation.},
  comment = {TRK_subspace},
  file = {:papers\\2004 CNF, Visual tracking using learned linear subspaces (Ho, Kriegman).pdf:PDF},
  issn = {1063-6919 },
  keywords = { PCA; SVD; affine warps; computationally inexpensive algorithm; illumination
	variation; image sequences; image space; imaging conditions; learned
	linear subspaces; partial occlusion; pose variation; robust visual
	tracking algorithm; subspace estimation;L2-reconstruction error norm;
	image motion analysis; image reconstruction; image representation;
	image sequences; learning (artificial intelligence); principal component
	analysis; singular value decomposition; tracking;},
  timestamp = {00,070}
}

@ARTICLE{1998_JNL_RandomSubspace_Ho,
  author = {Tin Kam Ho},
  title = {The random subspace method for constructing decision forests},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1998},
  volume = {20},
  pages = {832 -844},
  number = {8},
  month = aug,
  abstract = {Much of previous attention on decision trees focuses on the splitting
	criteria and optimization of tree sizes. The dilemma between overfitting
	and achieving maximum accuracy is seldom resolved. A method to construct
	a decision tree based classifier is proposed that maintains highest
	accuracy on training data and improves on generalization accuracy
	as it grows in complexity. The classifier consists of multiple trees
	constructed systematically by pseudorandomly selecting subsets of
	components of the feature vector, that is, trees constructed in randomly
	chosen subspaces. The subspace method is compared to single-tree
	classifiers and other forest construction methods by experiments
	on publicly available datasets, where the method's superiority is
	demonstrated. We also discuss independence between trees in a forest
	and relate that to the combined classification accuracy},
  comment = {TRK_subspace},
  doi = {10.1109/34.709601},
  file = {:papers\\1998 JNL, The Random Subspace Method for Constructing Decision Forests (Ho).pdf:PDF},
  issn = {0162-8828},
  keywords = {classification accuracy;decision forests;decision tree based classifier;decision
	trees;feature vector;generalization accuracy;maximum accuracy;overfitting;random
	subspace method;decision theory;learning (artificial intelligence);pattern
	classification;random processes;trees (mathematics);},
  timestamp = {01,000}
}

@ARTICLE{1964_JNL_BayesianEstimation_Ho,
  author = { Ho, Y. and Lee, R.},
  title = {A Bayesian approach to problems in stochastic estimation and control},
  journal = {Automatic Control, IEEE Transactions on},
  year = {1964},
  volume = {9},
  pages = { 333 - 339},
  number = {4},
  month = {oct},
  abstract = { In this paper, a general class of stochastic estimation and control
	problems is formulated from the Bayesian Decision-Theoretic viewpoint.
	A discussion as to how these problems can be solved step by step
	in principle and practice from this approach is presented. As a specific
	example, the closed form Wiener-Kalman solution for linear estimation
	in Gaussian noise is derived. The purpose of the paper is to show
	that the Bayesian approach provides; 1) a general unifying framework
	within which to pursue further researches in stochastic estimation
	and control problems, and 2) the necessary computations and difficulties
	that must be overcome for these problems. An example of a nonlinear,
	non-Gaussian estimation problem is also solved.},
  comment = {?},
  file = {:papers\\1964 JNL, A Bayesian approach to problems in stochastic estimation and control (Ho, Lee, TAC, 224).pdf:PDF},
  issn = {0018-9286},
  keywords = { Bayes procedures; Stochastic control; Stochastic estimation;},
  timestamp = {00,300}
}

@ARTICLE{1998_JNL_SpectralDiscrimination_Holden,
  author = {H. Holden and E. LeDrew},
  title = {Spectral Discrimination of Healthy and Non-Healthy Corals Based on
	Cluster Analysis, Principal Components Analysis, and Derivative Spectroscopy},
  journal = {Remote Sensing of Environment},
  year = {1998},
  volume = {65},
  pages = {217 - 224},
  number = {2},
  comment = {classification},
  doi = {DOI: 10.1016/S0034-4257(98)00029-7},
  file = {:papers\\1998 JNL, Spectral Discrimination of Healthy and Non-Healthy Corals Based on Cluster Analysis, Principal Components Analysis, and Derivative Spectroscopy (Holden).pdf:PDF},
  issn = {0034-4257},
  timestamp = {00,100},
  url = {http://www.sciencedirect.com/science/article/B6V6V-3V7SF4M-1B/2/34b99e154cb5ab0c2af3be24aa3df1b8}
}

@ARTICLE{2009_JNL_HumanMotionTracking_Horaud,
  author = {Horaud, R. and Niskanen, M. and Dewaele, G. and Boyer, E.},
  title = {Human Motion Tracking by Registering an Articulated Surface to 3D
	Points and Normals},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2009},
  volume = {31},
  pages = {158 -163},
  number = {1},
  month = {jan. },
  abstract = {We address the problem of human motion tracking by registering a surface
	to 3-D data. We propose a method that iteratively computes two things:
	Maximum likelihood estimates for both the kinematic and free-motion
	parameters of an articulated object, as well as probabilities that
	the data are assigned either to an object part, or to an outlier
	cluster. We introduce a new metric between observed points and normals
	on one side, and a parameterized surface on the other side, the latter
	being defined as a blending over a set of ellipsoids. We claim that
	this metric is well suited when one deals with either visual-hull
	or visual-shape observations. We illustrate the method by tracking
	human motions using sparse visual-shape data (3-D surface points
	and normals) gathered from imperfect silhouettes.},
  comment = {trk_people},
  doi = {10.1109/TPAMI.2008.108},
  file = {:papers\\2009 JNL, Human Motion Tracking by Registering an Articulated Surface to 3D Points and Normals (Horaud, Niskanen, Dewaele, PAMI, 6).pdf:PDF},
  issn = {0162-8828},
  keywords = {3D data;articulated surface;human motion tracking;maximum likelihood
	estimates;sparse visual-shape data;visual-shape observations;image
	motion analysis;image registration;maximum likelihood estimation;probability;tracking;Algorithms;Artificial
	Intelligence;Computer Simulation;Humans;Image Interpretation, Computer-Assisted;Imaging,
	Three-Dimensional;Joints;Models, Biological;Movement;Pattern Recognition,
	Automated;},
  timestamp = {-}
}

@ARTICLE{1981_JNL_OpticalFlow_HornSchunck,
  author = {Horn, B.K.P. and Schunck, B.G.},
  title = {Determining optical flow},
  journal = {Artificial Intelligence},
  year = {1981},
  volume = {17},
  pages = {185 - 203},
  number = {1-3},
  abstract = {Optical flow cannot be computed locally, since only one independent
	measurement is available from the image sequence at a point, while
	the flow velocity has two components. A second constraint is needed.
	A method for finding the optical flow pattern is presented which
	assumes that the apparent velocity of the brightness pattern varies
	smoothly almost everywhere in the image. An iterative implementation
	is shown which successfully computes the optical flow for a number
	of synthetic image sequences. The algorithm is robust in that it
	can handle image sequences that are quantized rather coarsely in
	space and time. It is also insensitive to quantization of brightness
	levels and additive noise. Examples are included where the assumption
	of smoothness is violated at singular points or along lines in the
	image},
  address = {Netherlands},
  comment = {motion},
  copyright = {Copyright 1982, IEE},
  file = {:papers\\1981 JNL, Determining Optical Flow (Horn, Schunck).pdf:PDF},
  issn = {0004-3702},
  keywords = {iterative methods;pattern recognition;picture processing;},
  language = {English},
  timestamp = {05,000},
  url = {http://dx.doi.org/10.1016/0004-3702(81)90024-2}
}

@ARTICLE{2000_JNL_3DreconstructionSingleCamera,
  author = {Howe, N.R. and Leventon, M.E. and Freeman, W.T.},
  title = {Bayesian reconstruction of 3d human motion from single-camera video.},
  journal = {Advances in Neural Information Processing Systems},
  year = {2000},
  comment = {recog_pose},
  file = {:papers\\2000 JNL, Bayesian reconstruction of 3d human motion from single-camera video (Howe, ANIP, 189).pdf:PDF},
  timestamp = {00,200}
}

@ARTICLE{2004_JNL_SURVEYiu_Hu,
  author = {Hu, Weiming and Tan, Tieniu and Wang, Liang and Maybank, Steve},
  title = {A survey on visual surveillance of object motion and behaviors},
  journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications
	and Reviews},
  year = {2004},
  volume = {34},
  pages = {334-352},
  abstract = {Visual surveillance in dynamic scenes, especially for humans and vehicles,
	is currently one of the most active research topics in computer vision.
	It has a wide spectrum of promising applications, including access
	control in special areas, human identification at a distance, crowd
	flux statistics and congestion analysis, detection of anomalous behaviors,
	and interactive surveillance using multiple cameras, etc. In general,
	the processing framework of visual surveillance in dynamic scenes
	includes the following stages: modeling of environments, detection
	of motion, classification of moving objects, tracking, understanding
	and description of behaviors, human identification, and fusion of
	data from multiple cameras. We review recent developments and general
	strategies of all these stages. Finally, we analyze possible research
	directions, e.g., occlusion handling, a combination of two- and three-dimensional
	tracking, a combination of motion analysis and biometrics, anomaly
	detection and behavior prediction, content-based retrieval of surveillance
	videos, behavior understanding and natural language description,
	fusion of information from multiple sensors, and remote surveillance.
	2004 IEEE.},
  comment = {survey},
  file = {:papers\\2004 JNL, A survey on visual surveillance of object motion and behaviors (SURVEY, Maybank, TSMCC, 520).pdf:PDF},
  keywords = {Computer vision Algorithms Cameras Feature extraction Image segmentation
	Image sensors Motion estimation Object recognition Sensor data fusion
	Tracking (position)},
  owner = {salman},
  timestamp = {00,500}
}

@INPROCEEDINGS{1997_CNF_ObjectIdentificationBayesian_HuangRussell,
  author = {Huang, T. and Russell, S.},
  title = {Object identification in a Bayesian context},
  booktitle = {IJCAI-97. Proceedings of the Fifteenth International Joint Conference
	on Artificial Intelligence},
  year = {1997},
  volume = {vol.2},
  abstract = {Object identification-the task of deciding that two observed objects
	are in fact one and the same object-is a fundamental requirement
	for any situated agent that reasons about individuals. Object identity,
	as represented by the equality operator between two terms in predicate
	calculus, is essentially a first-order concept. Raw sensory observations,
	on the other hand, are essentially propositional-especially when
	formulated as evidence in standard probability theory. This paper
	describes patterns of reasoning that allow identity sentences to
	be grounded in sensory observations, thereby bridging the gap. We
	begin by defining a physical event space over which probabilities
	are defined. We then introduce an identity criterion, which selects
	those events that correspond to identity between observed objects.
	From this, we are able to compute the probability that any two objects
	are the same, given a stream of observations of many objects. We
	show that the appearance probability, which defines how an object
	can be expected to appear at subsequent observations given its current
	appearance, is a natural model for this type of reasoning. We apply
	the theory to the task of recognizing cars observed by cameras at
	widely separated sites in a freeway network, with new heuristics
	to handle the inevitable complexity of matching large numbers of
	objects and with online learning of appearance probability models.
	Despite extremely noisy observations, we are able to achieve high
	levels of performance},
  comment = {analysis},
  copyright = {Copyright 1999, IEE},
  file = {:papers\\1997 CNF, Object identification in a bayesian context (Huang, Stuartrussell, IJCAI, 105).pdf:PDF},
  keywords = {Bayes methods;identification;inference mechanisms;object recognition;},
  language = {English},
  timestamp = {00,100}
}

@INPROCEEDINGS{2005_CNF_TRKoccl_Huang,
  author = {Huang, Y. and Essa, I.},
  title = {Tracking multiple objects through occlusions},
  booktitle = {Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
	Society Conference on},
  year = {2005},
  volume = {2},
  pages = { 1051 - 1058 vol. 2},
  abstract = {We present an approach for tracking varying number of objects through
	both temporally and spatially significant occlusions. Our method
	builds on the idea of object permanence to reason about occlusions.
	To this end, tracking is performed at both the region level and the
	object level. At the region level, a customized genetic algorithm
	is used to search for optimal region tracks. This limits the scope
	of object trajectories. At the object level, each object is located
	based on adaptive appearance models, spatial distributions and inter-occlusion
	relationships. The proposed architecture is capable of tracking objects
	even in the presence of long periods of full occlusions. We demonstrate
	the viability of this approach by experimenting on several videos
	of a user interacting with a variety of objects on a desktop.},
  comment = {TRK_occlusion},
  file = {:papers\\2005 CNF, Tracking multiple objects through occlusions (Huang).pdf:PDF},
  issn = {1063-6919 },
  keywords = { adaptive appearance model; genetic algorithm; inter-occlusion relationship;
	multiple object tracking; object trajectory; optimal region track;
	spatial distribution; spatial occlusion; temporal occlusion; user
	interaction; genetic algorithms; hidden feature removal; object detection;
	tracking;},
  timestamp = {00,040}
}

@ARTICLE{2002_JNL_SequentialMCMTT_Hue,
  author = {Hue, C. and Le Cadre, J. P. and Perez, P.},
  title = {Sequential Monte Carlo methods for multiple target tracking and data
	fusion},
  journal = {Signal Processing, IEEE Transactions on},
  year = {2002},
  volume = {50},
  pages = {309-325},
  number = {2},
  abstract = {The classical particle filter deals with the estimation of one state
	process conditioned on a realization of one observation process.
	We extend it here to the estimation of multiple state processes given
	realizations of several kinds of observation processes. The new algorithm
	is used to track with success multiple targets in a bearings-only
	context, whereas a JPDAF diverges. Making use of the ability of the
	particle filter to mix different types of observations, we then investigate
	how to join passive and active measurements for improved tracking},
  comment = {trk_filtering},
  file = {:papers\\2002 JNL, Sequential Monte Carlo methods for multiple target tracking and data fusion (Hue, Cadre, Perez, TSP, 193).pdf:PDF},
  keywords = {Monte Carlo methods filtering theory sensor fusion sequential estimation
	target tracking Bayesian estimation active measurements bearings-only
	tracking classical particle filter data fusion multiple state processes
	multiple target tracking observation processes passive measurements
	sequential Monte Carlo methods},
  owner = {salman},
  timestamp = {00,200}
}

@ARTICLE{1968_JNL_MeanAccuracyStatPatternRecog_Hughes,
  author = {Hughes, G.},
  title = {On the mean accuracy of statistical pattern recognizers},
  journal = {Information Theory, IEEE Transactions on},
  year = {1968},
  volume = {14},
  pages = {55--63},
  number = {1},
  comment = {PRML},
  file = {:papers\\1968 JNL, On the mean accuracy of statistical pattern recognizers (Hughes, TIT, 456).pdf:PDF},
  owner = {salman},
  timestamp = {00,500}
}

@INPROCEEDINGS{2009_CNF_BayesianFacialRecognition_Hui,
  author = {Hui, Zhao and Zhiliang, Wang},
  title = {An Improved Greedy Search Algorithm of Bayesian Network Structures
	for Facial Action Units Recognition},
  booktitle = {Natural Computation, 2009. ICNC '09. Fifth International Conference
	on},
  year = {2009},
  volume = {1},
  abstract = {Bayesian Networks (BN) are an effective method to recognize facial
	action units (AUs) combinations, which is a key issue of AUs recognition.
	Learning BN structures from data is NP-hard. Greedy search algorithm
	is a practical approach to learn BN from data, but it is liable to
	get stuck at a local maximum. In this paper, an improved greedy search
	algorithm is proposed in order to deal with the above-mentioned problem.
	The proposed algorithm starts from a prior structure, which is constructed
	by prior knowledge and simply statistics of AUs database, then updates
	the prior BN structure not only with the BN structure that has maximum
	score among all of the nearest neighbors of the prior BN structure,
	but also updates it with some BN structures that have higher score.
	The experiments show that the proposed algorithm is computationally
	simple, easy to implement, and may effectively avoid getting stuck
	at a local maximum.},
  comment = {recog_action},
  file = {:papers\\2009 CNF, An Improved Greedy Search Algorithm of Bayesian  Network Structures for Facial Action Units Recognition (ZhaoICNC).pdf:PDF},
  keywords = {belief networks computational complexity face recognition greedy algorithms
	search problems Bayesian network structures NP-hard problem facial
	action units recognition improved greedy search},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{1993_CNF_TrackingNonrigid_HuttenlocherNohRucklidge,
  author = {Huttenlocher, D. P. and Noh, J. J. and Rucklidge, W. J.},
  title = {Tracking non-rigid objects in complex scenes},
  booktitle = {Computer Vision, 1993. Proceedings., Fourth International Conference
	on},
  year = {1993},
  pages = {93-101},
  abstract = {The authors describe a model-based method for tracking nonrigid objects
	moving in a complex scene. The method operates by extracting two-dimensional
	models of an object from a sequence of images. The basic idea underlying
	the technique is to decompose the image of a solid object moving
	in space into two components: a two-dimensional motion and a two-dimensional
	shape change. The motion component is factored out and the shape
	change is represented explicitly by a sequence of two-dimensional
	models, one corresponding to each image frame. The major assumption
	underlying the method is that the two-dimensional shape of an object
	will change slowly from one frame to the next. There is no assumption,
	however, that the two-dimensional image motion between successive
	frames will be small},
  comment = {trk_people},
  file = {:papers\\1993 CNF, Tracking nonrigid objects in complex scenes (Huttenlocher, Noh, Rucklidge, ICCV, 195).pdf:PDF},
  keywords = {image recognition motion estimation object detection complex scenes
	model-based method nonrigid objects tracking solid object two-dimensional
	models two-dimensional motion two-dimensional shape change},
  owner = {salman},
  timestamp = {00,200}
}

@ARTICLE{2007_JNL_2D3Drealtime_Ideses,
  author = {Ideses, Ianir and Yaroslavsky, Leonid and Fishbain, Barak},
  title = {Real-time 2D to 3D video conversion},
  journal = {Journal of Real-Time Image Processing},
  year = {2007},
  volume = {2},
  pages = {3-9},
  number = {1},
  abstract = {Abstract&nbsp;&nbsp;We present a real-time implementation of 2D to
	3D video conversion using compressed video. In our method, compressed
	2D video is analyzed by extracting motion vectors. Using the motion
	vector maps, depth maps are built for each frame and the frames are
	segmented to provide object-wise depth ordering. These data are then
	used to synthesize stereo pairs. 3D video synthesized in this fashion
	can be viewed using any stereoscopic display. In our implementation,
	anaglyph projection was selected as the 3D visualization method,
	because it is mostly suited to standard displays.},
  comment = {3D_stereo},
  file = {:papers\\2007 JNL, Real-time 2D to 3D video conversion (Ideses, Yaroslavsky, Fishbain).pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{1997_CNF_ClosedWorldTracking_Intille,
  author = {Intille, S. S. and Davis, J. W. and Bobick, A. F.},
  title = {Real-time closed-world tracking},
  booktitle = {Computer Vision and Pattern Recognition, 1997. Proceedings., 1997
	IEEE Computer Society Conference on},
  year = {1997},
  pages = {697-703},
  comment = {trk_people},
  file = {:papers\\1997 CNF, Real-time closed-world tracking (Intille, Bobick, CVPR, 220).pdf:PDF},
  keywords = {image matching motion estimation object recognition real-time systems
	tracking algorithm testing child tracking closed-world assumption
	contextual information erratic movement image correspondence image
	feature selection interactive narrative playspace multiple nonrigid
	object tracking object collisions real-time closed-world tracking
	algorithm},
  owner = {salman},
  timestamp = {00,200}
}

@ARTICLE{1998_JNL_VideoIndexingMosaics_Irani,
  author = {Irani, M. and Anandan, P.},
  title = {Video indexing based on mosaic representations},
  journal = {Proceedings of the IEEE},
  year = {1998},
  volume = {86},
  pages = {905-921},
  number = {5},
  abstract = {Video is a rich source of information. It provides visual information
	about scenes. This information is implicitly buried inside the raw
	video data, however, and is provided with the cost of very high temporal
	redundancy. While the standard sequential form of video storage is
	adequate for viewing in a movie mode, it fails to support rapid access
	to information of interest that is required in many of the emerging
	applications of video. This paper presents an approach for efficient
	access, use and manipulation of video data. The video data are first
	transformed from their sequential and redundant frame-based representation,
	in which the information about the scene is distributed over many
	frames, to an explicit and compact scene-based representation, to
	which each frame can be directly related. This compact reorganization
	of the video data supports nonlinear browsing and efficient indexing
	to provide rapid access directly to information of interest. This
	paper describes a new set of methods for indexing into the video
	sequence based on the scene-based representation. These indexing
	methods are based on geometric and dynamic information contained
	in the video. These methods complement the more traditional content-based
	indexing methods, which utilize image appearance information (namely,
	color and texture properties) but are considerably simpler to achieve
	and are highly computationally efficient},
  comment = {analysis},
  file = {:papers\\1998 JNL, Video indexing based on mosaic representations (Irani, Anandan, ProcIEEE, 239).pdf:PDF},
  keywords = {image representation image sequences indexing multimedia computing
	visual databases content-based indexing dynamic information frame-based
	representation geometric information image appearance information
	information access mosaic representations nonlinear browsing scene-based
	representation temporal redundancy video databases video indexing
	video manipulation video sequence video storage visual information},
  owner = {salman},
  timestamp = {00,250}
}

@ARTICLE{1994_JNL_OccludingTransparentMotions_Irani,
  author = {Irani, M. and Rousso, B. and Peleg, S.},
  title = {Computing occluding and transparent motions},
  journal = {International Journal of Computer Vision},
  year = {1994},
  volume = {12},
  pages = {5-16},
  abstract = {Computing the motions of several moving objects in image sequences
	involves simultaneous motion analysis and segmentation. This task
	can become complicated when image motion changes significantly between
	frames, as with camera vibrations. Such vibrations make tracking
	in longer sequences harder, as temporal motion constancy cannot be
	assumed. The problem becomes even more difficult in the case of transparent
	motions. A method is presented for detecting and tracking occluding
	and transparent moving objects, which uses temporal integration without
	assuming motion constancy. Each new frame in the sequence is compared
	to a dynamic internal representation image of the tracked object.
	The internal representation image is constructed by temporally integrating
	frames after registration based on the motion computation. The temporal
	integration maintains sharpness of the tracked object, while blurring
	objects that have other motions. Comparing new frames to the internal
	representation image causes the motion analysis algorithm to continue
	tracking the same object in subsequent frames, and to improve the
	segmentation},
  comment = {motion},
  file = {:papers\\1994 JNL, Computing Occluding and Transparent Motions (Irani, IJCV, 358).pdf:PDF},
  keywords = {image segmentation motion estimation},
  owner = {salman},
  timestamp = {00,400}
}

@ARTICLE{1998_JNL_Condensation_IsardBlake,
  author = {Isard, Michael and Blake, Andrew},
  title = {CONDENSATION - Conditional density propagation for visual tracking},
  journal = {International Journal of Computer Vision},
  year = {1998},
  volume = {29},
  pages = {5-28},
  number = {Compendex},
  abstract = {The problem of tracking curves in dense visual clutter is challenging.
	Kalman filtering is inadequate because it is based on Gaussian densities
	which, being unimodal, cannot represent simultaneous alternative
	hypotheses. The CONDENSATION algorithm uses "factored sampling",
	previously applied to the interpretation of static images, in which
	the probability distribution of possible interpretations is represented
	by a randomly generated set. CONDENSATION uses learned dynamical
	models, together with visual observations, to propagate the random
	set over time. The result is highly robust tracking of agile motion.
	Notwithstanding the use of stochastic methods, the algorithm runs
	in near real-time.},
  comment = {trk_filtering},
  file = {:papers\\1998 JNL, CONDENSATION - conditional density propagation for visual tracking (Isard, Blake, IJCV, 2887).pdf:PDF},
  keywords = {Computer vision Algorithms Computer simulation Degrees of freedom
	(mechanics) Kalman filtering Mathematical models Probability distributions
	Real time systems Signal processing},
  owner = {salman},
  timestamp = {03,000}
}

@INPROCEEDINGS{2001_CNF_TRKhuman_Isard,
  author = {Isard, M. and MacCormick, J.},
  title = {BRAMBLE: a Bayesian multiple-blob tracker},
  booktitle = {Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International
	Conference on},
  year = {2001},
  volume = {2},
  pages = {34-41 vol.2},
  abstract = {Blob trackers have become increasingly powerful in recent years largely
	due to the adoption of statistical appearance models which allow
	effective background subtraction and robust tracking of deforming
	foreground objects. It has been standard, however, to treat background
	and foreground modelling as separate processes-background subtraction
	is followed by blob detection and tracking-which prevents a principled
	computation of image likelihoods. This paper presents two theoretical
	advances which address this limitation and lead to a robust multiple-person
	tracking system suitable for single-camera real-time surveillance
	applications. The first innovation is a multi-blob likelihood function
	which assigns directly comparable likelihoods to hypotheses containing
	different numbers of objects. This likelihood function has a rigorous
	mathematical basis: it is adapted from the theory of Bayesian correlation,
	but uses the assumption of a static camera to create a more specific
	background model while retaining a unified approach to background
	and foreground modelling. Second we introduce a Bayesian filter for
	tracking multiple objects when the number of objects present is unknown
	and varies over time. We show how a particle filter can be used to
	perform joint inference on both the number of objects present and
	their configurations. Finally we demonstrate that our system runs
	comfortably in real time on a modest workstation when the number
	of blobs in the scene is small},
  comment = {trk_people},
  file = {:papers\\2001 CNF, BraMBLe_A bayesian multiple-blob tracker (Isard, MacCormick, ICCV, 430).pdf:PDF},
  keywords = {Bayes methods computer vision inference mechanisms statistical analysis
	tracking Bayesian correlation Bayesian filter Bayesian multiple-blob
	tracker BraMBLe background modelling background subtraction foreground
	modelling image likelihoods multiple-person tracking system robust
	tracking single-camera real-time surveillance statistical appearance
	models},
  owner = {salman},
  timestamp = {00,450}
}

@ARTICLE{6071760,
  author = {Ivins, J. and Porrill, J.},
  title = {Constrained active region models for fast tracking in color image
	sequences},
  journal = {Computer Vision and Image Understanding},
  year = {1998},
  volume = { 72},
  pages = {54 - 71},
  number = { 1},
  note = {constrained active region models;fast tracking;color image sequences;image
	segmentation;computer vision;deformable models;edge detection;statistical
	characteristics;frame-rate tracking;color video images;color representations;intensity-based
	RGB space;hue;saturation;affine motion;local energy minima;iterative
	gradient descent;least-squares minimization;active region model;affine
	transformation;},
  abstract = {Image segmentation is a fundamental problem in computer vision, for
	which deformable models offer a partial solution. Most deformable
	models work by performing some kind of edge detection; complementary
	region growing methods have not often been used. As a result, deformable
	models that track regions rather than edges have yet to be developed
	to a great extent. Active region models are a relatively new type
	of deformable model driven by a region energy that is a function
	of the statistical characteristics of an image. This paper describes
	the use of constrained active region models for frame-rate tracking
	in color video images on widely available computer hardware. Two
	of the many color representations now in use are reviewed for this
	purpose: the intensity-based RGB space and the more intuitive HSV
	space. Normalized RGB, which is essentially a measure of hue and
	saturation, emerges as the preferred representation because it is
	invariant to illumination changes and can be obtained from many frame-grabbers
	via a simple fast software transformation. Three types of motion
	are examined for constraining deformable models: rigid models can
	only translate and rotate to fit image features; conformal models
	can also change size; affine models exhibit two kinds of shearing
	in addition to the other components. Two methods are described for
	producing affine motion, given the desired unconstrained motion calculated
	by searching for local energy minima lying perpendicular to the model
	boundary. An existing method, based on iterative gradient descent,
	computes translating, rotating, scaling, and shearing forces which
	can be combined to produce affine and other types of motion. A faster,
	more accurate method uses least-squares minimization to approximate
	the desired motion; with this method it is also possible to derive
	specific equations for rigid and conformal motion and to correct
	for the aperture problem associated with the perpendicular search
	method. The advantages of the new least-squares method are illustrated
	by using it to drive an active region model via an affine transformation
	which tracks the movements of a robot arm at frame rate in color
	video images},
  address = {USA},
  comment = {trk_color},
  copyright = {Copyright 1998, IEE},
  file = {:papers\\1998 JNL, Constrained active region models for fast tracking in color image sequences (Ivins, Porrill).pdf:PDF},
  issn = {1077-3142},
  keywords = {computer vision;image segmentation;image sequences;least squares approximations;minimisation;},
  language = {English},
  timestamp = {000,020},
  url = {http://dx.doi.org/10.1006/cviu.1997.0653}
}

@INPROCEEDINGS{1996_CNF_VideoBook_Iyengar,
  author = {Iyengar, G. and Lippman, A. B.},
  title = {VideoBook: an experiment in characterization of video},
  booktitle = {Proceedings of 3rd IEEE International Conference on Image Processing,
	16-19 Sept. 1996},
  year = {1996},
  abstract = {We present a framework for content based query and retrieval of information
	from large video databases. This framework enables content based
	retrieval of video sequences by characterizing the sequences using
	motion, texture and colorimetry cues. This characterization is biologically
	inspired and results in a compact parameter space where every segment
	of video is represented by an 8 dimensional vector. Searching and
	retrieval is done in real-time with high accuracy in this parameter
	space. The present version of the VideoBook has 165 video sequences,
	each 15 seconds long at 30 frames a second representing storage of
	65 Giga bytes. The VideoBook is able to search and retrieve video
	sequences with 92% accuracy in real-time. Experiments thus demonstrate
	that the characterization is capable of extracting higher level structure
	from raw pixel values},
  comment = {analysis},
  file = {:papers\\1996 CNF, VideoBook_ an experiment in characterization of video (Iyangar, Lipman, ICIP, 31).pdf:PDF},
  keywords = {colorimetry image colour analysis image representation image segmentation
	image sequences image texture motion estimation query processing
	video signal processing visual databases},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{1981_JNL_ImageDC_Jain,
  author = {Jain, A.K.},
  title = {Image data compression: A review},
  journal = {Proceedings of the IEEE},
  year = {1981},
  volume = {69},
  pages = { 349 - 389},
  number = {3},
  month = {march},
  abstract = { With the continuing growth of modern communications technology, demand
	for image transmission and storage is increasing rapidly. Advances
	in computer technology for mass storage and digital processing have
	paved the way for implementing advanced data compression techniques
	to improve the efficiency of transmission and storage of images.
	In this paper a large variety of algorithms for image data compression
	are considered. Starting with simple techniques of sampling and pulse
	code modulation (PCM), state of the art algorithms for two-dimensional
	data transmission are reviewed. Topics covered include differential
	PCM (DPCM) and predictive coding, transform coding, hybrid coding,
	interframe coding, adaptive techniques, and applications. Effects
	of channel errors and other miscellaneous related topics are also
	considered. While most of the examples and image models have been
	specialized for visual images, the techniques discussed here could
	be easily adapted more generally for multidimensional data compression.
	Our emphasis here is on fundamentals of the various techniques. A
	comprehensive bibliography with comments is included for a reader
	interested in further details of the theoretical and experimental
	results discussed here.},
  comment = {survey},
  file = {:papers\\1981 JNL, Image data compression_ A review (Jain).pdf:PDF},
  issn = {0018-9219},
  timestamp = {00,550}
}

@ARTICLE{2000_JNL_SURVEYprml_Jain,
  author = {Jain, A.K. and Duin, R.P.W. and Jianchang Mao},
  title = {Statistical pattern recognition: a review},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {4 -37},
  number = {1},
  month = {Jan},
  abstract = {The primary goal of pattern recognition is supervised or unsupervised
	classification. Among the various frameworks in which pattern recognition
	has been traditionally formulated, the statistical approach has been
	most intensively studied and used in practice. More recently, neural
	network techniques and methods imported from statistical learning
	theory have been receiving increasing attention. The design of a
	recognition system requires careful attention to the following issues:
	definition of pattern classes, sensing environment, pattern representation,
	feature extraction and selection, cluster analysis, classifier design
	and learning, selection of training and test samples, and performance
	evaluation. In spite of almost 50 years of research and development
	in this field, the general problem of recognizing complex patterns
	with arbitrary orientation, location, and scale remains unsolved.
	New and emerging applications, such as data mining, web searching,
	retrieval of multimedia data, face recognition, and cursive handwriting
	recognition, require robust and efficient pattern recognition techniques.
	The objective of this review paper is to summarize and compare some
	of the well-known methods used in various stages of a pattern recognition
	system and identify research topics and applications which are at
	the forefront of this exciting and challenging field},
  comment = {survey},
  doi = {10.1109/34.824819},
  file = {:papers\\2000 JNL, Statistical pattern recognition_ a review (Jain, Duin, Mao, PAMI, 2426).pdf:PDF},
  issn = {0162-8828},
  keywords = {classifier design;cluster analysis;complex patterns;cursive handwriting
	recognition;data mining;multimedia data retrieval;neural network
	techniques;pattern classes;pattern representation;performance evaluation;sensing
	environment;statistical learning theory;statistical pattern recognition;supervised
	classification;unsupervised classification;web searching;Bayes methods;decision
	theory;learning (artificial intelligence);neural nets;parameter estimation;pattern
	recognition;},
  timestamp = {02,500}
}

@ARTICLE{1979_JNL_AccumulativePictureDifferences_Jain,
  author = {Jain, Ramesh and Nagel, H. H.},
  title = {On the Analysis of Accumulative Difference Pictures from Image Sequences
	of Real World Scenes},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1979},
  volume = {PAMI-1},
  pages = {206-214},
  number = {2},
  abstract = {The count of events where sample areas from the second and subsequent
	frames of a TV-image sequence are incompatible with the corresponding
	sample area of the first frame are accumulated in a first-order difference
	picture (FODP). Analysis of this FODP provides a separate estimate
	for images of moving objects and of stationary scene components.
	We start from the hypothesis that the first frame represents the
	stationary scene component. Once it has been recognized that a subarea
	of this initial estimate corresponds to the image of a moving object,
	the grey values in this subarea are replaced by later estimates of
	the stationary background at this position. No knowledge specific
	to a particular scene is utilized in the algorithm. The results for
	two scene sequences are presented.},
  comment = {analysis},
  file = {:papers\\1979 JNL, On the analysis of accumulative difference pictures from image sequences of real world scenes (Jain, Nagel, PAMI, 161).pdf:PDF},
  owner = {salman},
  timestamp = {00,200}
}

@ARTICLE{2008_JNL_SingleParticleTracking_Jaqaman,
  author = {Jaqaman, Khuloud and Loerke, Dinah and Mettlen, Marcel and Kuwata,
	Hirotaka and Grinstein, Sergio and Schmid, Sandra L and Danuser,
	Gaudenz},
  title = {Robust single-particle tracking in live-cell time-lapse sequences.},
  journal = {Nature Methods},
  year = {2008},
  volume = {5},
  pages = {695 - 702},
  number = {8},
  abstract = {Single-particle tracking (SPT) is often the rate-limiting step in
	live-cell imaging studies of subcellular dynamics. Here we present
	a tracking algorithm that addresses the principal challenges of SPT,
	namely high particle density, particle motion heterogeneity, temporary
	particle disappearance, and particle merging and splitting. The algorithm
	first links particles between consecutive frames and then links the
	resulting track segments into complete trajectories. Both steps are
	formulated as global combinatorial optimization problems whose solution
	identifies the overall most likely set of particle trajectories throughout
	a movie. Using this approach, we show that the GTPase dynamin differentially
	affects the kinetics of long- and short-lived endocytic structures
	and that the motion of CD36 receptors along cytoskeleton-mediated
	linear tracks increases their aggregation probability. Both applications
	indicate the requirement for robust and complete tracking of dense
	particle fields to },
  comment = {trk},
  file = {:papers\\2008 JNL, Robust single-particle tracking in live-cell time-lapse sequences (Jaqaman et al,, Nature, 32).pdf:PDF},
  issn = {15487091},
  keywords = {PARTICLES, CELLS, CELL membranes, GUANOSINE triphosphatase, MOLECULES},
  timestamp = {00,032},
  url = {http://www.library.gatech.edu:2048/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=33380436&site=ehost-live}
}

@INPROCEEDINGS{2003_CNF_TrackingMultipleCamerasDisjointViews_JavedRasheedShafiqueMubarakshah,
  author = {Javed, O. and Rasheed, Z. and Shafique, K. and Shah, M.},
  title = {Tracking across multiple cameras with disjoint views},
  booktitle = {Computer Vision, 2003. Proceedings. Ninth IEEE International Conference
	on},
  year = {2003},
  abstract = {Conventional tracking approaches assume proximity in space, time and
	appearance of objects in successive observations. However, observations
	of objects are often widely separated in time and space when viewed
	from multiple non-overlapping cameras. To address this problem, we
	present a novel approach for establishing object correspondence across
	non-overlapping cameras. Our multicamera tracking algorithm exploits
	the redundance in paths that people and cars tend to follow, e.g.
	roads, walk-ways or corridors, by using motion trends and appearance
	of objects, to establish correspondence. Our system does not require
	any inter-camera calibration, instead the system learns the camera
	topology and path probabilities of objects using Parzen windows,
	during a training phase. Once the training is complete, correspondences
	are assigned using the maximum a posteriori (MAP) estimation framework.
	The learned parameters are updated with changing trajectory patterns.
	Experiments with real world videos are reported, which validate the
	proposed approach.},
  comment = {trk_people_multiCamera},
  file = {:papers\\2003 CNF, Tracking across multiple cameras with disjoint views (Javed, Rasheed, Shafique, Mubarakshah, ICCV, 0, maybe 167).pdf:PDF},
  keywords = {cameras computer vision image motion analysis maximum likelihood estimation
	object detection topology tracking Parzen windows appearance proximity
	camera topology learning disjoint views maximum a posteriori estimation
	motion trends multicamera tracking algorithm multiple cameras nonoverlapping
	cameras object correspondence path probabilities path redundance
	space proximity time proximity trajectory patterns videos},
  owner = {salman},
  timestamp = {00,170}
}

@ARTICLE{1982_JNL_MaxEntropy_Jaynes,
  author = {Jaynes, E.T.},
  title = {On the rationale of maximum-entropy methods},
  journal = {Proceedings of the IEEE},
  year = {1982},
  volume = {70},
  pages = { 939 - 952},
  number = {9},
  month = {sept.},
  abstract = { We discuss the relations between maximum-entropy (MAXENT) and other
	methods of spectral analysis such as the Schuster, Blackman-Tukey,
	maximum-likelihood, Bayesian, and Autoregressive (AR, ARMA, or ARIMA)
	models, emphasizing that they are not in conflict, but rather are
	appropriate in different problems. We conclude that: 1) "Orthodox"
	sampling theory methods are useful in problems where we have a known
	model (sampling distribution) for the properties of the noise, but
	no appreciable prior information about the quantities being estimated.
	2) MAXENT is optimal in problems where we have prior information
	about multiplicities, but no noise. 3) The full Bayesian solution
	includes both of these as special cases and is needed in problems
	where we have both prior information and noise. 4) AR models are
	in one sense a special case of MAXENT, but in another sense they
	are ubiquitous in all spectral analysis problems with discrete time
	series. 5) Empirical methods such as Blackman-Tukey, which do not
	invoke even a likelihood function, are useful in the preliminary,
	exploratory phase of a problem where our knowledge is sufficient
	to permit intuitive judgments about how to organize a calculation
	(smoothing, decimation, windows, prewhitening, padding with zeroes,
	etc.) but insufficient to set up a quantitative model which would
	do the proper things for us automatically and optimally.},
  comment = {IT},
  file = {:papers\\1982 JNL, On the rationale of maximum-entropy methods (Jaynes, ProcIEEE, 683).pdf:PDF},
  issn = {0018-9219},
  timestamp = {00,700}
}

@ARTICLE{2003_JNL_TRKsubspace_Jepson,
  author = {Jepson, A.D. and Fleet, D.J. and El-Maraghi, T.F.},
  title = {Robust online appearance models for visual tracking},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2003},
  volume = {25},
  pages = { 1296 - 1311},
  number = {10},
  month = {oct.},
  abstract = { We propose a framework for learning robust, adaptive, appearance
	models to be used for motion-based tracking of natural objects. The
	model adapts to slowly changing appearance, and it maintains a natural
	measure of the stability of the observed image structure during tracking.
	By identifying stable properties of appearance, we can weight them
	more heavily for motion estimation, while less stable properties
	can be proportionately downweighted. The appearance model involves
	a mixture of stable image structure, learned over long time courses,
	along with two-frame motion information and an outlier process. An
	online EM-algorithm is used to adapt the appearance model parameters
	over time. An implementation of this approach is developed for an
	appearance model based on the filter responses from a steerable pyramid.
	This model is used in a motion-based tracking algorithm to provide
	robustness in the face of image outliers, such as those caused by
	occlusions, while adapting to natural changes in appearance such
	as those due to facial expressions or variations in 3D pose.},
  comment = {trk_people},
  doi = {10.1109/TPAMI.2003.1233903},
  file = {:papers\\2003 JNL, Robust online appearance models for visual tracking (Jepson, Fleet, Thomas El-Maraghi, PAMI, 437).pdf:PDF},
  issn = {0162-8828},
  keywords = { adaptive appearance models; appearance models; image outliers; motion-based
	tracking; natural objects; occlusion; optical flow; tracking; computer
	vision; image sequences; motion estimation; object recognition;},
  review = {salman: nice review},
  timestamp = {00,450}
}

@INPROCEEDINGS{2008_CNF_HumanActions_Jingen,
  author = {Jingen, Liu and Ali, S. and Shah, M.},
  title = {Recognizing human actions using multiple features},
  booktitle = {Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference
	on},
  year = {2008},
  pages = {1-8},
  abstract = {In this paper, we propose a framework that fuses multiple features
	for improved action recognition in videos. The fusion of multiple
	features is important for recognizing actions as often a single feature
	based representation is not enough to capture the imaging variations
	(view-point, illumination etc.) and attributes of individuals (size,
	age, gender etc.). Hence, we use two types of features: i) a quantized
	vocabulary of local spatio-temporal (ST) volumes (or cuboids), and
	ii) a quantized vocabulary of spin-images, which aims to capture
	the shape deformation of the actor by considering actions as 3D objects
	(x, y, t). To optimally combine these features, we treat different
	features as nodes in a graph, where weighted edges between the nodes
	represent the strength of the relationship between entities. The
	graph is then embedded into a k-dimensional space subject to the
	criteria that similar nodes have Euclidian coordinates which are
	closer to each other. This is achieved by converting this constraint
	into a minimization problem whose solution is the eigenvectors of
	the graph Laplacian matrix. This procedure is known as Fiedler embedding.
	The performance of the proposed framework is tested on publicly available
	data sets. The results demonstrate that fusion of multiple features
	helps in achieving improved performance, and allows retrieval of
	meaningful features and videos from the embedding space.},
  comment = {recog_action_volume},
  file = {:papers\\2008 CNF, Recognizing human actions using multiple features (Liu, Ali, Mubarak Shah, CVPR, 30).pdf:PDF},
  keywords = {eigenvalues and eigenfunctions feature extraction graph theory image
	recognition image representation matrix algebra video retrieval Euclidian
	coordinates Fiedler embedding graph Laplacian matrix human action
	recognition imaging variations local spatiotemporal volumes multiple
	features quantized vocabulary single feature based representation
	video recognition},
  owner = {salman},
  timestamp = {00,030}
}

@INPROCEEDINGS{1999_CNF_TransductiveInferenceTextSVM_Joachims,
  author = {Joachims, T.},
  title = {Transductive inference for text classification using support vector
	machines},
  booktitle = {Proceedings of ICML-99: Sixteenth International Conference on Machine
	Learning, 27-30 June 1999},
  year = {1999},
  abstract = {Introduces transductive support vector machines (TSVMs) for text classification.
	While regular support vector machines (SVMs) try to induce a general
	decision function for a learning task, TSVMs take into account a
	particular test set and try to minimize misclassifications of just
	those particular examples. The paper presents an analysis of why
	TSVMs are well suited for text classification. These theoretical
	findings are supported by experiments on three test collections.
	The experiments show substantial improvements over inductive methods,
	especially for small training sets, cutting the number of labeled
	training examples down to a 20th on some tasks. This work also proposes
	an algorithm for training TSVMs efficiently, handling 10,000 examples
	and more},
  comment = {PRML},
  file = {:papers\\1999 CNF, Transductive inference for text classification using support vector machines (Joachims, ICME, 1078).pdf:PDF},
  keywords = {classification inference mechanisms learning by example minimisation
	relevance feedback text analysis},
  owner = {salman},
  timestamp = {01,000}
}

@ARTICLE{1973_JNL_VisualPerception_Johansson,
  author = {Johansson, G},
  title = {Visual perception of biological motion and a model for its analysis},
  journal = {Perception \& Psychophysics},
  year = {1973},
  volume = {14},
  pages = {201-211},
  comment = {motion_biology},
  file = {:papers\\1973 JNL, Visual perception of biological motion and a model for its analysis (Johansson, PP, 1579).pdf:PDF},
  issn = {0031-5117},
  timestamp = {01,500},
  unique-id = {{ISI:A1973R353300001}}
}

@BOOK{2002_BOOK_PCA_Jolliffe,
  title = {Principal component analysis},
  publisher = {Springer verlag},
  year = {2002},
  author = {Jolliffe, I.T.},
  file = {:papers\\2002 BOOK, Principal Component Analysis (Jolliffe).pdf:PDF}
}

@ARTICLE{2004_JNL_UKF_Julier,
  author = {Julier, S.J. and Uhlmann, J.K.},
  title = {Unscented filtering and nonlinear estimation},
  journal = {Proceedings of the IEEE},
  year = {2004},
  volume = {92},
  pages = { 401 - 422},
  number = {3},
  month = {mar},
  abstract = { The extended Kalman filter (EKF) is probably the most widely used
	estimation algorithm for nonlinear systems. However, more than 35
	years of experience in the estimation community has shown that is
	difficult to implement, difficult to tune, and only reliable for
	systems that are almost linear on the time scale of the updates.
	Many of these difficulties arise from its use of linearization. To
	overcome this limitation, the unscented transformation (UT) was developed
	as a method to propagate mean and covariance information through
	nonlinear transformations. It is more accurate, easier to implement,
	and uses the same order of calculations as linearization. This paper
	reviews the motivation, development, use, and implications of the
	UT.},
  comment = {trk_filtering},
  doi = {10.1109/JPROC.2003.823141},
  file = {:papers\\2004 JNL, Unscented filtering and nonlinear estimation (Julier, Uhlmann, ProcIEEE, 900).pdf:PDF},
  issn = {0018-9219},
  keywords = { EKF; extended Kalman filter; nonlinear estimation; nonlinear systems;
	nonlinear transformations; unscented filtering; unscented transformation;
	Kalman filters; covariance analysis; filtering theory; nonlinear
	estimation; nonlinear filters; nonlinear systems;},
  timestamp = {00,900}
}

@TECHREPORT{1996_REP_ApproxNLtransformPDF_Julier,
  author = {Simon Julier and Jeffrey K. Uhlmann},
  title = {A General Method for Approximating Nonlinear Transformations of Probability
	Distributions},
  institution = {Robotics Research Group, Department of Engineering Science, University
	of Oxford},
  year = {1996},
  comment = {trk_filtering},
  file = {:papers\\1996 REP, A General Method for Approximating Nonlinear Transformations of Probability Distributions (Julier, Uhlmann, Oxford, 303).pdf:PDF},
  timestamp = {00,300}
}

@INPROCEEDINGS{1997_CNF_UKF_Julier,
  author = {Simon J. Julier and Jeffrey K. Uhlmann},
  title = {A New Extension of the Kalman Filter to Nonlinear Systems},
  booktitle = {Proc. of Aerosense: The 11th Int. Symp. on
	
	Aerospace/Defence Sensing, Simulation and Controls},
  year = {1997},
  pages = {182--193},
  comment = {trk_filtering},
  file = {:papers\\1997 CNF, A New Extension of the Kalman Filter to Nonlinear Systems (Julier, Uhlmann, Aerosense, 1237).pdf:PDF},
  timestamp = {01,200}
}

@ARTICLE{1967_JNL_Bhattacharyya_Kailath,
  author = {Kailath, T.},
  title = {The Divergence and Bhattacharyya Distance Measures in Signal Selection},
  journal = {Communication Technology, IEEE Transactions on},
  year = {1967},
  volume = {15},
  pages = {52 -60},
  month = {february },
  abstract = {Minimization of the error probability to determine optimum signals
	is often difficult to carry out. Consequently, several suboptimum
	performance measures that are easier than the error probability to
	evaluate and manipulate have been studied. In this partly tutorial
	paper, we compare the properties of an often used measure, the divergence,
	with a new measure that we have called the Bhattacharyya distance.
	This new distance measure is often easier to evaluate than the divergence.
	In the problems we have worked, it gives results that are at least
	as good as, and are often better, than those given by the divergence.},
  comment = {SP},
  doi = {10.1109/TCOM.1967.1089532},
  file = {:papers\\1967 JNL, The Divergence and Bhattacharyya Distance Measures in Signal Selection (Kailath, TCT, 558).pdf:PDF},
  issn = {0018-9332},
  keywords = {Communication theory;Detection;Digital signals;Fading;Matched filters;signal-to-noise
	ratio (SNR);},
  timestamp = {00,500}
}

@INPROCEEDINGS{1988_CNF_AdvancesMultisensorSurveillance_KanadeCollinsLiptonBurtWixson,
  author = {Takeo Kanade and Robert T. Collins and Alan J. Lipton and Peter Burt
	and Lambert Wixson},
  title = {Advances in Cooperative Multi-Sensor Video Surveillance},
  booktitle = {Darpa IU Workshop},
  year = {1998},
  pages = {324},
  comment = {?},
  file = {:papers\\1998 CNF, Advances in cooperative multi-sensor Video Surveillance (Kanade, Collins, Lipton, DARPA, 216).pdf:PDF},
  timestamp = {00,200}
}

@INPROCEEDINGS{1995_CNF_PF_Kanazawa,
  author = {Kanazawa, K. and Koller, D. and Russell, S.},
  title = {Stochastic simulation algorithms for dynamic probabilistic networks},
  booktitle = {Proceedings of the Eleventh Conference on Uncertainty in Artificial
	Intelligence},
  year = {1995},
  pages = {346--351},
  organization = {Citeseer}
}

@INPROCEEDINGS{2004_CNF_ReacquisitionUsingInvariantAppearance_KangCohenMedioni,
  author = {Kang, Jinman and Cohen, I. and Medioni, G.},
  title = {Object reacquisition using invariant appearance model},
  booktitle = {Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International
	Conference on},
  year = {2004},
  volume = {4},
  abstract = {We present an approach for reacquisition of detected moving objects.
	We address the tracking problem by modeling the appearance of the
	moving region using stochastic models. The appearance of the object
	is described by multiple models representing spatial distributions
	of objects' colors and edges. This representation is invariant to
	2D rigid and scale transformation. It provides a good description
	of the object being tracked, and produces an efficient blob similarity
	measure for tracking. Three different similarity measures are proposed,
	and compared to show the performance of each model. The proposed
	appearance model allows to track a large number of moving people
	with partial and total occlusions and permits to reacquire objects
	that have been previously tracked. We demonstrate the performance
	of the system on several real video surveillance sequences.},
  comment = {?},
  file = {:papers\\2004 CNF, Object Reacquisition Using Invariant Appearance Model (Kang, Cohen, Medioni, ICPR, 16).pdf:PDF},
  keywords = {image motion analysis image sequences object detection stochastic
	processes tracking 2D rigid and scale transformation invariant appearance
	model moving object detection object reacquisition stochastic model
	video surveillance sequence},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{2003_CNF_TrackingWithinAndAcrossCameraStreams_KangCohenMedioni,
  author = {Kang, Jinman and Cohen, I. and Medioni, G.},
  title = {Continuous tracking within and across camera streams},
  booktitle = {Computer Vision and Pattern Recognition, 2003. Proceedings. 2003
	IEEE Computer Society Conference on},
  year = {2003},
  volume = {1},
  pages = {I-267-I-272 vol.1},
  abstract = {This paper presents a new approach for continuous tracking of moving
	objects observed by multiple, heterogeneous cameras. Our approach
	simultaneously processes video streams from stationary and pan-tilt-zoom
	cameras. The detection of moving objects from moving camera streams
	is performed by defining an adaptive background model that takes
	into account the camera motion approximated by an affine transformation.
	We address the tracking problem by separately modeling motion and
	appearance of the moving objects using two probabilistic models.
	For the appearance model, multiple color distribution components
	are proposed for ensuring a more detailed description of the object
	being tracked. The motion model is obtained using a Kalman filter
	(KF) process, which predicts the position of the moving object. The
	tracking is performed by the maximization of a joint probability
	model. The novelty of our approach consists in modeling the multiple
	trajectories observed by the moving and stationary cameras in the
	same KF framework. It allows deriving a more accurate motion measurement
	for objects simultaneously viewed by the two cameras and an automatic
	handling of occlusions, errors in the detection and camera handoff.
	We demonstrate the performances of the system on several video surveillance
	sequences.},
  comment = {trk_people},
  file = {:papers\\2003 CNF, Continuous Tracking Within and Across Camera Streams (Kang, Cohen, Medioni, CVPR, 107).pdf:PDF},
  keywords = {Kalman filters image motion analysis image sequences object detection
	optical tracking surveillance video signal processing Kalman filter
	adaptive background model affine transformation camera motion camera
	stream continuous tracking heterogeneous camera motion measurement
	motion model moving object multiple camera multiple color distribution
	object tracking pan-tilt-zoom camera probability model stationary
	camera video sequence video surveillance},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{7919284,
  author = {Seonghoon Kang and Bon-Woo Hwang and Seong-Whan Lee},
  title = {Multiple people tracking based on temporal color feature},
  journal = {International Journal of Pattern Recognition and Artificial Intelligence},
  year = {2003},
  volume = { 17},
  pages = {931 - 49},
  number = { 6},
  note = {temporal color feature;image sequences;time weighted color information;robust
	medium term people tracking;color value;color region;appearance frequency;occlusion
	problems;shape;image texture;people identification;appearance model;temporal
	color;time period;feature extraction;face recognition;},
  abstract = {We present a method for detecting and tracking multiple people totally
	occluded or out of sight in a scene for some period of time in image
	sequences. Our approach is to use time weighted color information
	(i.e. the temporal color) for robust medium-term people tracking.
	The temporal color is the set of pairs of a color value and its associated
	weights. The weight is related to the size, duration and frequency
	of appearance of the color region, as well as the number of people
	adjacent to the target person. It assures our system to continuously
	track people moving in a group with occlusion. Most systems have
	built an appearance model for each person to solve occlusion problems.
	The appearance model contains certain information on the target person-color,
	shape, texture, position, velocity and face pattern. We use temporal
	color in the appearance model for the identification of the people
	occluded or out of sight in the scene upon their reappearance. Experimental
	results show that the temporal color is more stable than shape or
	intensity in various cases},
  address = {Singapore},
  comment = {trk_color},
  copyright = {Copyright 2004, IEE},
  file = {:papers\\2003 JNL, Multiple people tracking based on temporal color feature (Kang, Hwang, Lee).pdf:PDF},
  issn = {0218-0014},
  keywords = {face recognition;feature extraction;image colour analysis;image sequences;optical
	tracking;},
  language = {English},
  timestamp = {00,010},
  url = {http://dx.doi.org/10.1142/S0218001403002708}
}

@INPROCEEDINGS{1988_JNL_Snakes_Kass,
  author = {Kass, M. and Witkin, A. and Terzopoulos, D.},
  title = {Snakes: active contour models},
  booktitle = {Proceedings of the First International Conference on Computer Vision},
  year = {1988},
  abstract = {A snake is an energy-minimizing spline guided by external constraint
	forces and influenced by image forces that pull it toward features
	such as lines and edges. Snakes are active contour models; they lock
	into nearby edges, localizing them accurately. Scale-space continuation
	can be used to enlarge the capture region surrounding a feature.
	Snakes provide a unified account of a number of visual problems,
	including detection of edges, lines, and subjective contours, motion
	tracking, and stereo matching. The authors have used snakes successfully
	for interactive interpretation, in which user-imposed constraint
	forces guide the snake near features of interest},
  comment = {trk_contour},
  file = {:papers\\1987 JNL, Snakes_ Active Contour Models (Kass, Witkin, Terzopoulos, IJCV, 9588).pdf:PDF},
  owner = {salman},
  timestamp = {10,000}
}

@BOOK{1993_BOOK_SSP_Kay,
  title = {Fundamentals of Statistical Signal Processing, Volume I: Estimation
	Theory (v. 1)},
  publisher = {Prentice Hall},
  year = {1993},
  author = {Kay, Steven M.},
  edition = {1},
  month = {April},
  abstract = {{This text is geared towards a one-semester graduate-level course
	in statistical signal processing and estimation theory. The author
	balances technical detail with practical and implementation issues,
	delivering an exposition that is both theoretically rigorous and
	application-oriented. The book covers topics such as minimum variance
	unbiased estimators, the Cramer-Rao bound, best linear unbiased estimators,
	maximum likelihood estimation, recursive least squares, Bayesian
	estimation techniques, and the Wiener and Kalman filters. The author
	provides numerous examples, which illustrate both theory and applications
	for problems such as high-resolution spectral analysis, system identification,
	digital filter design, adaptive beamforming and noise cancellation,
	and tracking and localization. The primary audience will be those
	involved in the design and implementation of optimal estimation algorithms
	on digital computers. The text assumes that you have a background
	in probability and random processes and linear and matrix algebra
	and exposure to basic signal processing. Students as well as researchers
	and practicing engineers will find the text an invaluable introduction
	and resource for scalar and vector parameter estimation theory and
	a convenient reference for the design of successive parameter estimation
	algorithms.} {<P><B></B> A unified presentation of parameter estimation
	for those involved in the design and implementation of statistical
	signal processing algorithms. <B></B> Covers important approaches
	to obtaining an optimal estimator and analyzing its performance;
	and includes numerous examples as well as applications to real- world
	problems. <B>MARKETS:</B> For practicing engineers and scientists
	who design and analyze signal processing systems, i.e., to extract
	information from noisy signals \&\#8212; radar engineer, sonar engineer,
	geophysicist, oceanographer, biomedical engineer, communications
	engineer, economist, statistician, physicist, etc. </P>}},
  comment = {PRML_book},
  day = {05},
  file = {:papers\\1993 BOOK, Fundamentals Of Statistical Signal Processing-Estimation Theory (Kay, Prentice Hall).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0133457117},
  posted-at = {2010-04-17 03:30:14},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0133457117}
}

@ARTICLE{11096636,
  author = {Ketchantang, W. and Derrode, S. and Martin, L. and Bourennane, S.},
  title = {Pearson-based mixture model for color object tracking},
  journal = {Machine Vision and Applications},
  year = {2008},
  volume = { 19},
  pages = {457 - 66},
  number = { 5-6},
  note = {Pearson-based mixture model;color object tracking;Gaussian mixture
	model;Kalman filtering;color video sequences;},
  abstract = {To track objects in video sequences, many studies have been done to
	characterize the target with respect to its color distribution. Most
	often, the Gaussian mixture model (GMM) is used to represent the
	object color density. In this paper, we propose to extend the normality
	assumption to more general families of distributions issued from
	the Pearson's system. Precisely, we propose a method called Pearson
	mixture model (PMM), used in conjunction with Gaussian copula, which
	is dynamically updated to adapt itself to the appearance change of
	the object during the sequence. This model is combined with Kalman
	filtering to predict the position of the object in the next frame.
	Experimental results on gray-level and color video sequences show
	tracking improvements compared to classical GMM. Especially, the
	PMM seems robust to illumination variations, pose and scale changes,
	and also to partial occlusions, but its computing time is higher
	than the computing time of GMM.},
  address = {Germany},
  comment = {trk_color},
  copyright = {Copyright 2010, The Institution of Engineering and Technology},
  file = {:papers\\2008 JNL, Pearson-based mixture model for color object tracking (Ketchantang, Derrode, Martin, Bourennane).pdf:PDF},
  issn = {0932-8092},
  keywords = {Gaussian processes;image colour analysis;image sequences;Kalman filters;target
	tracking;video signal processing;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1007/s00138-008-0124-4}
}

@INPROCEEDINGS{1999_CNF_BayesianMulticamera_KettnakerZabih,
  author = {Kettnaker, V. and Zabih, R.},
  title = {Bayesian multi-camera surveillance},
  booktitle = {Computer Vision and Pattern Recognition, 1999. IEEE Computer Society
	Conference on.},
  year = {1999},
  volume = {2},
  abstract = {The task of multicamera surveillance is to reconstruct the paths taken
	by all moving objects that are temporally visible from multiple non-overlapping
	cameras. We present a Bayesian formalization of this task, where
	the optimal solution is the set of object paths with the highest
	posterior probability given the observed data. We show how to efficiently
	approximate the maximum a posteriori solution by linear programming
	and present initial experimental results},
  comment = {trk_people_multiCamera},
  file = {:papers\\1999 CNF, Bayesian Multi-camera Surveillance (Kettnaker, Ramin Zabih, CVPR, 164).pdf:PDF},
  keywords = {Bayes methods motion estimation surveillance Bayesian formalization
	linear programming moving objects multi-camera surveillance reconstruct},
  owner = {salman},
  timestamp = {00,150}
}

@ARTICLE{2003_JNL_ConsistentLabelingTrackingMultipleCameras_KhanMubarakshah,
  author = {Khan, S. and Shah, M.},
  title = {Consistent labeling of tracked objects in multiple cameras with overlapping
	fields of view},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2003},
  volume = {25},
  pages = {1355-1360},
  number = {10},
  abstract = {We address the issue of tracking moving objects in an environment
	covered by multiple uncalibrated cameras with overlapping fields
	of view, typical of most surveillance setups. In such a scenario,
	it is essential to establish correspondence between tracks of the
	same object, seen in different cameras, to recover complete information
	about the object. We call this the problem of consistent labeling
	of objects when seen in multiple cameras. We employ a novel approach
	of finding the limits of field of view (FOV) of each camera as visible
	in the other cameras. We show that, if the FOV lines are known, it
	is possible to disambiguate between multiple possibilities for correspondence.
	We present a method to automatically recover these lines by observing
	motion in the environment, Furthermore, once these lines are initialized,
	the homography between the views can also be recovered. We present
	results on indoor and outdoor sequences containing persons and vehicles.},
  comment = {trk_people_multiCamera},
  file = {:papers\\2003 JNL, Consistent labeling  of tracked objects in multiple cameras with overlapping fields of view  (Sohaib Khan, Mubarak Shah, PAMI, 159).pdf:PDF},
  keywords = {cameras motion estimation optical tracking sensor fusion FOV lines
	camera handoff complete information recovery consistent labeling
	consistent object labeling field of view homography indoor sequences
	moving object tracking multiperspective video multiple cameras multiple
	uncalibrated cameras outdoor sequences overlapping fields of view
	surveillance setups tracked objects},
  owner = {salman},
  timestamp = {00,160}
}

@ARTICLE{2005_JNL_CodebookFGBG_Kim,
  author = {Kim, K. and Chalidabhongse, T.H. and Harwood, D. and Davis, L.},
  title = {Real-time foreground-background segmentation using codebook model},
  journal = {Real-time imaging},
  year = {2005},
  volume = {11},
  pages = {172--185},
  number = {3},
  comment = {TRK_codebook},
  file = {:papers\\2005 JNL, Real-time foregroundbackground segmentation (Kim).pdf:PDF},
  issn = {1077-2014},
  publisher = {Elsevier},
  timestamp = {00,240}
}

@ARTICLE{2001_CNF_SnakeTracking_Kim,
  author = {Won Kim and Ju-Jang Lee},
  title = {Visual tracking using Snake based on target's contour information
	},
  journal = {Industrial Electronics, 2001. Proceedings. ISIE 2001. IEEE International
	Symposium on},
  year = {2001},
  volume = {1},
  pages = {176-180 vol.1},
  abstract = {An active contour model, Snake, was developed as a useful segmenting
	and tracking tool for rigid or non-rigid (i.e. deformable) objects
	by Kass (1987). Snake is designed on the basis of Snake energies.
	Segmenting and tracking can be executed successfully by the process
	of energy minimization. Kass' Snake can be applied to the case of
	small changes between images because its solutions can be achieved
	on the basis of a variational approach. If a somewhat fast moving
	object exists in successive images, Kass' Snake will not operate
	well because the moving object may have large differences in its
	position or form between successive images. Snake's nodes may fall
	into the local minima in their motion to the new positions of the
	target object in next image. When the motion is too large to apply
	image flow energy to tracking, a jump mode is proposed for solving
	the problem. The vector used to make Snake's nodes jump to the new
	location can be obtained by processing the image flow. The effectiveness
	of the proposed Snake is confirmed by some simulations},
  doi = {10.1109/ISIE.2001.931777},
  keywords = {edge detection, image motion analysis, image sequences, minimisation,
	target tracking, variational techniquesSnake energies, active contour
	model, energy minimization, fast moving object, image flow energy,
	jump mode, segmentation, snake, successive images, target contour
	information, variational approach, visual tracking}
}

@ARTICLE{1987_JNL_NonGaussianSSmodelingNonStationaryTimeSeries_Kitagawa,
  author = {Kitagawa, Genshiro},
  title = {Non-Gaussian State-Space Modeling of Nonstationary Time Series},
  journal = {Journal of the American Statistical Association},
  year = {1987},
  volume = {82},
  pages = {1032--1041},
  number = {400},
  comment = {trk_filtering},
  copyright = {Copyright  1987 American Statistical Association},
  file = {:papers\\1987 JNL, Non-Gaussian state-space modeling of nonstationary time series (Kitagawa, JASA, 623).pdf:PDF},
  issn = {01621459},
  jstor_articletype = {primary_article},
  jstor_formatteddate = {Dec., 1987},
  publisher = {American Statistical Association},
  timestamp = {00,700},
  url = {http://www.jstor.org/stable/2289375}
}

@INPROCEEDINGS{2010_CNF_TRK_Kjellstro,
  author = {Kjellstro andm, H. and Kragic and, D. and Black, M.J.},
  title = {Tracking people interacting with objects},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference
	on},
  year = {2010},
  pages = {747 -754},
  month = june,
  abstract = {While the problem of tracking 3D human motion has been widely studied,
	most approaches have assumed that the person is isolated and not
	interacting with the environment. Environmental constraints, however,
	can greatly constrain and simplify the tracking problem. The most
	studied constraints involve gravity and contact with the ground plane.
	We go further to consider interaction with objects in the environment.
	In many cases, tracking rigid environmental objects is simpler than
	tracking high-dimensional human motion. When a human is in contact
	with objects in the world, their poses constrain the pose of body,
	essentially removing degrees of freedom. Thus what would appear to
	be a harder problem, combining object and human tracking, is actually
	simpler. We use a standard formulation of the body tracking problem
	but add an explicit model of contact with objects. We find that constraints
	from the world make it possible to track complex articulated human
	motion in 3D from a monocular camera.},
  comment = {TRK},
  doi = {10.1109/CVPR.2010.5540140},
  file = {:papers\\2010 JNL, Coupled Prediction Classification for Robust Visual Tracking (Patras).pdf:PDF},
  issn = {1063-6919},
  keywords = {3D human motion tracking;complex articulated human motion;degrees
	of freedom;environmental constraints;human tracking;monocular camera;object
	interaction;people tracking;rigid environmental object tracking;cameras;image
	motion analysis;object detection;optical tracking;pose estimation;},
  timestamp = {-}
}

@INPROCEEDINGS{2006_CNF_2D3D_Knorr,
  author = {Knorr, S. and Imre, E. and Ozkalayci, B. and Alatan, A.A. and Sikora,
	T.},
  title = {A Modular Scheme for 2D/3D Conversion of TV Broadcast},
  booktitle = {3D Data Processing, Visualization, and Transmission, Third International
	Symposium on},
  year = {2006},
  abstract = {The 3D reconstruction from 2D broadcast video is a challenging problem
	with many potential applications, such as 3DTV, free-viewpoint video
	or augmented reality. In this paper, a modular system capable of
	efficiently reconstructing 3D scenes from broadcast video is proposed.
	The system consists of four constitutive modules: tracking and segmentation,
	self-calibration, sparse reconstruction and, finally, dense reconstruction.
	This paper also introduces some novel approaches for moving object
	segmentation and sparse and dense reconstruction problems. According
	to the simulations for both synthetic and real data, the system achieves
	a promising performance for typical TV content, indicating that it
	is a significant step towards the 3D reconstruction of scenes from
	broadcast video.},
  comment = {3D_stereo},
  doi = {10.1109/3DPVT.2006.15},
  file = {:papers\\2006 CNF, A Modular Scheme for 2D_3D Conversion of TV Broadcast (Knorr, Imre, Ozkalayci, Alatan, Sikora).pdf:PDF},
  keywords = {2D broadcast video;3D scene reconstruction;TV broadcast;augmented
	reality;dense reconstruction;image segmentation;moving object segmentation;sparse
	reconstruction;image reconstruction;image segmentation;television
	broadcasting;},
  timestamp = {-}
}

@INPROCEEDINGS{2008_CNF_SURVEY_beh_Ko,
  author = {Ko, T.},
  title = {A survey on behavior analysis in video surveillance for homeland
	security applications},
  booktitle = {Applied Imagery Pattern Recognition Workshop, 2008. AIPR '08. 37th
	IEEE},
  year = {2008},
  pages = {1 -8},
  month = oct.,
  abstract = {Surveillance cameras are inexpensive and everywhere these days but
	the manpower required to monitor and analyze them is expensive. Consequently
	the videos from these cameras are usually monitored sparingly or
	not at all; they are often used merely as archive, to refer back
	to once an incident is known to have taken place. Surveillance cameras
	can be a far more useful tool if instead of passively recording footage,
	they can be used to detect events requiring attention as they happen,
	and take action in real time. This is the goal of automated visual
	surveillance: to obtain a description of what is happening in a monitored
	area, and then to take appropriate action based on that interpretation.
	Video surveillance for humans is one of the most active research
	topics in computer vision. It has a wide spectrum of promising homeland
	security applications. Video management and interpretation systems
	have become quite capable in recent years. This paper looks into
	how hardware and software can be put together to solve surveillance
	problems in an age of increased concern with public safety and security.
	In general, the framework of a video surveillance system includes
	the following stages: modeling of environments, detection of motion,
	classification of moving objects, tracking, behavior understanding
	and description, and fusion of information from multiple cameras.
	Despite recent progress in computer vision and other related areas,
	there are still major technical challenges to be overcome before
	reliable automated video surveillance can be realized. This paper
	reviews developments and general strategies of stages involved in
	video surveillance, and analyzes the feasibility and challenges for
	combining motion analysis, behavior analysis, and standoff biometrics
	for identification of known suspects, anomaly detection, and behavior
	understanding.},
  comment = {SURVEY_beh},
  doi = {10.1109/AIPR.2008.4906450},
  file = {:C\:\\salman\\work\\writing\\papers\\2008 CNF, A Survey on Behavior Analysis in Video Surveillance (Ko).pdf:PDF},
  issn = {1550-5219},
  keywords = {anomaly detection;behavior analysis;computer vision;homeland security
	application;motion analysis;standoff biometrics;surveillance camera;video
	surveillance;computer vision;image motion analysis;national security;video
	cameras;video surveillance;},
  timestamp = {?}
}

@INPROCEEDINGS{2003_CNF_TransductionMultiviewLearningEmails_Kockelkorn,
  author = {Kockelkorn, Michael and Luneburg, Andreas and Scheffer, Tobias},
  title = {Using transduction and multi-view learning to answer emails},
  booktitle = {7th European Conference on Principles and Practice of Knowledge Discovery
	in Databases, September 22, 2003 - September 26, 2003},
  year = {2003},
  abstract = {Many organizations and companies have to answer large amounts of emails.
	Often, most of these emails contain variations of relatively few
	frequently asked questions. We address the problem of predicting
	which of several frequently used answers a user will choose to respond
	to an email. Our approach effectively utilizes the data that is typically
	available in this setting: inbound and outbound emails stored on
	a server. We take into account that there are no explicit links between
	inbound and corresponding outbound mails on the server. We map the
	problem to a semi-supervised classification problem that can be addressed
	by algorithms such as the transductive support vector machine and
	multi-view learning. We evaluate our approach using emails sent to
	a corporate customer service department.},
  comment = {PRML},
  file = {:papers\\2003 CNF, Using Transduction and Multi-view Learning (Kockelkorn, Luneburg, Scheffer, PPKDD, 17).pdf:PDF},
  keywords = {Learning systems Algorithms Automation Communication systems Electronic
	mail Problem solving Societies and institutions},
  timestamp = {-}
}

@BOOK{1995_BOOK_LP_Kolman,
  title = {Elementary Linear Programming with Applications, Second Edition (Computer
	Science and Scientific Computing)},
  publisher = {Academic Press},
  year = {1995},
  author = {Kolman, Bernard and Beck, Robert E.},
  edition = {2},
  month = {July},
  citeulike-article-id = {7095106},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20&amp;path=ASIN/012417910X},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21&amp;path=ASIN/012417910X},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21&amp;path=ASIN/012417910X},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/012417910X},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/012417910X/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20&path=ASIN/012417910X},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/012417910X},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN012417910X},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=012417910X&index=books&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/012417910X},
  day = {06},
  file = {:papers\\1995 BOOK, Elementary Linear Programming with Applications.pdf:PDF},
  howpublished = {Hardcover},
  isbn = {012417910X},
  posted-at = {2010-08-31 00:32:36},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20&path=ASIN/012417910X}
}

@ARTICLE{2003_JNL_ActiveShapeTracking_Koschan,
  author = {Andreas Koschan and Sangkyu Kang and Joonki Paik and Besma Abidi
	and Mongi Abidi},
  title = {Color active shape models for tracking non-rigid objects},
  journal = {Pattern Recognition Letters},
  year = {2003},
  volume = {24},
  pages = {1751 - 1765},
  number = {11},
  abstract = {Active shape models can be applied to tracking non-rigid objects in
	video image sequences. Traditionally these models do not include
	color information in their formulation. In this paper, we present
	a hierarchical realization of an enhanced active shape model for
	color video tracking and we study the performance of both hierarchical
	and non-hierarchical implementations in the RGB, YUV, and HSI color
	spaces.},
  comment = {trk},
  doi = {DOI: 10.1016/S0167-8655(02)00330-6},
  file = {:papers\\2003 JNL, Color active shape models for tracking non-rigid objects (Koschan, Kang, Paik, Abidi, Abidi).pdf:PDF},
  issn = {0167-8655},
  keywords = {Video tracking},
  timestamp = {?},
  url = {http://www.sciencedirect.com/science/article/B6V15-47JCTJG-B/2/5640e10e926ec8639a1cfb3b56c82575}
}

@ARTICLE{1995_JNL_OptimalityRVQ_Kossentini,
  author = {Kossentini, F. and Smith, M.J.T. and Barnes, C.F.},
  title = {Necessary conditions for the optimality of variable-rate residual
	vector quantizers},
  journal = {IEEE Transactions on Information Theory},
  year = {1995},
  volume = {41},
  pages = {1903 -1914},
  abstract = {Necessary conditions for the optimality of variable-rate residual
	vector quantizers are derived, and an iterative descent algorithm
	based on a Lagrangian formulation is introduced for designing residual
	vector quantizers having minimum average distortion subject to an
	entropy constraint. Simulation results for entropy-constrained residual
	vector quantizers are presented for memoryless Gaussian, Laplacian,
	and uniform sources. A Gauss-Markov source is also considered. The
	rate-distortion performance is shown to be competitive with that
	of entropy-constrained vector quantization and entropy-constrained
	trellis-coded quantization },
  comment = {VQ_RVQ},
  doi = {10.1109/18.476315},
  file = {:papers\\1995 JNL, Necessary conditions for the optimality of variable-rate residual vector quantizers (Kossentini, Smith, Barnes).pdf:PDF},
  issn = {0018-9448},
  keywords = {Gauss-Markov source;Lagrangian formulation;Laplacian sources;entropy
	constraint;iterative descent algorithm;memoryless Gaussian sources;minimum
	average distortion;necessary conditions;optimality;rate-distortion
	performance;simulation;source coding;uniform sources;variable-rate
	residual vector quantizers;Gaussian processes;Markov processes;entropy;iterative
	methods;optimisation;rate distortion theory;source coding;vector
	quantisation;},
  timestamp = {00,030}
}

@INPROCEEDINGS{1992_CNF_ImageCodingRVQ_Kossentini,
  author = {Kossentini, F. and Smith, M.J.T. and Barnes, C.F.},
  title = {Image coding with variable rate RVQ},
  booktitle = {Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., IEEE
	International Conference on},
  year = {1992},
  abstract = {A new residual vector quantizer (RVQ) design algorithm is modified
	so that the multistage structure can be exploited to produce variable-rate
	RVQ (VR-RVQ) systems. VR-RVQ systems are shown to have very useful
	properties: (1) the codebook storage requirement and the search complexity
	are both reduced; (2) the VR-RVQ system is able to exploit the spatial
	variance of perceptually important information; and (3) the VR-RVQ
	codebook can operate over a wide range of rates, without having to
	store several codebooks. Experiments were performed using VR-RVQ
	systems with vectors of many sizes, and results show significant
	improvement over fixed-rate RVQ systems with the same block size},
  comment = {VQ_RVQ},
  doi = {10.1109/ICASSP.1992.226224},
  file = {:papers\\1992 CNF, Image coding with variable rate RVQ (Kossentini, Smith, Barnes).pdf:PDF},
  issn = {1520-6149},
  keywords = {VR-RVQ systems;codebook storage requirement;image coding;multistage
	structure;perceptually important information;search complexity;spatial
	variance;variable rate residual vector quantisation;image coding;vector
	quantisation;},
  timestamp = {-}
}

@INPROCEEDINGS{2000_CNF_Comm_Kozick,
  author = {Kozick, R.J. and Sadler, B.M. and Kosinski, J.},
  title = {Limits on classifying pulsed signals},
  booktitle = {MILCOM 2000. 21st Century Military Communications Conference Proceedings},
  year = {2000},
  volume = {2},
  pages = {962 -966 vol.2},
  abstract = {Fundamental bounds are presented for classifying pulsed signals observed
	in additive Gaussian noise. The bounds specify the maximum number
	of signals that can be reliably classified as a function of the signal
	to noise ratio (SNR) and the dimension of the subspace containing
	the signals. The bounds are based on a ldquo;sphere packing rdquo;
	arrangement for the signals that maximizes the probability of correct
	classification. The bounds are easy to compute and provide a reference
	to facilitate comparisons between classification algorithms. An example
	is presented in which the signals consist of RF pulses with different
	center frequencies, and the sphere packing bounds are compared with
	the performance of the maximum likelihood classifier},
  doi = {10.1109/MILCOM.2000.904073},
  file = {:papers\\2000 CNF, Limits on classifying pulsed signals.pdf:PDF},
  keywords = {AWGN;RF pulses;SNR;additive white Gaussian noise;center frequencies;classification
	algorithms;correct classification probability;error probability;maximum
	likelihood classifier;pulsed signals classification;signal to noise
	ratio;sphere packing bounds;subspace dimension;AWGN;error statistics;signal
	classification;}
}

@INPROCEEDINGS{2009_CNF_AnomalyDetection_Kratz,
  author = {Kratz, L. and Nishino, K.},
  title = {Anomaly detection in extremely crowded scenes using spatio-temporal
	motion pattern models},
  booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference
	on},
  year = {2009},
  abstract = {Extremely crowded scenes present unique challenges to video analysis
	that cannot be addressed with conventional approaches. We present
	a novel statistical framework for modeling the local spatio-temporal
	motion pattern behavior of extremely crowded scenes. Our key insight
	is to exploit the dense activity of the crowded scene by modeling
	the rich motion patterns in local areas, effectively capturing the
	underlying intrinsic structure they form in the video. In other words,
	we model the motion variation of local space-time volumes and their
	spatial-temporal statistical behaviors to characterize the overall
	behavior of the scene. We demonstrate that by capturing the steady-state
	motion behavior with these spatio-temporal motion pattern models,
	we can naturally detect unusual activity as statistical deviations.
	Our experiments show that local spatio-temporal motion pattern modeling
	offers promising results in real-world scenes with complex activities
	that are hard for even human observers to analyze.},
  comment = {trk_crowd},
  file = {:papers\\2009 CNF, Anomaly Detection in Extremely Crowded Scenes (Kratz, Ko Nishino Drexel, CVPR).pdf:PDF},
  keywords = {human factors image motion analysis image recognition video signal
	processing anomaly detection extremely crowded scenes local space-time
	volumes local spatio-temporal motion pattern behavior motion variation
	rich motion patterns spatial-temporal statistical behaviors spatio-temporal
	motion pattern models steady-state motion behavior video analysis},
  owner = {salman},
  timestamp = {-}
}

@BOOK{2006_BOOK_AdvEnggMath_Kreyszig,
  title = {Advanced Engineering Mathematics 9th edition},
  publisher = {John Wiley\&SonsInc,,2006 9th edition},
  year = {2006},
  author = {Erwin Kreyszig},
  month = {January},
  comment = {math_book},
  day = {01},
  file = {:papers\\2006 BOOK, Advanced engineering mathematics (Erwin Kreyszig, Wiley 9ed).pdf:PDF},
  howpublished = {Paperback},
  posted-at = {2010-04-23 01:00:33},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/B0038DD9OU}
}

@INPROCEEDINGS{2000_CNF_EasyLiving_Krumm,
  author = {Krumm, J. and Harris, S. and Meyers, B. and Brumitt, B. and Hale,
	M. and Shafer, S.},
  title = {Multi-camera multi-person tracking for EasyLiving},
  booktitle = {Visual Surveillance, 2000. Proceedings. Third IEEE International
	Workshop on},
  year = {2000},
  abstract = {While intelligent environments are often cited as a reason for doing
	work on visual person-tracking, really making an intelligent environment
	exposes many real-world problems in visual tracking that must be
	solved to make the technology practical. In the context of our EasyLiving
	project in intelligent environments, we created a practical person-tracking
	system that solves most of the real-world problems. It uses two sets
	of color stereo cameras for tracking multiple people during live
	demonstrations in a living room. The stereo images are used for locating
	people, and the color images are used for maintaining their identities.
	The system runs quickly enough to make the room feel responsive,
	and it tracks multiple people standing, walking, sitting, occluding,
	and entering and leaving the space},
  comment = {trk_people_multiCamera},
  doi = {10.1109/VS.2000.856852},
  file = {:papers\\2000 CNF, Multi-camera multi-person tracking for EasyLiving (Krumm et al., VS, 432).pdf:PDF},
  keywords = {EasyLiving project;color images;color stereo cameras;computer vision;intelligent
	environments;live demonstrations;living room;multi-camera multi-person
	tracking;people location;software architecture;stereo images;visual
	person-tracking;cameras;computer vision;image colour analysis;software
	architecture;stereo image processing;tracking;video cameras;video
	signal processing;},
  timestamp = {00,400}
}

@ARTICLE{2000_JNL_EasyLiv_Krumm,
  author = {Krumm, J. and Harris, S. and Meyers, B. and Brumitt, B. and Hale,
	M. and Shafer, S.},
  title = {Multi-camera multi-person tracking for easyliving},
  journal = {vs},
  year = {2000},
  pages = {3},
  publisher = {Published by the IEEE Computer Society}
}

@ARTICLE{1982_JNL_EllipticalFourier_Kuhl,
  author = {Frank P. Kuhl and Charles R. Giardina},
  title = {Elliptic Fourier features of a closed contour},
  journal = {Computer Graphics and Image Processing},
  year = {1982},
  volume = {18},
  pages = {236 - 258},
  number = {3},
  abstract = {A direct procedure for obtaining the Fourier coefficients of a chain-encoded
	contour is presented. Advantages of the procedure are that it does
	not require integration or the use of fast Fourier transform techniques,
	and that bounds on the accuracy of the image contour reconstruction
	are easy to specify. Elliptic properties of the Fourier coefficients
	are shown and used for a convenient and intuitively pleasing procedure
	of normalizing a Fourier contour representation. Extension of the
	contour representation to arbitrary objects at arbitrary aspect angle
	is discussed. The procedures have direct application to a variety
	of pattern recognition problems that involve analysis of well-defined
	image contours.},
  comment = {trk_contour},
  file = {:papers\\1982 JNL, Elliptic Fourier features of a closed contour (Kuhl, Giardina).pdf:PDF},
  issn = {0146-664X},
  timestamp = {00,500}
}

@ARTICLE{1955_JNL_HungarianMethod_Kuhn,
  author = {Kuhn, H. W.},
  title = {The Hungarian method for the assignment problem},
  journal = {Naval Research Logistics Quart. 2},
  year = {1955},
  volume = {Quart 2},
  pages = {83-97},
  abstract = {Assuming that numerical scores are available for the performance of
	each of n persons on each of n jobs, the "assignment problem" is
	the quest for an assignment of persons to jobs so that the sum of
	the n scores so obtained is as large as possible. It is shown that
	ideas latent in the work of two Hungarian mathematicians may be exploited
	to yield a new method of solving this problem.},
  comment = {trk_correspondence},
  file = {:papers\\1955 JNL, The_Hungarian_method for the assignment problem (Kuhn, NRL, 1754).pdf:PDF},
  keywords = {Traveling salesman problem Algorithms Computational methods Linear
	programming Matrix algebra Optimization Problem solving Theorem proving},
  timestamp = {02,000}
}

@INPROCEEDINGS{2003_CNF_DRF_SanjivHebert,
  author = {Kumar, Sanjiv and Hebert, M.},
  title = {Discriminative random fields: a discriminative framework for contextual
	interaction in classification},
  booktitle = {ICCV 2003: 9th International Conference on Computer Vision, 13-16
	Oct. 2003},
  year = {2003},
  volume = {vol.2},
  abstract = {In this work we present discriminative random fields (DRFs), a discriminative
	framework for the classification of image regions by incorporating
	neighborhood interactions in the labels as well as the observed data.
	The discriminative random fields offer several advantages over the
	conventional Markov random field (MRF) framework. First, the DRFs
	allow to relax the strong assumption of conditional independence
	of the observed data generally used in the MRF framework for tractability.
	This assumption is too restrictive for a large number of applications
	in vision. Second, the DRFs derive their classification power by
	exploiting the probabilistic discriminative models instead of the
	generative models used in the MRF framework. Finally, all the parameters
	in the DRF model are estimated simultaneously from the training data
	unlike the MRF framework where likelihood parameters are usually
	learned separately from the field parameters. We illustrate the advantages
	of the DRFs over the MRF framework in an application of man-made
	structure detection in natural images taken from the Corel database},
  comment = {?},
  file = {:papers\\2003 CNF, Discriminative random fields_ a discriminative framework for contextual interaction in classification (Sanjiv, Hebert, ICCV, 219).pdf:PDF},
  keywords = {computer vision image classification image segmentation Markov processes
	random processes},
  owner = {salman},
  timestamp = {00,200}
}

@ARTICLE{2005_JNL_SpaceTimeInterestPoints_Laptev,
  author = {Laptev, Ivan},
  title = {On space-time interest points},
  journal = {International Journal of Computer Vision},
  year = {2005},
  volume = {64},
  pages = {107-123},
  abstract = {Local image features or interest points provide compact and abstract
	representations of patterns in an image. In this paper, we extend
	the notion of spatial interest points into the spatio-temporal domain
	and show how the resulting features often reflect interesting events
	that can be used for a compact representation of video data as well
	as for interpretation of spatio-temporal events. To detect spatio-temporal
	events, we build on the idea of the Harris and Forstner interest
	point operators and detect local structures in space-time where the
	image values have significant local variations in both space and
	time. We estimate the spatio-temporal extents of the detected events
	by maximizing a normalized spatio-temporal Laplacian operator over
	spatial and temporal scales. To represent the detected events, we
	then compute local, spatio-temporal, scale-invariant N-jets and classify
	each event with respect to its jet descriptor. For the problem of
	human motion analysis, we illustrate how a video representation in
	terms of local space-time features allows for detection of walking
	people in scenes with occlusions and dynamic cluttered backgrounds.
	2005 Springer Science + Business Media, Inc.},
  comment = {recog_action_interestPoint},
  file = {:papers\\2005 JNL, On space-time interest points (Laptev, IJCV, 298).pdf:PDF},
  keywords = {Computer vision Feature extraction Image analysis Motion compensation
	Pattern recognition Problem solving Video signal processing},
  owner = {salman},
  timestamp = {00,300}
}

@PHDTHESIS{1980_PhD_TexturedSegmentation_Laws,
  author = {Laws},
  title = {Textured image segmentation},
  school = {Electrical Engineering, University of Southern California},
  year = {1980},
  comment = {seg},
  owner = {salman},
  timestamp = {00,500}
}

@ARTICLE{9599716,
  author = {Hyung-Soo Lee and Daijin Kim},
  title = {Robust face tracking by integration of two separate trackers: skin
	color and facial shape},
  journal = {Pattern Recognition},
  year = {2007},
  volume = { 40},
  pages = {3225 - 35},
  number = { 11},
  note = {face tracking;facial shape;skin color density;skin color tracker;illumination
	change;},
  abstract = {This paper proposes a robust face tracking method based on the condensation
	algorithm that uses skin color and facial shape as observation measures.
	Two trackers are used for robust tracking: one tracks the skin color
	regions and the other tracks the facial shape regions. The two trackers
	are coupled using an importance sampling technique, where the skin
	color density obtained from the skin color tracker is used as the
	importance function to generate samples for the shape tracker. The
	samples of the skin color tracker within the chosen shape region
	are updated with higher weights. Also, an adaptive color model is
	used to avoid the effect of illumination change in the skin color
	tracker. The proposed face tracker performs more robustly than either
	the skin-color-based tracker or the facial shape-based tracker, given
	the presence of background clutter and/or illumination changes. [All
	rights reserved Elsevier].},
  address = {UK},
  comment = {trk_color},
  copyright = {Copyright 2007, The Institution of Engineering and Technology},
  file = {:papers\\2007 JNL, Robust face tracking by integration of two separate trackers_ skin color and facial shape (Lee, Kim).pdf:PDF},
  issn = {0031-3203},
  keywords = {face recognition;image colour analysis;},
  language = {English},
  timestamp = {00,010},
  url = {http://dx.doi.org/10.1016/j.patcog.2007.03.003}
}

@ARTICLE{2005_JNL_manifoldTRK_Lee,
  author = {Kuang-Chih Lee and Jeffrey Ho and Ming-Hsuan Yang and David Kriegman},
  title = {Visual tracking and recognition using probabilistic appearance manifolds},
  journal = {Computer Vision and Image Understanding},
  year = {2005},
  volume = {99},
  pages = {303 - 331},
  number = {3},
  abstract = {This paper presents an algorithm for modeling, tracking, and recognizing
	human faces in video sequences within one integrated framework. Conventional
	video-based face recognition systems have usually been embodied with
	two independent components: the tracking and recognition modules.
	In contrast, our algorithm emphasizes an algorithmic architecture
	that tightly couples these two components within a single framework.
	This is accomplished through a novel appearance model which is utilized
	simultaneously by both modules, even with their disparate requirements
	and functions. The complex nonlinear appearance manifold of each
	registered person is partitioned into a collection of submanifolds
	where each models the face appearances of the person in nearby poses.
	The submanifold is approximated by a low-dimensional linear subspace
	computed by principal component analysis using images sampled from
	training video sequences. The connectivity between the submanifolds
	is modeled as transition probabilities between pairs of submanifolds,
	and these are learned directly from training video sequences. The
	integrated task of tracking and recognition is formulated as a maximum
	a posteriori estimation problem. Within our framework, the tracking
	and recognition modules are complementary to each other, and the
	capability and performance of one are enhanced by the other. Our
	approach contrasts sharply with more rigid conventional approaches
	in which these two modules work independently and in sequence. We
	report on a number of experiments and results that demonstrate the
	robustness, effectiveness, and stability of our algorithm.},
  comment = {TRK_subspace},
  file = {:papers\\2005 JNL, Visual tracking and recognition using probabilistic appearance manifolds (Lee).pdf:PDF},
  issn = {1077-3142},
  keywords = {Face tracking},
  timestamp = {00,060}
}

@CONFERENCE{2002_CNF_gait_Lee,
  author = {Lee, L. and Grimson, WEL},
  title = {Gait analysis for recognition and classification},
  booktitle = {Automatic Face and Gesture Recognition, 2002. Proceedings. Fifth
	IEEE International Conference on},
  year = {2002},
  pages = {148--155},
  organization = {IEEE},
  comment = {TRK_gait},
  file = {:papers\\2003 CNF, Gait analysis for recognition and classification (Lee).pdf:PDF},
  isbn = {0769516025},
  timestamp = {00,300}
}

@ARTICLE{2000_JNL_MonitoringActivitiesMultipleStreams_Lee,
  author = {Lee, L. and Romano, R. and Stein, G.},
  title = {Monitoring activities from multiple video streams: establishing a
	common coordinate frame},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {758-767},
  number = {8},
  abstract = {Monitoring of large sites requires coordination between multiple cameras,
	which in turn requires methods for relating events between distributed
	cameras. This paper tackles the problem of automatic external calibration
	of multiple cameras in an extended scene, that is, full recovery
	of their 3D relative positions and orientations. Because the cameras
	are placed far apart, brightness or proximity constraints cannot
	be used to match static features, so we instead apply planar geometric
	constraints to moving objects tracked throughout the scene. By robustly
	matching and fitting tracked objects to a planar model, we align
	the scene's ground plane across multiple views and decompose the
	planar alignment matrix to recover the 3D relative camera and ground
	plane positions. We demonstrate this technique in both a controlled
	lab setting where we test the effects of errors in the intrinsic
	camera parameters, and in an uncontrolled, outdoor setting. In the
	latter, we do not assume synchronized cameras and we show that enforcing
	geometric constraints enables us to align the tracking data in time.
	In spite of noise in the intrinsic camera parameters and in the image
	data, the system successfully transforms multiple views of the scene's
	ground plane to an overhead view and recovers the relative 3D camera
	and ground plane positions},
  comment = {trk_cars_multiCamera},
  file = {:papers\\2000 JNL, Monitoring Activities from Multiple Video Streams_ Establishing a Common Coordinate Frame (Lee, Romano, Stein, PAMI, 224).pdf:PDF},
  keywords = {computational geometry computer vision computerised monitoring image
	matching matrix algebra sensor fusion tracking 3D relative orientation
	recovery 3D relative position recovery activity monitoring automatic
	external calibration brightness constraints common coordinate frame
	distributed cameras multiple camera coordination multiple video streams
	planar alignment matrix planar geometric constraints proximity constraints
	robust fitting robust matching tracked objects tracking data alignment
	uncontrolled outdoor setting},
  owner = {salman},
  timestamp = {00,250}
}

@ARTICLE{2009_JNL_PoseTracking_Lee,
  author = {Lee, Mun Wai and Nevatia, R.},
  title = {Human Pose Tracking in Monocular Sequence Using Multilevel Structured
	Models},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2009},
  volume = {31},
  pages = {27 -38},
  number = {1},
  month = {jan. },
  abstract = {Tracking human body poses in monocular video has many important applications.
	The problem is challenging in realistic scenes due to background
	clutter, variation in human appearance and self-occlusion. The complexity
	of pose tracking is further increased when there are multiple people
	whose bodies may inter-occlude. We proposed a three-stage approach
	with multi-level state representation that enables a hierarchical
	estimation of 3D body poses. Our method addresses various issues
	including automatic initialization, data association, self and inter-occlusion.
	At the first stage, humans are tracked as foreground blobs and their
	positions and sizes are coarsely estimated. In the second stage,
	parts such as face, shoulders and limbs are detected using various
	cues and the results are combined by a grid-based belief propagation
	algorithm to infer 2D joint positions. The derived belief maps are
	used as proposal functions in the third stage to infer the 3D pose
	using data-driven Markov chain Monte Carlo. Experimental results
	on several realistic indoor video sequences show that the method
	is able to track multiple persons during complex movement including
	sitting and turning movements with self and inter-occlusion.},
  comment = {trk_people},
  doi = {10.1109/TPAMI.2008.35},
  file = {:papers\\2009 JNL, Human Pose Tracking in Monocular Sequence Using Multilevel Structured Mode (Lee, Nevatia, PAMI, 6)ls.pdf:PDF},
  issn = {0162-8828},
  keywords = {2D joint positions;3D body poses;automatic initialization;belief maps;data
	association;data-driven Markov chain Monte Carlo;grid-based belief
	propagation algorithm;human pose tracking;monocular sequence;monocular
	video;multilevel state representation;multilevel structured models;realistic
	indoor video sequences;realistic scenes;Markov processes;Monte Carlo
	methods;image representation;image sequences;pose estimation;sensor
	fusion;tracking;video signal processing;Computer Simulation;Humans;Image
	Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Models,
	Biological;Posture;Video Recording;Whole Body Imaging;},
  timestamp = {-}
}

@INPROCEEDINGS{2005_CNF_PedestrianDet_Leibe,
  author = {Leibe, B. and Seemann, E. and Schiele, B.},
  title = {Pedestrian detection in crowded scenes},
  booktitle = {Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
	Society Conference on},
  year = {2005},
  volume = {1},
  pages = { 878 - 885 vol. 1},
  month = {june},
  abstract = { In this paper, we address the problem of detecting pedestrians in
	crowded real-world scenes with severe overlaps. Our basic premise
	is that this problem is too difficult for any type of model or feature
	alone. Instead, we present an algorithm that integrates evidence
	in multiple iterations and from different sources. The core part
	of our method is the combination of local and global cues via probabilistic
	top-down segmentation. Altogether, this approach allows examining
	and comparing object hypotheses with high precision down to the pixel
	level. Qualitative and quantitative results on a large data set confirm
	that our method is able to reliably detect pedestrians in crowded
	scenes, even when they overlap and partially occlude each other.
	In addition, the flexible nature of our approach allows it to operate
	on very small training sets.},
  comment = {det_pedestrian},
  doi = {10.1109/CVPR.2005.272},
  file = {:papers\\2005 CNF, Pedestrian detection in crowded scenes (Leibe, Seemann, Schiele, CVPR, 246).pdf:PDF},
  issn = {1063-6919 },
  keywords = { crowded scenes; global cues; local cues; object hypotheses; pedestrian
	detection; probabilistic top-down segmentation; feature extraction;
	image segmentation; object detection; probability;},
  timestamp = {00,250}
}

@ARTICLE{11202274,
  author = {Leichter, I. and Lindenbaum, M. and Rivlin, E.},
  title = {Mean Shift tracking with multiple reference color histograms},
  journal = {Computer Vision and Image Understanding},
  year = {2010},
  volume = { 114},
  pages = {400 - 8},
  number = { 3},
  note = {mean shift tracking;multiple reference color histogram;image sequences;object
	color histogram;multiple reference histogram;mean shift tracker;convex
	hull;},
  abstract = {The Mean Shift tracker is a widely used tool for robustly and quickly
	tracking the location of an object in an image sequence using the
	object's color histogram. The reference histogram is typically set
	to that in the target region in the frame where the tracking is initiated.
	Often, however, no single view suffices to produce a reference histogram
	appropriate for tracking the target. In contexts where multiple views
	of the target are available prior to the tracking, this paper enhances
	the Mean Shift tracker to use multiple reference histograms obtained
	from these different target views. This is done while preserving
	both the convergence and the speed properties of the original tracker.
	We first suggest a simple method to use multiple reference histograms
	for producing a single histogram that is more appropriate for tracking
	the target. Then, to enhance the tracking further, we propose an
	extension to the Mean Shift tracker where the convex hull of these
	histograms is used as the target model. Many experimental results
	demonstrate the successful tracking of targets whose visible colors
	change drastically and rapidly during the sequence, where the basic
	Mean Shift tracker obviously fails. [All rights reserved Elsevier].},
  address = {USA},
  comment = {trk_color},
  copyright = {Copyright 2010, The Institution of Engineering and Technology},
  file = {:papers\\2010 JNL, Mean Shift tracking with multiple reference color histograms (Leichter, Lindenbaum, Rivlin).pdf:PDF},
  issn = {1077-3142},
  keywords = {image colour analysis;image sequences;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1016/j.cviu.2009.12.006}
}

@ARTICLE{4586387,
  author = {Leichter, I. and Lindenbaum, M. and Rivlin, E.},
  title = {Tracking by Affine Kernel Transformations Using Color and Boundary
	Cues},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2009},
  volume = {31},
  pages = {164 -171},
  number = {1},
  month = {jan. },
  abstract = {Kernel-based trackers aggregate image features within the support
	of a kernel (a mask) regardless of their spatial structure. These
	trackers spatially fit the kernel (usually in location and in scale)
	such that a function of the aggregate is optimized. We propose a
	kernel-based visual tracker that exploits the constancy of color
	and the presence of color edges along the target boundary. The tracker
	estimates the best affinity of a spatially aligned pair of kernels,
	one of which is color-related and the other of which is object boundary-related.
	In a sense, this work extends previous kernel-based trackers by incorporating
	the object boundary cue into the tracking process and by allowing
	the kernels to be affinely transformed instead of only translated
	and isotropically scaled. These two extensions make for more precise
	target localization. A more accurately localized target also facilitates
	safer updating of its reference color model, further enhancing the
	tracker's robustness. The improved tracking is demonstrated for several
	challenging image sequences.},
  comment = {trk_color},
  doi = {10.1109/TPAMI.2008.194},
  file = {:papers\\2009 JNL, Tracking by Affine Kernel Transformations Using Color and Boundary Cues (Leichter, Lindenbaum, Rivlin).pdf:PDF},
  issn = {0162-8828},
  keywords = {affine kernel transformations;color edges;image features;image sequences;kernel-based
	visual tracker;object boundary cue;reference color model;target localization;edge
	detection;feature extraction;image colour analysis;image sequences;tracking;Algorithms;Artificial
	Intelligence;Color;Colorimetry;Computer Simulation;Cues;Image Interpretation,
	Computer-Assisted;Models, Theoretical;Pattern Recognition, Automated;},
  timestamp = {-}
}

@BOOK{1993_BOOK_RandomProcesses_Garcia,
  title = {Probability and Random Processes for Electrical Engineering (2nd
	Edition)},
  publisher = {Addison-Wesley},
  year = {1993},
  author = {Leon-Garcia, Albert},
  edition = {2},
  month = {August},
  abstract = {{<P>This textbook offers an interesting, straightforward introduction
	to probability and random processes. While helping students to develop
	their problem-solving skills, the book enables them to understand
	how to make the transition from real problems to probability models
	for those problems. To keep students motivated, the author uses a
	number of practical applications from various areas of electrical
	and computer engineering that demonstrate the relevance of probability
	theory to engineering practice. Discrete-time random processes are
	used to bridge the transition between random variables and continuous-time
	random processes. Additional material has been added to the second
	edition to provide a more substantial introduction to random processes.</P>}},
  comment = {SP_book},
  day = {10},
  file = {:papers\\1993 BOOK, Probability and Random Processes for Electrical Engineering (Garcia, Addison-Wesley, 2nd ed).pdf:PDF},
  howpublished = {Paperback},
  isbn = {020150037X},
  posted-at = {2010-04-15 15:39:43},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/020150037X}
}

@INPROCEEDINGS{2003_NF_UnsupervisedImprovementVisualDetectors_Levin,
  author = {Levin, A. and Viola, P. and Freund, Y.},
  title = {Unsupervised improvement of visual detectors using cotraining},
  booktitle = {Computer Vision, 2003. Proceedings. Ninth IEEE International Conference
	on},
  year = {2003},
  pages = {626-633 vol.1},
  abstract = {One significant challenge in the construction of visual detection
	systems is the acquisition of sufficient labeled data. We describe
	a new technique for training visual detectors which requires only
	a small quantity of labeled data, and then uses unlabeled data to
	improve performance over time. Unsupervised improvement is based
	on the cotraining framework of Blum and Mitchell, in which two disparate
	classifiers are trained simultaneously. Unlabeled examples which
	are confidently labeled by one classifier are added, with labels,
	to the training set of the other classifier. Experiments are presented
	on the realistic task of automobile detection in roadway surveillance
	video. In this application, cotraining reduces the false positive
	rate by a factor of 2 to 11 from the classifier trained with labeled
	data alone.},
  comment = {trk},
  file = {:papers\\2003 CNF, Unsupervised improvement  of visual detectors using cotraining (Levin, Viola, Freund, ICCV, 100).pdf:PDF},
  keywords = {image recognition object detection pattern classification surveillance
	unsupervised learning automobile detection labeled data acquisition
	pattern classifier roadway surveillance video unsupervised improvement
	visual detection system visual detector training},
  owner = {salman},
  timestamp = {00,100}
}

@INPROCEEDINGS{1994_CNF_TwoLearningAlgosText_LewisRinguette,
  author = {Lewis, David and Ringuette, Marc},
  title = {A Comparison of Two Learning Algorithms for Text Categorization},
  booktitle = {In Third Annual Symposium on Document Analysis and Information Retrieval},
  year = {1994},
  pages = {81-93},
  abstract = {This paper examines the use of inductive learning to categorize natural
	language documents into predefined content categories. Categorization
	of text is of increasing importance in information retrieval and
	natural language processing systems. Previous research on automated
	text categorization has mixed machine learning and knowledge engineering
	methods, making it difficult to draw conclusions about the performance
	of particular methods. In this paper we present empirical results
	on the performance of a Bayesian classifier and a decision tree learning
	algorithm on two text categorization data sets. We find that both
	algorithms achieve reasonable performance and allow controlled tradeoffs
	between false positives and false negatives. The stepwise feature
	selection in the decision tree algorithm is particularly effective
	in dealing with the large feature sets common in text categorization.
	However, even this algorithm is aided by an initial prefiltering
	of features, confirming the results...},
  comment = {PRML},
  file = {:papers\\1994 CNF, A Comparison of Two Learning Algorithms for Text Categorization (Lewis, Ringuette, DAIR, 636).pdf:PDF},
  owner = {salman},
  timestamp = {00,600}
}

@ARTICLE{2004_JNL_SoccerHMM_Sun,
  author = {Lexing, Xie and Peng, Xu and Shih-Fu, Chang and Divakaran, A. and
	Sun, H.},
  title = {Structure analysis of soccer video with domain knowledge and hidden
	Markov models},
  journal = {Pattern Recognition Letters},
  year = {2004},
  volume = {25},
  pages = {767-75},
  abstract = {In this paper, we present statistical techniques for parsing the structure
	of produced soccer programs. The problem is important for applications
	such as personalized video streaming and browsing systems, in which
	videos are segmented into different states and important states are
	selected based on user preferences. While prior work focuses on the
	detection of special events such as goals or corner kicks, this paper
	is concerned with generic structural elements of the game. We define
	two mutually exclusive states of the game, play and break based on
	the rules of soccer. Automatic detection of such generic states represents
	an original challenging issue due to high appearance diversities
	and temporal dynamics of such states in different videos. We select
	a salient feature set from the compressed domain, dominant color
	ratio and motion intensity, based on the special syntax and content
	characteristics of soccer videos. We then model the stochastic structures
	of each state of the game with a set of hidden Markov models. Finally,
	higher-level transitions are taken into account and dynamic programming
	techniques are used to obtain the maximum likelihood segmentation
	of the video sequence. The system achieves a promising classification
	accuracy of 83.5%, with light-weight computation on feature extraction
	and model inference, as well as a satisfactory accuracy in boundary
	timing},
  comment = {analysis},
  file = {:papers\\2004 JNL, Structure analysis of soccer video with domain knowledge and hidden Markov (Xie, Xu, Change, Divakaran, Sun, PRL, 300).pdf:PDF},
  keywords = {dynamic programming feature extraction hidden Markov models image
	classification image segmentation knowledge engineering maximum likelihood
	sequence estimation sport video signal processing},
  owner = {salman},
  timestamp = {00,300}
}

@ARTICLE{2001_JNL_TemporalObjectVerification_BaoxinChellappaQinfenDer,
  author = {Li, Baoxin and Chellappa, R. and Qinfen, Zheng and Der, S. Z.},
  title = {Model-based temporal object verification using video},
  journal = {IEEE Transactions on Image Processing},
  year = {2001},
  volume = {10},
  pages = {897-908},
  abstract = {An approach to model-based dynamic object verification and identification
	using video is proposed. From image sequences containing the moving
	object, we compute its motion trajectory. Then we estimate its three-dimensional
	(3-D) pose at each time step. Pose estimation is formulated as a
	search problem, with the search space constrained by the motion trajectory
	information of the moving object and assumptions about the scene
	structure. A generalized Hausdorff (1962) metric, which is more robust
	to noise and allows a confidence interpretation, is suggested for
	the matching procedure used for pose estimation as well as the identification
	and verification problem. The pose evolution curves are used to assist
	in the acceptance or rejection of an object hypothesis. The models
	are acquired from real image sequences of the objects. Edge maps
	are extracted and used for matching. Results are presented for both
	infrared and optical sequences containing moving objects involved
	in complex motions},
  comment = {analysis},
  file = {:papers\\2001 JNL, Model-based temporal object verification using video (Li, Chellappa, TIP, 12).pdf:PDF},
  keywords = {edge detection feature extraction image matching image motion analysis
	image sequences object recognition parameter estimation video signal
	processing},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{20084311651012,
  author = {Li, Peihua},
  title = {An adaptive binning color model for mean shift tracking},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  year = {2008},
  volume = {18},
  pages = {1293 - 1299},
  number = {9},
  note = {Color distribution;Independent component analysis (ICA);Mean shift
	(MS);Object tracking;},
  abstract = {The mean shift (MS) algorithm for object tracking using color has
	recently received a significant amount of attention thanks to its
	effectiveness and efficiency. Most current work, unfortunately, failing
	to notice that object color is usually very compactly distributed,
	partitions uniformly the whole color space and thus leads to a large
	number of void bins and limited capability of representing object
	color distribution. Also, there lacks a systematic way to determine
	automatically the number of bins. Aiming to address these problems,
	this paper presents an adaptive binning color model for MS tracking.
	First, the object color is analyzed based on a clustering algorithm
	and, according to the clustering result, the color space of the object
	is partitioned into subspaces by orthonormal transformation. Then,
	a color model is defined by considering the weighted number of pixels
	as well as intra-cluster distribution based on independent component
	analysis (ICA), and a similarity measure is introduced to evaluate
	likeness between the reference and the candidate models. Finally,
	the MS algorithm is developed based on the proposed color model and
	its computational complexity is analyzed. Experiments show that the
	proposed algorithm has better tracking performance than the conventional
	MS algorithm at the cost of moderately increasing computational load.
	&copy; 2008 IEEE.},
  address = {445 Hoes Lane / P.O. Box 1331, Piscataway, NJ 08855-1331, United
	States},
  comment = {trk_color},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2008 JNL, An Adaptive Binning Color Model for Mean Shift Tracking (Li).pdf:PDF},
  issn = {10518215},
  key = {Color},
  keywords = {Adaptive algorithms;Algorithms;Bins;Boolean functions;Clustering algorithms;Color
	image processing;Computational complexity;Flow of solids;Image coding;Image
	enhancement;Independent component analysis;Mathematical transformations;Neural
	networks;Optical properties;Vector quantization;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1109/TCSVT.2008.928528}
}

@ARTICLE{2004_JNL_TRKsubs_Li,
  author = {Yongmin Li},
  title = {On incremental and robust subspace learning},
  journal = {Pattern Recognition},
  year = {2004},
  volume = {37},
  pages = {1509 - 1518},
  number = {7},
  abstract = {Principal Component Analysis (PCA) has been of great interest in computer
	vision and pattern recognition. In particular, incrementally learning
	a PCA model, which is computationally efficient for large-scale problems
	as well as adaptable to reflect the variable state of a dynamic system,
	is an attractive research topic with numerous applications such as
	adaptive background modelling and active object recognition. In addition,
	the conventional PCA, in the sense of least mean squared error minimisation,
	is susceptible to outlying measurements. To address these two important
	issues, we present a novel algorithm of incremental PCA, and then
	extend it to robust PCA. Compared with the previous studies on robust
	PCA, our algorithm is computationally more efficient. We demonstrate
	the performance of these algorithms with experimental results on
	dynamic background modelling and multi-view face modelling.},
  comment = {TRK_subspace},
  doi = {DOI: 10.1016/j.patcog.2003.11.010},
  file = {:papers\\2003 JNL, On incremental and robust subspace learning (Li).pdf:PDF},
  issn = {0031-3203},
  keywords = {Principal Component Analysis},
  timestamp = {00,060},
  url = {http://www.sciencedirect.com/science/article/B6V14-4BNMG7H-2/2/e177e3878a17f6a410643f3222269150}
}

@CONFERENCE{1999_CNF_ModelFromIMG_Liebowitz,
  author = {Liebowitz, D. and Criminisi, A. and Zisserman, A.},
  title = {Creating architectural models from images},
  booktitle = {Computer Graphics Forum},
  year = {1999},
  volume = {18},
  number = {3},
  pages = {39--50},
  comment = {TRK},
  file = {:papers\\1999 CNF, Creating Architectural Models from Images (Liebowitz).pdf:PDF},
  issn = {0167-7055},
  timestamp = {00,200}
}

@ARTICLE{2000_JNL_TrackingClassificationFace_Kanade,
  author = {Lien, J. J. J. and Kanade, T. and Cohn, J. F. and Ching-Chung, Li},
  title = {Detection, tracking, and classification of action units in facial
	expression},
  journal = {Robotics and Autonomous Systems},
  year = {2000},
  volume = {31},
  pages = {131-46},
  number = {Copyright 2000, IEE},
  abstract = {Most of the current work on automated facial expression analysis attempt
	to recognize a small set of prototypic expressions, such as joy and
	fear. Such prototypic expressions, however, occur infrequently, and
	human emotions and intentions are communicated more often by changes
	in one or two discrete features. To capture the full range of facial
	expression, detection, tracking, and classification of fine-grained
	changes in facial features are needed. We developed the first version
	of a computer vision system that is sensitive to subtle changes in
	the face. The system includes three modules to extract feature information:
	dense-flow extraction using a wavelet motion model, facial-feature
	tracking, and edge and line extraction. The feature information thus
	extracted is fed to discriminant classifiers or hidden Markov models
	that classify it into FACS action units, the descriptive system to
	code fine-grained changes in facial expression. The system was tested
	on image sequences from 100 male and female subjects of varied ethnicity.
	Agreement with manual FACS coding was strong for the results based
	on dense-flow extraction and facial-feature tracking, and strong
	to moderate for edge and line extraction},
  comment = {trk_face},
  file = {:papers\\2000 JNL, Detection, tracking, and classification of action units in facial expression (Kanade, JRAS, 99).pdf:PDF},
  keywords = {computer vision face recognition feature extraction hidden Markov
	models pattern classification tracking wavelet transforms},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{2010_JNL_HumanDetectionSegmentation_Lin,
  author = {Zhe Lin and Davis, L.S.},
  title = {Shape-Based Human Detection and Segmentation via Hierarchical Part-Template
	Matching},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2010},
  volume = {32},
  pages = {604 -618},
  number = {4},
  month = {april },
  abstract = {We propose a shape-based, hierarchical part-template matching approach
	to simultaneous human detection and segmentation combining local
	part-based and global shape-template-based schemes. The approach
	relies on the key idea of matching a part-template tree to images
	hierarchically to detect humans and estimate their poses. For learning
	a generic human detector, a pose-adaptive feature computation scheme
	is developed based on a tree matching approach. Instead of traditional
	concatenation-style image location-based feature encoding, we extract
	features adaptively in the context of human poses and train a kernel-SVM
	classifier to separate human/nonhuman patterns. Specifically, the
	features are collected in the local context of poses by tracing around
	the estimated shape boundaries. We also introduce an approach to
	multiple occluded human detection and segmentation based on an iterative
	occlusion compensation scheme. The output of our learned generic
	human detector can be used as an initial set of human hypotheses
	for the iterative optimization. We evaluate our approaches on three
	public pedestrian data sets (INRIA, MIT-CBCL, and USC-B) and two
	crowded sequences from Caviar Benchmark and Munich Airport data sets.},
  comment = {det_pedestrian},
  doi = {10.1109/TPAMI.2009.204},
  file = {:papers\\2010 JNL, Shape-Based Human Detection and Segmentation via Hierarchical Part-Template Matching (Lin, Larrydavis, PAMI).pdf:PDF},
  issn = {0162-8828},
  keywords = {Caviar benchmark;Munich Airport;concatenation-style image location-based
	feature encoding;feature extraction;global shape-template-based schemes;human
	pattern;iterative occlusion compensation scheme;iterative optimization;kernel-SVM
	classifier;local part-based schemes;nonhuman pattern;part-template
	tree;pose estimation;pose-adaptive feature computation scheme;public
	pedestrian data sets;shape-based hierarchical part-template matching
	approach;shape-based human detection;shape-based human segmentation;tree
	matching approach;computer graphics;feature extraction;image classification;image
	matching;image segmentation;optimisation;pose estimation;shape recognition;support
	vector machines;trees (mathematics);},
  timestamp = {-}
}

@ARTICLE{1980_JNL_LBG_Linde,
  author = {Linde, Y. and Buzo, A. and Gray, R.},
  title = {An Algorithm for Vector Quantizer Design},
  journal = {Communications, IEEE Transactions on},
  year = {1980},
  volume = {28},
  pages = { 84 - 95},
  number = {1},
  month = {jan},
  abstract = { An efficient and intuitive algorithm is presented for the design
	of vector quantizers based either on a known probabilistic model
	or on a long training sequence of data. The basic properties of the
	algorithm are discussed and demonstrated by examples. Quite general
	distortion measures and long blocklengths are allowed, as exemplified
	by the design of parameter vector quantizers of ten-dimensional vectors
	arising in Linear Predictive Coded (LPC) speech compression with
	a complicated distortion measure arising in LPC analysis that does
	not depend only on the error vector.},
  comment = {VQ},
  file = {:papers\\1980 JNL, An Algorithm for Vector Quantizer Design (Linde, Buzo, Gray).pdf:PDF},
  issn = {0090-6778},
  keywords = { Quantization (signal); Signal quantization; Speech coding;},
  timestamp = {04,500}
}

@INPROCEEDINGS{1998_CNF_Tracking_Lipton,
  author = {Lipton, A.J. and Fujiyoshi, H. and Patil, R.S.},
  title = {Moving target classification and tracking from real-time video},
  booktitle = {Applications of Computer Vision, 1998. WACV '98. Proceedings., Fourth
	IEEE Workshop on},
  year = {1998},
  pages = {8 -14},
  month = {oct},
  abstract = {This paper describes an end-to-end method for extracting moving targets
	from a real-time video stream, classifying them into predefined categories
	according to image-based properties, and then robustly tracking them.
	Moving targets are detected using the pixel wise difference between
	consecutive image frames. A classification metric is applied these
	targets with a temporal consistency constraint to classify them into
	three categories: human, vehicle or background clutter. Once classified
	targets are tracked by a combination of temporal differencing and
	template matching. The resulting system robustly identifies targets
	of interest, rejects background clutter and continually tracks over
	large distances and periods of time despite occlusions, appearance
	changes and cessation of target motion},
  comment = {trk_people},
  doi = {10.1109/ACV.1998.732851},
  file = {:papers\\1998 CNF, Moving Target Detection and Classification from Real-Time Video (Lipton, Fujiyoshi, Patil, WACV, 644).pdf:PDF},
  keywords = {classification;classifying;image-based properties;moving targets;template
	matching;temporal consistency constraint;video stream;image classification;target
	tracking;video signal processing;},
  timestamp = {00,600}
}

@ARTICLE{2002_JNL_IntensityAndTextureRobustChangeDetection_Li,
  author = {Liyuan, Li and Leung, Maylor K.H.},
  title = {Integrating intensity and texture differences for robust change detection},
  journal = {Image Processing, IEEE Transactions on},
  year = {2002},
  volume = {11},
  pages = {105 -112},
  number = {2},
  month = {feb},
  abstract = {We propose a novel technique for robust change detection based upon
	the integration of intensity and texture differences between two
	frames. A new accurate texture difference measure based on the relations
	between gradient vectors is proposed. The mathematical analysis shows
	that the measure is robust with respect to noise and illumination
	changes. Two ways to integrate the intensity and texture differences
	have been developed. The first combines the two measures adaptively
	according to the weightage of texture evidence, while the second
	does it optimally with additional constraint of smoothness. The parameters
	of the algorithm are selected automatically based on a statistic
	analysis. An algorithm is developed for fast implementation. The
	computational complexity analysis indicates that the proposed technique
	can run in real-time. The experiment results are evaluated both visually
	and quantitatively. They show that by exploiting both intensity and
	texture differences for change detection, one can obtain much better
	segmentation results than using the intensity or structure difference
	alone},
  comment = {trk_BG},
  doi = {10.1109/83.982818},
  file = {:papers\\2002 JNL, Integrating intensity and texture differences for robust change detection (Li, Leung, TIP, 91).pdf:PDF},
  issn = {1057-7149},
  keywords = {computational complexity;gradient vectors;illumination changes;intensity
	differences;noise changes;robust change detection;segmentation;smoothness;statistic
	analysis;texture differences;image recognition;image segmentation;image
	sequences;image texture;video signal processing;},
  timestamp = {00,100}
}

@ARTICLE{1982_JNL_LeastSquaresQuantization_Lloyd,
  author = {Lloyd, S.},
  title = {Least squares quantization in PCM},
  journal = {Information Theory, IEEE Transactions on},
  year = {1982},
  volume = {28},
  pages = {129-137},
  number = {2},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with
	a given ensemble of signals to handle, the quantum values should
	be spaced more closely in the voltage regions where the signal amplitude
	is more likely to fall. It has been shown by Panter and Dite that,
	in the limit as the number of quanta becomes infinite, the asymptotic
	fractional density of quanta per unit voltage should vary as the
	one-third power of the probability density per unit voltage of signal
	amplitudes. In this paper the corresponding result for any finite
	number of quanta is derived; that is, necessary conditions are found
	that the quanta and associated quantization intervals of an optimum
	finite quantization scheme must satisfy. The optimization criterion
	used is that the average quantization noise power be a minimum. It
	is shown that the result obtained here goes over into the Panter
	and Dite result as the number of quanta become large. The optimum
	quautization schemes for<tex>2^{b}</tex>quanta,<tex>b=1,2, cdots,
	7</tex>, are given numerically for Gaussian and for Laplacian distribution
	of signal amplitudes.},
  comment = {VQ},
  file = {:papers\\1982 JNL, Least Squares Quantization in PCM (LLoyd, TIT, 1901).pdf:PDF},
  keywords = {Least-squares approximation PCM communication Quantization (signal)
	Signal quantization},
  owner = {salman},
  timestamp = {02,000}
}

@ARTICLE{2004_CNF_GraphCornerMatching_Lourakis,
  author = {Lourakis, M.I.A. and Argyros, A.A. and Marias, K.},
  title = {A graph-based approach to corner matching using mutual information
	as a local similarity measure},
  journal = {Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International
	Conference on},
  year = {2004},
  volume = {2},
  pages = { 827-830 Vol.2},
  month = {Aug.},
  abstract = { Corner matching constitutes a fundamental vision problem that serves
	as a building block of several important applications. The common
	approach to dealing with this problem starts by ranking potential
	matches according to their affinity, which is assessed with the aid
	of window-based intensity similarity measures. Then, actual matches
	are established by optimizing global criteria involving all potential
	matches. This paper puts forward a novel approach for solving the
	corner matching problem that uses mutual information as a window
	similarity measure, combined with graph matching techniques for determining
	a matching of corners that is globally optimal. Experimental results
	illustrate the effectiveness of the approach.},
  doi = {10.1109/ICPR.2004.1334386},
  issn = {1051-4651 },
  keywords = { graph theory, image matching, optimisation corner matching, global
	criteria optimization, graph matching technique, graph-based approach,
	local similarity measure, mutual information, vision problem, window-based
	intensity similarity measures}
}

@ARTICLE{1991_JNL_Fitting3Dmodels_Lowe,
  author = {Lowe, D.G.},
  title = {Fitting parameterized three-dimensional models to images},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1991},
  volume = {13},
  pages = {441 -450},
  number = {5},
  month = {may},
  abstract = {Model-based recognition and motion tracking depend upon the ability
	to solve for projection and model parameters that will best fit a
	3-D model to matching 2-D image features. The author extends current
	methods of parameter solving to handle objects with arbitrary curved
	surfaces and with any number of internal parameters representing
	articulation, variable dimensions, or surface deformations. Numerical
	stabilization methods are developed that take account of inherent
	inaccuracies in the image measurements and allow useful solutions
	to be determined even when there are fewer matches than unknown parameters.
	The Levenberg-Marquardt method is used to always ensure convergence
	of the solution. These techniques allow model-based vision to be
	used for a much wider class of problems than was possible with previous
	methods. Their application is demonstrated for tracking the motion
	of curved, parameterized objects},
  comment = {3D},
  doi = {10.1109/34.134043},
  file = {:papers\\1991 JNL, Fitting parameterized three-dimensional models to images (Lowe, PAMI, 627).pdf:PDF},
  issn = {0162-8828},
  keywords = {2D image matching;3D model;Levenberg-Marquardt method;arbitrary curved
	surfaces;model based pattern recognition;motion tracking;picture
	processing;curve fitting;pattern recognition;picture processing;},
  timestamp = {00,600}
}

@ARTICLE{2004_JNL_SIFT_Lowe,
  author = {Lowe, David G.},
  title = {Distinctive Image Features from Scale-Invariant Keypoints},
  journal = {International Journal of Computer Vision},
  year = {2004},
  volume = {60},
  pages = {91-110},
  number = {2},
  abstract = {This paper presents a method for extracting distinctive invariant
	features from images that can be used to perform reliable matching
	between different views of an object or scene. The features are invariant
	to image scale and rotation, and are shown to provide robust matching
	across a substantial range of affine distortion, change in 3D viewpoint,
	addition of noise, and change in illumination. The features are highly
	distinctive, in the sense that a single feature can be correctly
	matched with high probability against a large database of features
	from many images. This paper also describes an approach to using
	these features for object recognition. The recognition proceeds by
	matching individual features to a database of features from known
	objects using a fast nearest-neighbor algorithm, followed by a Hough
	transform to identify clusters belonging to a single object, and
	finally performing verification through least-squares solution for
	consistent pose parameters. This approach to recognition can robustly
	identify objects among clutter and occlusion while achieving near
	real-time performance.},
  comment = {feature},
  file = {:papers\\2004 JNL, Distinctive image features from scale-invariant keypoints (Lowe, IJCV, 6582).pdf:PDF},
  owner = {salman},
  timestamp = {06,600}
}

@INPROCEEDINGS{2006_CNF_PCAhog_Lu,
  author = {Wei-Lwun Lu and Little, J.J.},
  title = {Simultaneous Tracking and Action Recognition using the PCA-HOG Descriptor},
  booktitle = {Computer and Robot Vision, 2006. The 3rd Canadian Conference on},
  year = {2006},
  pages = { 6 - 6},
  abstract = {This paper presents a template-based algorithm to track and recognize
	athlete 146;s actions in an integrated system using only visual information.
	Conventional template-based action recognition systems usually consider
	action recognition and tracking as two independent problems, and
	solve them separately. In contrast, our algorithm emphasizes that
	tracking and action recognition can be tightly coupled into a single
	framework, where tracking assists action recognition and vise versa.
	Moreover, this paper proposes to represent the athletes by the PCA-HOG
	descriptor, which can be computed by first transforming the athletes
	to the grids of Histograms of Oriented Gradient (HOG) descriptor
	and then project it to a linear subspace by Principal Component Analysis
	(PCA). The exploitation of the PCA-HOG descriptor not only helps
	the tracker to be robust under illumination, pose, and view-point
	changes, but also implicitly centers the figure in the tracking region,
	which makes action recognition possible. Empirical results in hockey
	and soccer sequences show the effectiveness of this algorithm.},
  comment = {TRK_subspace},
  file = {:papers\\2006 CNF, Simultaneous Tracking and Action Recognition using the PCA-HOG Descriptor (Lu).pdf:PDF},
  timestamp = {00,020}
}

@INPROCEEDINGS{1981_CNF_IterativeImageRegistration_LucasKanade,
  author = {Lucas, Bruce D. and Kanade, Takeo},
  title = {An iterative image registration technique with an application to
	stereo vision},
  booktitle = {Proceedings of the 7th International Joint Conference on Artificial
	Intelligence.},
  year = {1981},
  volume = {2},
  pages = {674-679},
  comment = {registration},
  file = {:papers\\1981 CNF, An iterative image registration technique with an application to stereo (Lucas, Kanade, JCAI, 3842).pdf:PDF},
  keywords = {IMAGE PROCESSING},
  timestamp = {04,000}
}

@ARTICLE{2006_JNL_Camera_Lv,
  author = {Fengjun Lv and Tao Zhao and Nevatia, R.},
  title = {Camera calibration from video of a walking human},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2006},
  volume = {28},
  pages = {1513 -1518},
  number = {9},
  month = sept.,
  abstract = {A self-calibration method to estimate a camera's intrinsic and extrinsic
	parameters from vertical line segments of the same height is presented.
	An algorithm to obtain the needed line segments by detecting the
	head and feet positions of a walking human in his leg-crossing phases
	is described. Experimental results show that the method is accurate
	and robust with respect to various viewing angles and subjects},
  comment = {3D_camera},
  doi = {10.1109/TPAMI.2006.178},
  file = {:papers\\2006 JNL, Camera calibration from video of a walking human (Lv).pdf:PDF},
  issn = {0162-8828},
  keywords = {camera calibration;leg-crossing phases;self-calibration method;vertical
	line segments;video;walking human;calibration;image motion analysis;image
	sensors;video signal processing;},
  timestamp = {00,040}
}

@INPROCEEDINGS{2002_CNF_Calibration_Lv,
  author = {Fengjun Lv and Tao Zhao and Nevatia, R.},
  title = {Self-calibration of a camera from video of a walking human},
  booktitle = {Pattern Recognition, 2002. Proceedings. 16th International Conference
	on},
  year = {2002},
  volume = {1},
  pages = { 562 - 567 vol.1},
  abstract = {Analysis of human activity from a video camera is simplified by the
	knowledge of the camera's intrinsic and extrinsic parameters. We
	describe a technique to estimate such parameters from image observations
	without requiring measurements of scene objects. We first develop
	a general technique for calibration using vanishing points and vanishing
	line. We then describe a method for estimating the needed points
	and line by observing the motion of a human in the scene. Experimental
	results, including error estimates, are presented.},
  comment = {3D_camera},
  doi = {10.1109/ICPR.2002.1044793},
  file = {:papers\\2002 CNF, Self-calibration of a camera from video of a walking human (Lv).pdf:PDF},
  issn = {1051-4651 },
  keywords = { error estimates; extrinsic parameters; human activity analysis; image
	observations; intrinsic parameters; self-calibration; vanishing line;
	vanishing points; video camera; walking human; calibration; image
	motion analysis; image sequences; matrix algebra; parameter estimation;
	video cameras;},
  timestamp = {00,070}
}

@INPROCEEDINGS{2009_CNF_SURVEY_corresp_Ma,
  author = {Zhifeng Ma and Jiuqing Wan},
  title = {Survey of data association of moving objects tracking in video sensors
	network},
  booktitle = {Electronic Measurement Instruments, 2009. ICEMI '09. 9th International
	Conference on},
  year = {2009},
  pages = {4-250 -4-254},
  month = aug.,
  abstract = {Object tracking is a crucial task in computer vision systems for surveillance,
	traffic monitoring or intelligent homes. In all these cases, tracking
	is based on association of observations. Conventional tracking approaches
	assume similarity in space, time and appearance of objects in successive
	observations. However, Observations of objects are often widely separated
	in time and space when viewed from multiple non overlapping cameras.
	Moreover, appearance of one object is dramatically different among
	cameras due to the differences in illumination, pose and camera parameters.
	On the other hand, real world's time-space constraint (motion path
	information)on moving object and sensors position are always available
	in advance. Thus, this paper presents a survey of data association
	method across visual sensors network for moving objects tracking.
	In particular, previous research on various data association methods
	of non overlapping cameras is reviewed.},
  comment = {survey},
  doi = {10.1109/ICEMI.2009.5274078},
  file = {:C\:\\salman\\work\\writing\\papers\\2009 CNF, Survey of data association of moving objects tracking in video sensors network (Ma).pdf:PDF},
  keywords = {computer vision systems;data association method;illumination;intelligent
	homes;moving objects tracking;multiple nonoverlapping cameras;pose
	parameters;sensors position;surveillance;time-space constraint;traffic
	monitoring;video sensors network;computer vision;image motion analysis;sensor
	fusion;video cameras;video surveillance;wireless sensor networks;},
  timestamp = {?}
}

@ARTICLE{2000_JNL_ProbabilisticExclusionMacCormickBlake,
  author = {Maccormick, John and Blake, Andrew},
  title = {A Probabilistic exclusion principle for tracking multiple objects},
  journal = {International Journal of Computer Vision},
  year = {2000},
  volume = {39},
  pages = {57-71},
  number = {Compendex},
  abstract = {Tracking multiple targets is a challenging problem, especially when
	the targets are `identical', in the sense that the same model is
	used to describe each target. In this case, simply instantiating
	several independent 1-body trackers is not an adequate solution,
	because the independent trackers tend to coalesce onto the best-fitting
	target. This paper presents an observation density for tracking which
	solves this problem by exhibiting a probabilistic exclusion principle.
	Exclusion arises naturally from a systematic derivation of the observation
	density, without relying on heuristics. Another important contribution
	of the paper is the presentation of partitioned sampling, a new sampling
	method for multiple object tracking. Partitioned sampling avoids
	the high computational load associated with fully coupled trackers,
	while retaining the desirable properties of coupling.},
  comment = {trk_contour},
  file = {:papers\\2000 JNL, A probabilistic exclusion principle for tracking multiple objects (McCormick, Blake, IJCV, 369).pdf:PDF},
  keywords = {Computer vision Computer simulation Monte Carlo methods Object recognition
	Three dimensional computer graphics},
  owner = {salman},
  timestamp = {00,400}
}

@INPROCEEDINGS{1967_CNF_Kmeans_Macqueen,
  author = {MacQueen, J.},
  title = {Some methods for classification and analysis of multivariate observations},
  booktitle = {Proc. Fifth Berkeley Symp. on Math. Statist. and Prob.},
  year = {1967},
  comment = {VQ},
  file = {:papers\\1967 CNF, Some methods for classification and analysis of multivariate observations (Macqueen).pdf:PDF},
  timestamp = {06,000}
}

@ARTICLE{1985_JNL_VQ_Makhoul,
  author = {Makhoul, J. and Roucos, S. and Gish, H.},
  title = {Vector quantization in speech coding},
  journal = {Proceedings of the IEEE},
  year = {1985},
  volume = {73},
  pages = { 1551 - 1588},
  number = {11},
  month = {nov.},
  abstract = { Quantization, the process of approximating continuous-amplitude signals
	by digital (discrete-amplitude) signals, is an important aspect of
	data compression or coding, the field concerned with the reduction
	of the number of bits necessary to transmit or store analog data,
	subject to a distortion or fidelity criterion. The independent quantization
	of each signal value or parameter is termed scalar quantization,
	while the joint quantization of a block of parameters is termed block
	or vector quantization. This tutorial review presents the basic concepts
	employed in vector quantization and gives a realistic assessment
	of its benefits and costs when compared to scalar quantization. Vector
	quantization is presented as a process of redundancy removal that
	makes effective use of four interrelated properties of vector parameters:
	linear dependency (correlation), nonlinear dependency, shape of the
	probability density function (pdf), and vector dimensionality itself.
	In contrast, scalar quantization can utilize effectively only linear
	dependency and pdf shape. The basic concepts are illustrated by means
	of simple examples and the theoretical limits of vector quantizer
	performance are reviewed, based on results from rate-distortion theory.
	Practical issues relating to quantizer design, implementation, and
	performance in actual applications are explored. While many of the
	methods presented are quite general and can be used for the coding
	of arbitrary signals, this paper focuses primarily on the coding
	of speech signals and parameters.},
  comment = {survey, VQ},
  file = {:papers\\1985 JNL, Vector quantization in speech coding (Makhoul, Roucos, Gish).pdf:PDF},
  issn = {0018-9219},
  timestamp = {00,650}
}

@ARTICLE{1995_JNL_LevelSets_Malladi,
  author = {Malladi, R. and Sethian, J.A. and Vemuri, B.C.},
  title = {Shape modeling with front propagation: a level set approach},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1995},
  volume = {17},
  pages = {158 -175},
  number = {2},
  month = {feb},
  abstract = {Shape modeling is an important constituent of computer vision as well
	as computer graphics research. Shape models aid the tasks of object
	representation and recognition. This paper presents a new approach
	to shape modeling which retains some of the attractive features of
	existing methods and overcomes some of their limitations. The authors'
	techniques can be applied to model arbitrarily complex shapes, which
	include shapes with significant protrusions, and to situations where
	no a priori assumption about the object's topology is made. A single
	instance of the authors' model, when presented with an image having
	more than one object of interest, has the ability to split freely
	to represent each object. This method is based on the ideas developed
	by Osher and Sethian (1988) to model propagating solid/liquid interfaces
	with curvature-dependent speeds. The interface (front) is a closed,
	nonintersecting, hypersurface flowing along its gradient field with
	constant speed or a speed that depends on the curvature. It is moved
	by solving a ldquo;Hamilton-Jacobi rdquo; type equation written for
	a function in which the interface is a particular level set. A speed
	term synthesized from the image is used to stop the interface in
	the vicinity of object boundaries. The resulting equation of motion
	is solved by employing entropy-satisfying upwind finite difference
	schemes. The authors present a variety of ways of computing the evolving
	front, including narrow bands, reinitializations, and different stopping
	criteria. The efficacy of the scheme is demonstrated with numerical
	experiments on some synthesized images and some low contrast medical
	images},
  comment = {trk_contour},
  doi = {10.1109/34.368173},
  file = {:papers\\1995 JNL, Shape modeling with front propagation_ a level set approach (Malladi, Sethian, Vemuri).pdf:PDF},
  issn = {0162-8828},
  keywords = {Hamilton-Jacobi type equation;arbitrarily complex shapes;closed nonintersecting
	hypersurface;computer graphics;computer vision;curvature-dependent
	speeds;entropy-satisfying upwind finite difference schemes;equation
	of motion;front propagation;level set approach;low contrast medical
	images;object recognition;object representation;propagating solid/liquid
	interfaces;protrusions;reinitializations;shape modeling;stopping
	criteria;synthesized images;computer vision;entropy;object recognition;piecewise-linear
	techniques;},
  timestamp = {02,000}
}

@ARTICLE{1989_JNL_Wavelets_Mallat,
  author = {Mallat, S. G.},
  title = {A theory for multiresolution signal decomposition: the wavelet representation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1989},
  volume = {11},
  pages = {674-693},
  number = {7},
  abstract = {Multiresolution representations are effective for analyzing the information
	content of images. The properties of the operator which approximates
	a signal at a given resolution were studied. It is shown that the
	difference of information between the approximation of a signal at
	the resolutions 2<sup>j+1</sup> and 2<sup>j</sup> (where j is an
	integer) can be extracted by decomposing this signal on a wavelet
	orthonormal basis of <e1>L</e1><sup>2</sup>(<e1>R</e1><sup>n</sup>),
	the vector space of measurable, square-integrable <e1>n</e1>-dimensional
	functions. In <e1>L</e1><sup>2</sup>(<e1>R</e1>), a wavelet orthonormal
	basis is a family of functions which is built by dilating and translating
	a unique function &psi;(<e1>x</e1>). This decomposition defines an
	orthogonal multiresolution representation called a wavelet representation.
	It is computed with a pyramidal algorithm based on convolutions with
	quadrature mirror filters. Wavelet representation lies between the
	spatial and Fourier domains. For images, the wavelet representation
	differentiates several spatial orientations. The application of this
	representation to data compression in image coding, texture discrimination
	and fractal analysis is discussed},
  comment = {SP},
  file = {:papers\\1989 JNL, A theory for multiresolution signal decomposition_ The Wavelet Representation (Mallat, PAMI, 10257).pdf:PDF},
  keywords = {data compression encoding pattern recognition picture processing convolutions
	fractal analysis image coding multiresolution signal decomposition
	pyramidal algorithm quadrature mirror filters texture discrimination
	wavelet representation},
  owner = {salman},
  timestamp = {10,000}
}

@ARTICLE{2002_JNL_RegionTrackingLevelSetPDEs_Mansouri,
  author = {Mansouri, A. R.},
  title = {Region tracking via level set PDEs without motion computation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2002},
  volume = {24},
  pages = {947-961},
  number = {7},
  abstract = {We propose an approach to region tracking that is derived from a Bayesian
	formulation. The novelty of the approach is twofold. First, no motion
	field or motion parameters need to be computed. This removes a major
	burden since accurate motion computation has been and remains a challenging
	problem and the quality of region tracking algorithms based on motion
	critically depends on the computed motion fields and parameters.
	The second novelty of this approach, is that very little a priori
	information about the region being tracked is used in the algorithm.
	In particular, unlike numerous tracking algorithms, no assumption
	is made on the strength of the intensity edges of the boundary of
	the region being tracked, nor is its shape assumed to be of a certain
	parametric form. The problem of region tracking is formulated as
	a Bayesian estimation problem and the resulting tracking algorithm
	is expressed as a level set partial differential equation. We present
	further extensions to this partial differential equation, allowing
	the possibility of including additional information in the tracking
	process, such as priors on the region's intensity boundaries and
	we present the details of the numerical implementation. Very promising
	experimental results are provided},
  comment = {trk_region},
  file = {:papers\\2002 JNL, Region tracking via level set pdes without motion computation (Mansouri, PAMI, 107).pdf:PDF},
  keywords = {Bayes methods estimation theory image sequences partial differential
	equations probability Bayesian estimation camera motion image sequence
	analysis intensity edges level set PDEs level set equations level
	set partial differential equation natural object region tracking},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{2001_JNL_PCAvsLDA_Martinez,
  author = {Martinez, A.M. and Kak, A.C.},
  title = {PCA versus LDA},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2001},
  volume = {23},
  pages = {228 -233},
  number = {2},
  month = {feb},
  abstract = {In the context of the appearance-based paradigm for object recognition,
	it is generally believed that algorithms based on LDA (linear discriminant
	analysis) are superior to those based on PCA (principal components
	analysis). In this communication, we show that this is not always
	the case. We present our case first by using intuitively plausible
	arguments and, then, by showing actual results on a face database.
	Our overall conclusion is that when the training data set is small,
	PCA can outperform LDA and, also, that PCA is less sensitive to different
	training data sets},
  comment = {PRML_PCA},
  doi = {10.1109/34.908974},
  file = {:papers\\2001 JNL, PCA versus LDA (Martinez, Kak).pdf:PDF},
  issn = {0162-8828},
  keywords = {LDA;PCA;appearance-based paradigm;face database;linear discriminant
	analysis;object recognition;principal components analysis;image recognition;object
	recognition;principal component analysis;},
  timestamp = {00,800}
}

@ARTICLE{8731667,
  author = {Martinkauppi, B. and Soriano, M. and Pietikainen, M.},
  title = {Comparison of skin color detection and tracking methods under varying
	illumination},
  journal = {Journal of Electronic Imaging},
  year = {2005},
  volume = { 14},
  pages = {43014 - 1},
  number = { 4},
  note = {skin color detection;tracking methods;illumination intensity;chromaticity;intensity-invariant
	color coordinates;adaptive skin-color filter;statistical skin color
	model;sequence-based skin detection;skin segmentation;},
  abstract = {When skin areas such as faces and hands are imaged under natural environments
	their color appearance is frequently affected by variations in illumination
	intensity and chromaticity. In color-based skin tracking and detection,
	changing intensity is often dismissed either with the use of normalized,
	intensity-invariant color coordinates or by additionally modeling
	possible skin intensities. Chromaticity variations are rarely considered,
	although they are common in practice. In most approaches considering
	chromaticity, the experiments are done with a small or undefined
	variation range. It is difficult to compare different approaches
	and assess their applicability range for this reason. To improve
	the situation, we evaluate the performance of four state-of-the-art
	methods under drastic but practically common illumination changes.
	The effect of illumination chromaticity for skin is clearly defined,
	and based on it we draw conclusion about the performance of these
	approaches},
  address = {USA},
  comment = {trk_color},
  copyright = {Copyright 2006, IEE},
  file = {:papers\\2005 JNL, Comparison of skin color detection and tracking methods under varying illumination (Martinkauppi, Soriano, Pietikainen).pdf:PDF},
  issn = {1017-9909},
  keywords = {adaptive filters;image colour analysis;image segmentation;image sequences;lighting;skin;statistical
	analysis;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1117/1.2142307}
}

@ARTICLE{2001_JNL_PedestrianTRK_Masoud,
  author = {Masoud, O. and Papanikolopoulos, N.P.},
  title = {A novel method for tracking and counting pedestrians in real-time
	using a single camera},
  journal = {Vehicular Technology, IEEE Transactions on},
  year = {2001},
  volume = {50},
  pages = {1267 -1278},
  number = {5},
  month = sep,
  abstract = {This paper presents a real-time system for pedestrian tracking in
	sequences of grayscale images acquired by a stationary camera. The
	objective is to integrate this system with a traffic control application
	such as a pedestrian control scheme at intersections. The proposed
	approach can also be used to detect and track humans in front of
	vehicles. Furthermore, the proposed schemes can be employed for the
	detection of several diverse traffic objects of interest (vehicles,
	bicycles, etc.) The system outputs the spatio-temporal coordinates
	of each pedestrian during the period the pedestrian is in the scene.
	Processing is done at three levels: raw images, blobs, and pedestrians.
	Blob tracking is modeled as a graph optimization problem. Pedestrians
	are modeled as rectangular patches with a certain dynamic behavior.
	Kalman filtering is used to estimate pedestrian parameters. The system
	was implemented on a Datacube MaxVideo 20 equipped with a Datacube
	Max860 and was able to achieve a peak performance of over 30 frames
	per second. Experimental results based on indoor and outdoor scenes
	demonstrated the system s robustness under many difficult situations
	such as partial or full occlusions of pedestrians},
  comment = {TRK_pedestrian},
  doi = {10.1109/25.950328},
  file = {:papers\\2001 JNL, A novel method for tracking and counting pedestrians in real-time using a single camera (Masoud).pdf:PDF},
  issn = {0018-9545},
  keywords = {Datacube Max860;Datacube MaxVideo 20;Kalman filtering;bicycles;blob
	tracking;full occlusions;graph optimization;grayscale image sequences;image
	processing;indoor scenes;outdoor scenes;partial occlusions;pedestrian
	counting;pedestrian parameter estimation;pedestrian tracking;real-time
	system;rectangular patches;single camera;spatio-temporal coordinates;stationary
	camera;traffic control;traffic objects detection;vehicles;Kalman
	filters;filtering theory;graph theory;image sequences;object detection;optimisation;real-time
	systems;road traffic;tracking;traffic control;video cameras;video
	signal processing;},
  timestamp = {00,100}
}

@ARTICLE{1989_JNL_KalmanFilterDepth_Matthies,
  author = {Matthies, L. and Kanade, T. and Szeliski, R.},
  title = {Kalman filter-based algorithms for estimating depth from image sequences},
  journal = {International Journal of Computer Vision},
  year = {1989},
  volume = {3},
  pages = {209-38},
  abstract = {Using known camera motion to estimate depth from image sequences is
	an important problem in robot vision. Many applications of depth-from-motion,
	including navigation and manipulation, require algorithms that can
	estimate depth in an on-line, incremental fashion. This requires
	a representation that records the uncertainty in depth estimates
	and a mechanism that integrates new measurements with existing depth
	estimates to reduce the uncertainty over time. Kalman filtering provides
	this mechanism. Previous applications of Kalman filtering to depth-from-motion
	have been limited to estimating depth at the location of a sparse
	set of features. The authors introduce a new, pixel-based (iconic)
	algorithm that estimates depth and depth uncertainty at each pixel
	and incrementally refines these estimates over time. They describe
	the algorithm and contrast its formulation and performance to that
	of a feature-based Kalman filtering algorithm. They compare the performance
	of the two approaches by analyzing their theoretical convergence
	rates, by conducting quantitative experiments with images of a flat
	poster, and by conducting qualitative experiments with images of
	a realistic outdoor-scene model. The results show that the new method
	is an effective way to extract depth from lateral camera translations.
	This approach can be extended to incorporate generation motion and
	to integrate other sources of information, such as stereo. The algorithms
	developed, which combine Kalman filtering with iconic descriptions
	of depth, therefore can serve as a useful and general framework for
	low-level dynamic vision},
  comment = {3D},
  file = {:papers\\1989 JNL, Kalman filter-based algorithms for estimating depth from image sequences (Matthies, Kanade, Szeliski, IJCV, 494).pdf:PDF},
  keywords = {computer vision computerised picture processing Kalman filters},
  owner = {salman},
  timestamp = {00,500}
}

@ARTICLE{2010_JNL_TRK_simil_Maver,
  author = {Maver, J.},
  title = {Self-Similarity and Points of Interest},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2010},
  volume = {32},
  pages = {1211 -1226},
  number = {7},
  month = july,
  abstract = {In this work, we present a new approach to interest point detection.
	Different types of features in images are detected by using a common
	computational concept. The proposed approach considers the total
	variability of local regions. The total sum of squares computed on
	the intensity values of a local circular region is divided into three
	components: between-circumferences sum of squares, between-radii
	sum of squares, and the remainder. These three components normalized
	by the total sum of squares represent three new saliency measures,
	namely, radial, tangential, and residual. The saliency measures are
	computed for regions with different radii and scale spaces are built
	in this way. Local extrema in scale space of each of the saliency
	measures are located. They represent features with complementary
	image properties: blob-like features, corner-like features, and highly
	textured points. Results obtained on image sets of different object
	classes and image sets under different types of photometric and geometric
	transformations show high robustness of the method to intraclass
	variations as well as to different photometric transformations and
	moderate geometric transformations and compare favorably with the
	results obtained by the leading interest point detectors from the
	literature. The proposed approach gives a rich set of highly distinctive
	local regions that can be used for object recognition and image matching.},
  comment = {TRK},
  doi = {10.1109/TPAMI.2009.105},
  file = {:papers\\2010 JNL, Self-Similarity and Points of Interest (Maver).pdf:PDF},
  issn = {0162-8828},
  keywords = {between-circumferences sum-of-squares;between-radii sum-of-squares;blob-like
	features;common computational concept;corner-like features;geometric
	transformation;highly textured points;image matching;interest point
	detection;local region variability;object recognition;photometric
	transformation;radial measurement;remainder sum-of-squares;residual
	measurement;saliency measurement;tangential measurement;feature extraction;image
	matching;object detection;Artificial Intelligence;Automobiles;Face;Humans;Image
	Processing, Computer-Assisted;Models, Statistical;Motorcycles;Pattern
	Recognition, Automated;},
  timestamp = {-}
}

@ARTICLE{1960_JNL_QuantMinDistortion_Max,
  author = {Max, J.},
  title = {Quantizing for minimum distortion},
  journal = {Information Theory, IRE Transactions on},
  year = {1960},
  volume = {6},
  pages = {7 -12},
  number = {1},
  abstract = {This paper discusses the problem of the minimization of the distortion
	of a signal by a quantizer when the number of output levels of the
	quantizer is fixed. The distortion is defined as the expected value
	of some function of the error between the input and the output of
	the quantizer. Equations are derived for the parameters of a quantizer
	with minimum distortion. The equations are not soluble without recourse
	to numerical methods, so an algorithm is developed to simplify their
	numerical solution. The case of an input signal with normally distributed
	amplitude and an expected squared error distortion measure is explicitly
	computed and values of the optimum quantizer parameters are tabulated.
	The optimization of a quantizer subject to the restriction that both
	input and output levels be equally spaced is also treated, and appropriate
	parameters are tabulated for the same case as above.},
  comment = {VQ},
  doi = {10.1109/TIT.1960.1057548},
  file = {:papers\\1960 JNL, Quantizing for minimum distortion (Max).pdf:PDF},
  issn = {0096-1000},
  keywords = {Signal sampling/reconstruction;},
  timestamp = {01,000}
}

@MISC{2005_WHITE_Survey3Ddisplay_May,
  author = {Paul May},
  title = {A Survey of 3D Display Technologies},
  howpublished = {White paper, Ocuity Ltd.},
  year = {2005},
  comment = {3D_stereo},
  file = {:papers\\2005 WHITE, Survey of 3D display technologies (May).pdf:PDF},
  timestamp = {-},
  url = {http://www.ocuity.co.uk/Ocuity_white_paper_Survey_of_3D_display_technologies.pdf}
}

@ARTICLE{640267,
  author = {Mazor, E. and Averbuch, A. and Bar-Shalom, Y. and Dayan, J.},
  title = {Interacting multiple model methods in target tracking: a survey},
  journal = {Aerospace and Electronic Systems, IEEE Transactions on},
  year = {1998},
  volume = {34},
  pages = {103 -123},
  number = {1},
  month = jan,
  abstract = {The Interacting Multiple Model (IMM) estimator is a suboptimal hybrid
	filter that has been shown to be one of the most cost-effective hybrid
	state estimation schemes. The main feature of this algorithm is its
	ability to estimate the state of a dynamic system with several behavior
	modes which can ldquo;switch rdquo; from one to another. In particular,
	the IMM estimator can be a self-adjusting variable-bandwidth filter,
	which makes it natural for tracking maneuvering targets. The importance
	of this approach is that it is the best compromise available currently-between
	complexity and performance: its computational requirements are nearly
	linear in the size of the problem (number of models) while its performance
	is almost the same as that of an algorithm with quadratic complexity.
	The objective of this work is to survey and put in perspective the
	existing IMM methods for target tracking problems. Special attention
	is given to the assumptions underlying each algorithm and its applicability
	to various situations},
  comment = {TRK_survey},
  doi = {10.1109/7.640267},
  file = {:c\:\\salman\\work\\writing\\papers\\1998 JNL, Interacting multiple model methods in target tracking_ a survey (Mazor).pdf:PDF},
  issn = {0018-9251},
  keywords = {computational requirements;cost-effective hybrid state estimation;dynamic
	system;interacting multiple model;maneuvering targets;quadratic complexity;self-adjusting
	variable-bandwidth filter;suboptimal hybrid filter;target tracking;computational
	complexity;military computing;military systems;self-adjusting systems;sensor
	fusion;state estimation;target tracking;tracking;},
  timestamp = {00,400}
}

@ARTICLE{2001_JNL_EventDetection_Medioni,
  author = {Medioni, G. and Cohen, I. and Bremond, F. and Hongeng, S. and Nevatia,
	R.},
  title = {Event detection and analysis from video streams},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2001},
  volume = {23},
  pages = {873 -889},
  number = {8},
  month = {aug},
  abstract = {We present a system which takes as input a video stream obtained from
	an airborne moving platform and produces an analysis of the behavior
	of the moving objects in the scene. To achieve this functionality,
	our system relies on two modular blocks. The first one detects and
	tracks moving regions in the sequence. It uses a set of features
	at multiple scales to stabilize the image sequence, that is, to compensate
	for the motion of the observer, then extracts regions with residual
	motion and uses an attribute graph representation to infer their
	trajectories. The second module takes as input these trajectories,
	together with user-provided information in the form of geospatial
	context and goal context to instantiate likely scenarios. We present
	details of the system, together with results on a number of real
	video sequences and also provide a quantitative analysis of the results},
  comment = {analysis},
  doi = {10.1109/34.946990},
  file = {:papers\\2001 JNL, Event detection and analysis from video streams (Medioni, PAMI, 173).pdf:PDF},
  issn = {0162-8828},
  keywords = {airborne moving platform;attribute graph representation;event analysis;event
	detection;geospatial context;goal context;image sequence stabilization;modular
	blocks;moving region detection;moving region tracking;observer motion
	compensation;quantitative analysis;trajectory inference;video sequences;video
	streams;image sequences;inference mechanisms;motion compensation;remote
	sensing;stability;tracking;video signal processing;},
  timestamp = {00,200}
}

@BOOK{2004_BOOK_CV_Medioni,
  title = {Emerging Topics in Computer Vision},
  publisher = {Prentice Hall},
  year = {2004},
  author = {Gerard Medioni and Sing Bing Kang},
  file = {:papers\\2005 BOOK, Emerging Topics in Computer Vision (Medioni).pdf:PDF},
  isbn = {0131013661},
  url = {http://www.amazon.com/Emerging-Topics-Computer-Vision-Medioni/dp/0131013661%3FSubscriptionId%3D0JYN1NVW651KCA56C102%26tag%3Dtechkie-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0131013661}
}

@INPROCEEDINGS{2005_CNF_SceneAnalysisForJPEG2000_Meessen,
  author = {Jrme Meessen and Christophe Parisot and Xavier Desurmont and Jean-franois
	Delaigle},
  title = {Scene Analysis for Reducing Motion JPEG 2000 video Surveillance Delivery
	Bandwidth and Complexity},
  booktitle = {In IEEE International Conference on Image Processing (ICIP 05},
  year = {2005},
  pages = {577--580},
  comment = {Scene analysis},
  timestamp = {00,010}
}

@INPROCEEDINGS{2009_CNF_AbnormalCrowd_MubarakShah,
  author = {Mehran, R. and Oyama, A. and Shah, M.},
  title = {Abnormal crowd behavior detection using social force model},
  booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference
	on},
  year = {2009},
  pages = {935-942},
  abstract = {In this paper we introduce a novel method to detect and localize abnormal
	behaviors in crowd videos using Social Force model. For this purpose,
	a grid of particles is placed over the image and it is advected with
	the space-time average of optical flow. By treating the moving particles
	as individuals, their interaction forces are estimated using social
	force model. The interaction force is then mapped into the image
	plane to obtain Force Flow for every pixel in every frame. Randomly
	selected spatio-temporal volumes of Force Flow are used to model
	the normal behavior of the crowd. We classify frames as normal and
	abnormal by using a bag of words approach. The regions of anomalies
	in the abnormal frames are localized using interaction forces. The
	experiments are conducted on a publicly available dataset from University
	of Minnesota for escape panic scenarios and a challenging dataset
	of crowd videos taken from the web. The experiments show that the
	proposed method captures the dynamics of the crowd behavior successfully.
	In addition, we have shown that the social force approach outperforms
	similar approaches based on pure optical flow.},
  comment = {trk_crowd},
  file = {:papers\\2009 CNF, Abnormal crowd behavior detection using social force model (Mubarak Shah, CVPR).pdf:PDF},
  keywords = {Internet behavioural sciences computing image sequences object detection
	random processes video signal processing University of Minnesota
	Worle Wide Web abnormal crowd behavior detection abnormal frames
	crowd videos escape panic scenarios force flow image plane optical
	flow publicly available dataset randomly selected spatio-temporal
	volumes social force model},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{2006_CNF_IRtracking_Mei,
  author = {Xue Mei and Zhou, S.K. and Hao Wu},
  title = {Integrated Detection, Tracking and Recognition for IR Video-Based
	Vehicle Classification},
  journal = {Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings.
	2006 IEEE International Conference on},
  year = {2006},
  volume = {5},
  pages = {V-V},
  month = {May},
  abstract = {We present an approach for vehicle classification in IR video sequences
	by integrating detection, tracking and recognition. The method has
	two steps. First, the moving target is automatically detected using
	a detection algorithm. Next, we perform simultaneous tracking and
	recognition using an appearance-model based particle filter. The
	tracking result is evaluated at each frame. Low confidence in tracking
	performance initiates a new cycle of detection, tracking and classification.
	We demonstrate the robustness of the proposed method using outdoor
	IR video sequences},
  doi = {10.1109/ICASSP.2006.1661383},
  issn = {1520-6149},
  keywords = {image classification, image sequences, infrared imaging, object detection,
	particle filtering (numerical methods), road vehicles, video signal
	processingIR video-based vehicle classification, appearance-model
	based particle filter, detection algorithm, recognition, tracking,
	video sequences}
}

@INPROCEEDINGS{2000385280191,
  author = {Menser, Bernd and Wien, Mathias},
  title = {Segmentation and tracking of facial regions in color image sequences},
  booktitle = {Proceedings of SPIE},
  year = {2000},
  volume = {4067},
  pages = {II/ - },
  address = {Perth, Aust},
  note = {Face detection;},
  abstract = {In this paper a new algorithm for joint detection and segmentation
	of human faces in color images sequences is presented. A skin probability
	image is generated using a model for skin color. Instead of a binary
	segmentation to detect skin regions, connected operators are used
	to analyze the skin probability image at different threshold levels.
	A hierarchical scheme of operators using shape and texture simplifies
	the skin probability image. For the remaining connected components,
	the likelihood of being a face is estimated using principal components
	analysis. To track a detected face region through the sequence, the
	connected component that represent the face in the previous frame
	is projected into the current frame. Using the projected segment
	as a marker, connected operators extract the actual face region from
	the skin probability image.},
  comment = {trk_color},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2000 JNL, Segmentation and tracking of facial regions in color image sequences (Menser, Wien).pdf:PDF},
  issn = {0277786X},
  journal = {Proceedings of SPIE - The International Society for Optical Engineering},
  key = {Color image processing},
  keywords = {Feature extraction;Image analysis;Image segmentation;},
  language = {English},
  timestamp = {00,030}
}

@ARTICLE{1971_JNL_QuantMaxEntropy_Messerschmitt,
  author = {Messerschmitt, D.},
  title = {Quantizing for maximum output entropy (Corresp.)},
  journal = {Information Theory, IEEE Transactions on},
  year = {1971},
  volume = {17},
  pages = { 612 - 612},
  number = {5},
  abstract = { The entropy at the output of a quantizer is equal to the average
	mutual information between unquantized and quantized random variables.
	Thus, for a fixed number of quantization levels, output entropy is
	a reasonable information-theoretic criterion of quantizer fidelity.
	It is shown that, for a class of signal distributions, which includes
	the Gaussian, the quantizers with maximum output entropy (MOE) and
	minimum average error (MAE) are approximately the same within a multiplicative
	constant.},
  comment = {IT},
  file = {:papers\\1971 JNL, Quantizing for maximum output entropy (Messerschmitt, TIT, 7).pdf:PDF},
  issn = {0018-9448},
  keywords = { Entropy functions; Quantization (signal); Signal quantization;},
  timestamp = {-}
}

@ARTICLE{1993_JNL_MEphysics_MetaxasTerzopoulos,
  author = {Metaxas, D. and Terzopoulos, D.},
  title = {Shape and nonrigid motion estimation through physics-based synthesis},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1993},
  volume = {15},
  pages = {580-591},
  number = {6},
  comment = {motion},
  file = {:papers\\1993 JNL, Shape and Non-Rigid Motion Estimation through Physics-Based Synthesis (Mataxas, Terzopoulos, PAMI, 334).pdf:PDF},
  keywords = {Kalman filters computer animation filtering and prediction theory
	matrix algebra motion estimation state estimation 3D shape estimation
	Lagrange equations of motion Riccati procedure articulated objects
	continuous Kalman filtering theory covariance matrix dynamic models
	flexible objects geometric primitives incomplete information instantaneous
	uncertainties model fitting nonrigid motion estimation physics-based
	synthesis tracking},
  owner = {salman},
  timestamp = {00,300}
}

@ARTICLE{8484612,
  author = {Metta, G. and Gasteratos, A. and Sandini, G.8484612},
  title = {Learning to track colored objects with log-polar vision},
  journal = {Mechatronics},
  year = {2004},
  volume = { 14},
  pages = {989 - 1006},
  number = { 9},
  note = {colored object tracking;log polar vision;space variant vision;color
	segmentation technique;5 degree of freedom robotic head;5 dof robotic
	head;color information;closed loop controller;open loop controller;self-supervised
	learning;motion control;},
  abstract = {An approach bringing together space-variant vision through a simple
	color segmentation technique and learning is presented. The proposed
	approach is employed to control the movement of a 5 degree of freedom
	(d.o.f.) robotic head. Color information is used to determine the
	position of the object of interest in the image plane and, consequently,
	to track it during its motion. The distance of the target from the
	center of the image is used to feed both a closed-loop and an open-loop
	controller. Most important, the parameters of the controllers are
	learnt on-line in a self-supervised fashion. Experiments are presented
	to demonstrate empirically the feasibility of the approach and its
	application to a real world control problem},
  address = {UK},
  comment = {trk_color},
  copyright = {Copyright 2005, IEE},
  file = {:papers\\2004 JNL, Learning to track colored objects with log-polar vision (Metta, Gasteratos, Sandini).pdf:PDF},
  issn = {0957-4158},
  keywords = {closed loop systems;image colour analysis;image motion analysis;image
	segmentation;learning (artificial intelligence);mobile robots;motion
	control;open loop systems;robot vision;},
  language = {English},
  timestamp = {00,020},
  url = {http://dx.doi.org/10.1016/j.mechatronics.2004.05.003}
}

@ARTICLE{2005_JNL_EvaluationLocalDescriptors_Mikolajczyk,
  author = {Mikolajczyk, K. and Schmid, C.},
  title = {A performance evaluation of local descriptors},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2005},
  volume = {27},
  pages = {1615-1630},
  number = {10},
  abstract = {In this paper, we compare the performance of descriptors computed
	for local interest regions, as, for example, extracted by the Harris-Affine
	detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors
	have been proposed in the literature. It is unclear which descriptors
	are more appropriate and how their performance depends on the interest
	region detector. The descriptors should be distinctive and at the
	same time robust to changes in viewing conditions as well as to errors
	of the detector. Our evaluation uses as criterion recall with respect
	to precision and is carried out for different image transformations.
	We compare shape context [Belongie, S, et al., April 2002], steerable
	filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y
	and Sukthankar, R, 2004], differential invariants [Koenderink, J
	and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003],
	SIFT [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman,
	A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation
	for different types of interest regions. We also propose an extension
	of the SIFT descriptor and show that it outperforms the original
	method. Furthermore, we observe that the ranking of the descriptors
	is mostly independent of the interest region detector and that the
	SIFT-based descriptors perform best. Moments and steerable filters
	show the best performance among the low dimensional descriptors.},
  comment = {feature},
  file = {:papers\\2005 JNL, A performance evaluation of local descriptors (Mikolajczyk, Schmid, PAMI, 1703).pdf:PDF},
  keywords = {correlation methods filtering theory image classification image matching
	complex filters cross-correlation image transformations local descriptors
	moment invariants performance evaluation spin images steerable filters
	Algorithms Artificial Intelligence Computer Simulation Data Interpretation,
	Statistical Image Enhancement Image Interpretation, Computer-Assisted
	Information Storage and Retrieval Models, Statistical Pattern Recognition,
	Automated Software Software Validation},
  owner = {salman},
  timestamp = {01,700}
}

@ARTICLE{2004_JNL_SIFT_Mikolajczyk,
  author = {Mikolajczyk, Krystian and Schmid, Cordelia},
  title = {Scale \& Affine Invariant Interest Point Detectors},
  journal = {International Journal of Computer Vision},
  year = {2004},
  volume = {60},
  pages = {63-86},
  number = {1},
  abstract = {In this paper we propose a novel approach for detecting interest points
	invariant to scale and affine transformations. Our scale and affine
	invariant detectors are based on the following recent results: (1)
	Interest points extracted with the Harris detector can be adapted
	to affine transformations and give repeatable results (geometrically
	stable). (2) The characteristic scale of a local structure is indicated
	by a local extremum over scale of normalized derivatives (the Laplacian).
	(3) The affine shape of a point neighborhood is estimated based on
	the second moment matrix.},
  comment = {feature},
  file = {:papers\\2004 JNL, Scale Affine Invariant Interest Point Detectors (Mikolajczyk, Schmid, IJCV, 1545).pdf:PDF},
  timestamp = {01,500}
}

@INPROCEEDINGS{2002_CNF_AffineInvariantInterestPointDetector_MikolajczykSchmid,
  author = {Mikolajczyk, K. and Schmid, C.},
  title = {An affine invariant interest point detector},
  booktitle = {Computer Vision - ECCV 2002. 7th European Conference on Computer
	Vision. Proceedings, 28-31 May 2002},
  year = {2002},
  abstract = {This paper presents a novel approach for detecting affine invariant
	interest points. Our method can deal with significant affine transformations
	including large scale changes. Such transformations introduce significant
	changes in the point location as well as in the scale and shape of
	the neighbourhood of an interest point. Our approach allows one to
	solve for these problems simultaneously. It is based on three key
	ideas: 1) the second moment matrix computed in a point can be used
	to normalize a region in an affine invariant way (skew and stretch);
	2) the scale of the local structure is indicated by local extrema
	of normalized derivatives over scale; and 3) an affine-adapted Harris
	detector determines the location of interest points. A multi-scale
	version of this detector is used for initialization. An iterative
	algorithm then modifies location, scale and neighbourhood of each
	point and converges to affine invariant points. For matching and
	recognition, the image is characterized by a set of affine invariant
	points; the affine transformation associated with each point allows
	the computation of an affine invariant descriptor which is also invariant
	to affine illumination changes. A quantitative comparison of our
	detector with existing ones shows a significant improvement in the
	presence of large affine deformations. Experimental results for wide
	baseline matching show an excellent performance in the presence of
	large perspective transformations including significant scale changes.
	Results for recognition are very good for a database with more than
	5000 images},
  comment = {feature},
  file = {:papers\\2002 CNF, An Affine Invariant  Interest Point Detector (Mikolajczyk, Schmid, ECCV, 1516).pdf:PDF},
  keywords = {feature extraction image matching image recognition iterative methods
	method of moments},
  timestamp = {01,500}
}

@ARTICLE{2003_JNL_M2tracker_MittalLarrydavis,
  author = {Mittal, A. and Davis, L. S.},
  title = {M2Tracker: a multi-view approach to segmenting and tracking people
	in a cluttered scene},
  journal = {International Journal of Computer Vision},
  year = {2003},
  volume = {51},
  pages = {189-203},
  number = {Copyright 2003, IEE},
  abstract = {When occlusion is minimal, a single camera is generally sufficient
	to detect and track objects. However, when the density of objects
	is high, the resulting occlusion and lack of visibility suggests
	the use of multiple cameras and collaboration between them so that
	an object is detected using information available from all the cameras
	in the scene. In this paper, we present a system that is capable
	of segmenting, detecting and tracking multiple people in a cluttered
	scene using multiple synchronized surveillance cameras located far
	from each other. The system is fully automatic, and takes decisions
	about object detection and tracking using evidence collected from
	many pairs of cameras. Innovations that help us tackle the problem
	include a region-based stereo algorithm capable of finding 3D points
	inside an object knowing only the projections of the object (as a
	whole) in two views, a segmentation algorithm using Bayesian classification
	and the use of occlusion analysis to combine evidence from different
	camera pairs. The system has been tested using different densities
	of people in the scene. This helps us determine the number of cameras
	required for a particular density of people. Experiments have also
	been conducted to verify and quantify the efficacy of the occlusion
	analysis scheme},
  comment = {trk_people},
  file = {:papers\\2003 JNL, M2Tracker_ A Multi-View Approach to Segmenting and Tracking People in a Cluttered Scene (Mittal, Larrydavis, IJCV, 290).pdf:PDF},
  keywords = {Bayes methods computer vision image classification image segmentation
	object detection optical tracking stereo image processing},
  timestamp = {00,300}
}

@INPROCEEDINGS{2007_CNF_OpticalFlowCUDA_Mizukami,
  author = {Mizukami, Y. and Tadamura, K.},
  title = {Optical Flow Computation on Compute Unified Device Architecture},
  booktitle = {Image Analysis and Processing, 2007. ICIAP 2007. 14th International
	Conference on},
  year = {2007},
  pages = {179 -184},
  month = {10-14},
  abstract = {In this study, the implementation of an image processing technique
	on compute unified device architecture (CUDA) is discussed. CUDA
	is a new hardware and software architecture developed by NVIDIA Corporation
	for the general- purpose computation on graphics processing units.
	CUDA features an on-chip shared memory with very fast general read
	and write access, which enables threads in a block to share their
	data effectively. CUDA also provides a user- friendly development
	environment through an extension to the C programming language. This
	study focused on CUDA implementation of a representative optical
	flow computation proposed by Horn and Schunck in 1981. Their method
	produces the dense displacement field and has a straightforward processing
	procedure. A CUDA implementation of Horn and Schunck's method is
	proposed and investigated based on simulation results.},
  comment = {GPU_CUDA},
  doi = {10.1109/ICIAP.2007.4362776},
  file = {:papers\\2007 CNF, Optical Flow Computation on Compute Unified Device Architecture (Mizukami, Tadamura).pdf:PDF},
  keywords = {C programming language;CUDA;compute unified device architecture;graphics
	processing units;hardware architecture;image processing technique;on-chip
	shared memory;optical flow computation;software architecture;digital
	signal processing chips;image sequences;software architecture;},
  timestamp = {00,015}
}

@ARTICLE{2007_JNL_TRKsurvey_Mocanu,
  author = {Mocanu, I.},
  title = {Image Retrieval by Shape Based on Contour Techniques A Comparative
	Study},
  journal = {Applied Computational Intelligence and Informatics, 2007. SACI '07.
	4th International Symposium on},
  year = {2007},
  pages = {219-223},
  month = {17 2007-May 18},
  abstract = {In content-based image retrieval shape is an important low-level image
	feature. A shape representation should satisfy several properties
	such as affine invariance, robustness, compactness, low computation
	complexity and perceptual similarity measurement. This paper presents
	several approaches for shape representation and similarity, like
	Fourier descriptors-based method, turning angle method, centroid-radii
	method, distance histograms method and centroid-radii and turning
	angle method. The comparison results are presented, too.},
  comment = {survey},
  doi = {10.1109/SACI.2007.375514},
  keywords = {Fourier transforms, content-based retrieval, image representation,
	image retrievalFourier descriptor, centroid-radii method, content-based
	image retrieval, contour technique, distance histogram, shape representation,
	turning angle method}
}

@TECHREPORT{1999_REP_SURVEYmotion_Moeslund,
  author = {Moeslund, Thomas B.},
  title = {Summaries of 107 Computer Vision-Based Human Motion Capture Papers},
  institution = {University of Aalborg},
  year = {1999},
  comment = {survey},
  file = {:papers\\1999 JNL, Summaries of 107 Computer Vision-Based Human Motion Capture Papers (SURVEY, UnivTechReport, 26).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{2001_JNL_SURVEYmotion_Moeslund,
  author = {Moeslund, T. B. and Granum, E.},
  title = {A survey of computer vision-based human motion capture},
  journal = {Computer Vision and Image Understanding},
  year = {2001},
  volume = {81},
  pages = {231-68},
  abstract = {A comprehensive survey of computer vision-based human motion capture
	literature from the past two decades is presented. The focus is on
	a general overview based on a taxonomy of system functionalities,
	broken down into four processes: initialization, tracking, pose estimation,
	and recognition. Each process is discussed and divided into subprocesses
	and/or categories of methods to provide a reference to describe and
	compare the more than 130 publications covered by the survey. References
	are included throughout the paper to exemplify important issues and
	their relations to the various methods. A number of general assumptions
	used in this research field are identified and the character of these
	assumptions indicates that the research field is still in an early
	stage of development. To evaluate the state of the art, the major
	application areas are identified and performances are analyzed in
	light of the methods presented in the survey. Finally, suggestions
	for future research directions are offered},
  comment = {survey},
  file = {:papers\\2001 JNL, A Survey of Computer Vision-Based Human Motion Capture (SURVEY, Moeslund, Granum, CVIU, 797).pdf:PDF},
  keywords = {computer vision motion estimation},
  owner = {salman},
  review = {nice table of previous work},
  timestamp = {00,800}
}

@ARTICLE{2006_JNL_HumanMotion_Moeslund,
  author = {Moeslund, Thomas B. and Hilton, Adrian and Kruger, Volker},
  title = {A survey of advances in vision-based human motion capture and analysis},
  journal = {Computer Vision and Image Understanding},
  year = {2006},
  volume = {104},
  pages = {90-126},
  abstract = {This survey reviews advances in human motion capture and analysis
	from 2000 to 2006, following a previous survey of papers up to 2000
	[T.B. Moeslund, E. Granum, A survey of computer vision-based human
	motion capture, Computer Vision and Image Understanding, 81(3) (2001)
	231-268.]. Human motion capture continues to be an increasingly active
	research area in computer vision with over 350 publications over
	this period. A number of significant research advances are identified
	together with novel methodologies for automatic initialization, tracking,
	pose estimation, and movement recognition. Recent research has addressed
	reliable tracking and pose estimation in natural scenes. Progress
	has also been made towards automatic understanding of human actions
	and behavior. This survey reviews recent trends in video-based human
	capture and analysis, as well as discussing open problems for future
	research to achieve automatic visual analysis of human movement.
	2006 Elsevier Inc. All rights reserved.},
  comment = {survey},
  file = {:papers\\2006 JNL, A survey of advances in vision-based human motion capture and analysis (SURVEY, Kruger, CVIU, 376).pdf:PDF},
  keywords = {Man machine systems Computer vision Data acquisition Surveying Vision
	Visualization},
  owner = {salman},
  review = {nice table of previous work},
  timestamp = {00,380}
}

@ARTICLE{1997_JNL_EigenTRK_Moghaddam,
  author = {Moghaddam, B. and Pentland, A.},
  title = {Probabilistic visual learning for object representation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1997},
  volume = {19},
  pages = {696-710},
  number = {7},
  abstract = {We present an unsupervised technique for visual learning, which is
	based on density estimation in high-dimensional spaces using an eigenspace
	decomposition. Two types of density estimates are derived for modeling
	the training data: a multivariate Gaussian (for unimodal distributions)
	and a mixture-of-Gaussians model (for multimodal distributions).
	Those probability densities are then used to formulate a maximum-likelihood
	estimation framework for visual search and target detection for automatic
	object recognition and coding. Our learning technique is applied
	to the probabilistic visual modeling, detection, recognition, and
	coding of human faces and nonrigid objects, such as hands},
  comment = {TRK_subspace},
  file = {:papers\\1997 JNL, Probabilistic visual learning for object representation (Moghaddam, Pentland, PAMI, 1163).pdf:PDF},
  keywords = {Gaussian distribution image coding image recognition maximum likelihood
	estimation object recognition probability unsupervised learning density
	estimation eigenspace decomposition high-dimensional spaces human
	faces human hands maximum-likelihood estimation framework mixture-of-Gaussians
	model multivariate Gaussian model nonrigid objects object coding
	object representation probabilistic visual learning probability densities
	target detection unsupervised technique visual search},
  owner = {salman},
  timestamp = {01,200}
}

@ARTICLE{2001_JNL_TRK_Mohan,
  author = {Mohan, A. and Papageorgiou, C. and Poggio, T.},
  title = {Example-based object detection in images by components},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2001},
  volume = {23},
  pages = {349--361},
  number = {4},
  __markedentry = {[salman]},
  comment = {TRK_ped},
  doi = {10.1109/34.917571},
  file = {:papers\\2001 JNL, Example-Based Object Detection in Images by Components (Mohan).pdf:PDF},
  owner = {salman},
  timestamp = {00,550}
}

@INPROCEEDINGS{2003_CNF_BGsubtractionDynamicScenes_Monnet,
  author = {Monnet, A. and Mittal, A. and Paragios, N. and Visvanathan, Ramesh},
  title = {Background modeling and subtraction of dynamic scenes},
  booktitle = {Computer Vision, 2003. Proceedings. Ninth IEEE International Conference
	on},
  year = {2003},
  pages = {1305-1312 vol.2},
  abstract = {Background modeling and subtraction is a core component in motion
	analysis. The central idea behind such module is to create a probabilistic
	representation of the static scene that is compared with the current
	input to perform subtraction. Such approach is efficient when the
	scene to be modeled refers to a static structure with limited perturbation.
	In this paper, we address the problem of modeling dynamic scenes
	where the assumption of a static background is not valid. Waving
	trees, beaches, escalators, natural scenes with rain or snow are
	examples. Inspired by the work proposed by Doretto et al. (2003),
	we propose an on-line auto-regressive model to capture and predict
	the behavior of such scenes. Towards detection of events we introduce
	a new metric that is based on a state-driven comparison between the
	prediction and the actual frame. Promising results demonstrate the
	potentials of the proposed framework.},
  comment = {trk_BG},
  file = {:papers\\2003 CNF, Background modeling and subtraction of dynamic scenes (Monnet, Mittal, Paragios, Ramesh, ICCV, 165).pdf:PDF},
  keywords = {computer vision feature extraction image motion analysis image representation
	natural scenes object detection probability background modeling background
	subtraction dynamic scenes image detection motion analysis on-line
	auto-regressive model perturbation probabilistic representation real-time
	video analysis scene modeling state-driven comparison static background
	static scene static structure waving trees},
  owner = {salman},
  timestamp = {00,170}
}

@BOOK{1999_BOOK_MathematicalMethods_MoonStirling,
  title = {Mathematical Methods and Algorithms for Signal Processing},
  publisher = {Prentice Hall},
  year = {1999},
  author = {Moon, Todd K. and Stirling, Wynn C.},
  month = {August},
  comment = {SP_book},
  day = {14},
  file = {:papers\\1999 BOOK, Mathematical Methods and Algorithms for Signal Processing (Moon, Stirling, Prentice Hall).pdf:PDF},
  howpublished = {Paperback},
  isbn = {0201361868},
  posted-at = {2010-04-15 15:35:33},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0201361868}
}

@INPROCEEDINGS{2007_CNF_SURVEY_CG_Moore,
  author = {Moore, P. and Molloy, D.},
  title = {A Survey of Computer-Based Deformable Models},
  booktitle = {Machine Vision and Image Processing Conference, 2007. IMVIP 2007.
	International},
  year = {2007},
  pages = {55 -66},
  month = sept.,
  abstract = {This paper presents a survey of the research carried out to date in
	the area of computer-based deformable modelling. Due to their cross-disciplinary
	nature, deformable modelling techniques have been the subject of
	vigorous research over the past three decades and have found numerous
	applications in the fields of machine vision (image analysis, image
	segmentation, image matching, and motion tracking), visualisation
	(shape representation and data fitting), and computer graphics (shape
	modelling, simulation, and animation). Previous review papers have
	been field/application specific and have therefore been limited in
	their coverage of techniques. This survey focuses on general deformable
	models for computer-based modelling, which can be used for computer
	graphics, visualisation, and various image processing applications.
	The paper organizes the various approaches by technique and provides
	a description, critique, and overview of applications for each. Finally,
	the state of the art of deformable modelling is discussed, and areas
	of importance for future research are suggested.},
  comment = {survey},
  doi = {10.1109/IMVIP.2007.31},
  file = {:C\:\\salman\\work\\writing\\papers\\2007 CNF, A Survey of Computer-Based Deformable Models (Moore).pdf:PDF},
  keywords = {computer graphics;computer-based deformable models;cross-disciplinary
	nature;data fitting;image analysis;image matching;image segmentation;machine
	vision;motion tracking;shape modelling;shape representation;computer
	vision;data visualisation;image representation;image segmentation;},
  timestamp = {?}
}

@INPROCEEDINGS{1979_CNF_VisualMappingRobotRover_Moravec,
  author = {Moravec, H.},
  title = {Visual mapping by a robot rover},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence
	(IJCAI)},
  year = {1979},
  comment = {?},
  owner = {salman},
  timestamp = {00,180}
}

@ARTICLE{9531954,
  author = {Moreno-Noguer, F. and Sanfeliu, A. and Samaras, D.},
  title = {Integration of deformable contours and a multiple hypotheses Fisher
	color model for robust tracking in varying illuminant environments},
  journal = {Image and Vision Computing},
  year = {2007},
  volume = { 25},
  pages = {285 - 96},
  number = { 3},
  note = {deformable contour;Fisher color model;robust tracking;illuminant
	environment;figure-ground segmentation;image sequence;linear discriminant
	analysis;color distribution;},
  abstract = {In this paper, we propose a new technique to perform figure-ground
	segmentation in image sequences of moving objects under varying illumination
	conditions. Unlike most of the algorithms that adapt color, there
	is not the assumption of smooth change of the viewing conditions.
	To cope with this, we propose the use of a new colorspace that maximizes
	the foreground/background class separability based on the 'Linear
	Discriminant Analysis' method. Moreover, we introduce a technique
	that formulates multiple hypotheses about the next state of the color
	distribution (some of these hypotheses take into account small and
	gradual changes in the color model and others consider more abrupt
	and unexpected variations) and the hypothesis that generates the
	best object segmentation is used to remove noisy edges from the image.
	This simplifies considerably the final step of fitting a deformable
	contour to the object boundary, thus allowing a standard snake formulation
	to successfully track non-rigid contours. In the same manner, the
	contour estimate is used to correct the color model. The integration
	of color and shape is done in a stage called 'sample concentration',
	introduced as a final step to the well-known CONDENSATION algorithm
	[All rights reserved Elsevier].},
  address = {Netherlands},
  comment = {trk_color},
  copyright = {Copyright 2007, The Institution of Engineering and Technology},
  file = {:papers\\2007 JNL, Integration of deformable contours and a multiple hypotheses Fisher color model for robust tracking in varying illuminant environments (Noguer, Sanfeliu, Samaras).pdf:PDF},
  issn = {0262-8856},
  keywords = {image colour analysis;image denoising;image segmentation;image sequences;tracking;},
  language = {English},
  timestamp = {00,010},
  url = {http://dx.doi.org/10.1016/j.imavis.2005.10.008}
}

@ARTICLE{10316133,
  author = {Morioka, K. and Joo-Ho Lee and Kuroda, Y. and Hashimoto, H.},
  title = {Hybrid tracking based on color histogram for intelligent space},
  journal = {Artificial Life and Robotics},
  year = {2007},
  volume = { 11},
  pages = {204 - 10},
  number = { 2},
  note = {hybrid tracking algorithm;color histogram;intelligent space;vision
	sensor network;contact-free wide-area location system;modified color
	tracker;Kalman filter;mean shift procedure;adaptive feedback loop;},
  abstract = {The vision sensor network is expected to achieve a contact-free wide-area
	location system without any additional burden on users in intelligent
	environments. In this article, a tracking algorithm for a location
	system in an intelligent environment is described. A modified color
	tracker based on a Kalman filter and a mean shift procedure is proposed
	in order to improve the robustness for occlusion and rapid movement.
	To handle the sudden change in object movement, we propose a hybrid
	tracking algorithm, including an adaptive feedback loop, based on
	the statistics of color histogram models after the mean-shift process.
	Experimental results showed that the proposed method achieves more
	robust tracking of multiple objects than the conventional method.},
  address = {Japan},
  comment = {trk_color},
  copyright = {Copyright 2008, The Institution of Engineering and Technology},
  file = {:papers\\2007 JNL, Hybrid tracking based on color histogram for intelligent space (Morioka, Lee, Kuroda, Hashimoto).pdf:PDF},
  issn = {1433-5298},
  keywords = {distributed sensors;feedback;image colour analysis;image sensors;Kalman
	filters;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1007/s10015-007-0429-9}
}

@INPROCEEDINGS{1998_CNF_SingularityAnalysis_Rehg,
  author = {Morris, D. D. and Rehg, J. M.},
  title = {Singularity analysis for articulated object tracking},
  booktitle = {Computer Vision and Pattern Recognition, 1998. Proceedings. 1998
	IEEE Computer Society Conference on},
  year = {1998},
  pages = {289-296},
  abstract = {We analyze the use of kinematic constraints for articulated object
	tracking. Conditions for the occurrence of singularities in 3-D models
	are presented and their effects on tracking are characterized We
	describe a novel 2-D Scaled Prismatic Model (SPM) for figure registration.
	In contrast to 3-D kinematic models, the SPM has fewer singularity
	problems and does not require detailed knowledge of the 3-D kinematics.
	We fully characterize the singularities in the SPM and illustrate
	tracking through singularities using synthetic and real examples
	with 3-D and 2-D models. Our results demonstrate the significant
	benefits of the SPM in tracking with a single source of video},
  comment = {trk},
  file = {:papers\\1998 CNF, Singularity analysis for articulated object tracking (Morris, Rehg, CVPR, 133).pdf:PDF},
  keywords = {computer vision kinematics object recognition 3-D models articulated
	object tracking figure registration kinematic constraints scaled
	prismatic model singularity analysis},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{1989_JNL_PiecewiseSmoothFunctions_MumfordShah,
  author = {Mumford, D. and Shah, J.},
  title = {Optimal approximations by piecewise smooth functions and associated
	variational problems},
  journal = {Communications on Pure and Applied Mathematics},
  year = {1989},
  volume = {42},
  pages = {577-685},
  number = {5},
  abstract = {No Abstract.},
  comment = {feature_contour},
  file = {:papers\\1989 JNL, Optimal approximations by piecewise smooth functions and variational (Mumford, Shah, CPAM, 1989).pdf:PDF},
  keywords = {approximation new_1},
  timestamp = {02,000}
}

@ARTICLE{1995_JNL_ImgManifolds_Murase,
  author = {Murase, Hiroshi and Nayar, Shree K.},
  title = {Visual learning and recognition of 3-d objects from appearance},
  journal = {International Journal of Computer Vision},
  year = {1995},
  volume = {14},
  pages = {5-24},
  abstract = {The problem of automatically learning object models for recognition
	and pose estimation is addressed. In contrast to the traditional
	approach, the recognition problem is formulated as one of matching
	appearance rather than shape. The appearance of an object in a two-dimensional
	image depends on its shape, reflectance properties, pose in the scene,
	and the illumination conditions. While shape and reflectance are
	intrinsic properties and constant for a rigid object, pose and illumination
	vary from scene to scene. A compact representation of object appearance
	is proposed that is parametrized by pose and illumination. For each
	object of interest, a large set of images is obtained by automatically
	varying pose and illumination. This image set is compressed to obtain
	a low-dimensional subspace, called the eigenspace, in which the object
	is represented as a manifold. Given an unknown input image, the recognition
	system projects the image to eigenspace. The object is recognized
	based on the manifold it lies on. The exact position of the projection
	on the manifold determines the object's pose in the image.},
  affiliation = {NTT Basic Research Laboratory Atsugi-Shi 243-01 Kanagawa Japan},
  comment = {TRK_subspace},
  file = {:papers\\1995 JNL, Visual learning and recognition of 3-d objects from appearance (Murase).pdf:PDF},
  issn = {0920-5691},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Netherlands},
  timestamp = {01,800}
}

@ARTICLE{1968_JNL_RankingAssignments_Murty,
  author = {Murty, K.},
  title = {An algorithm for ranking all the assignments in order of increasing
	cost.},
  journal = {Operations Research},
  year = {1968},
  volume = {16},
  pages = {682-686},
  comment = {trk_correspondence},
  timestamp = {00,170}
}

@ARTICLE{2006_JNL_LearningPCA_Nagabhush,
  author = {P. Nagabhushan and D.S. Guru and B.H. Shekar},
  title = {Visual learning and recognition of 3D objects using two-dimensional
	principal component analysis: A robust and an efficient approach},
  journal = {Pattern Recognition},
  year = {2006},
  volume = {39},
  pages = {721 - 725},
  number = {4},
  abstract = {Inspired by the conviction that the successful model employed for
	face recognition [M. Turk, A. Pentland, Eigenfaces for recognition,
	J. Cogn. Neurosci. 3(1) (1991) 71-86] should be extendable for object
	recognition [H. Murase, S.K. Nayar, Visual learning and recognition
	of 3-D objects from appearance, International J. Comput. Vis. 14(1)
	(1995) 5-24], in this paper, a new technique called two-dimensional
	principal component analysis (2D-PCA) [J. Yang et al., Two-dimensional
	PCA: a new approach to appearance based face representation and recognition,
	IEEE Trans. Patt. Anal. Mach. Intell. 26(1) (2004) 131-137] is explored
	for 3D object representation and recognition. 2D-PCA is based on
	2D image matrices rather than 1D vectors so that the image matrix
	need not be transformed into a vector prior to feature extraction.
	Image covariance matrix is directly computed using the original image
	matrices, and its eigenvectors are derived for feature extraction.
	The experimental results indicate that the 2D-PCA is computationally
	more efficient than conventional PCA (1D-PCA) [H. Murase, S.K. Nayar,
	Visual learning and recognition of 3-D objects from appearance, International
	J. Comput. Vis. 14(1) (1995) 5-24]. It is also revealed through experimentation
	that the proposed method is more robust to noise and occlusion.},
  comment = {TRK_subspace},
  file = {:papers\\2006 JNL, Visual learning and recognition of 3D objects using two-dimensional principal component analysis_ A robust and an efficient approach (Nagabhushan).pdf:PDF},
  issn = {0031-3203},
  timestamp = {-}
}

@ARTICLE{1988_JNL_ImageVQ_Nasrabadi,
  author = {Nasrabadi, N.M. and King, R.A.},
  title = {Image coding using vector quantization: a review},
  journal = {Communications, IEEE Transactions on},
  year = {1988},
  volume = {36},
  pages = {957 -971},
  number = {8},
  month = {aug},
  abstract = {A review of vector quantization techniques used for encoding digital
	images is presented. First, the concept of vector quantization is
	introduced, then its application to digital images is explained.
	Spatial, predictive, transform, hybrid, binary, and subband vector
	quantizers are reviewed. The emphasis is on the usefulness of the
	vector quantization when it is combined with conventional image coding
	techniques, or when it is used in different domains},
  comment = {survey, VQ},
  doi = {10.1109/26.3776},
  file = {:papers\\1988 JNL, Image coding using vector quantization_ a review (Nasrabadi, King).pdf:PDF},
  issn = {0090-6778},
  keywords = {binary vector quantiser;digital images;encoding;hybrid vector quantiser;image
	coding;predictive vector quantiser;review;spatial vector quantiser;subband
	vector quantizers;transform vector quantiser;vector quantization;vector
	quantization techniques;encoding;picture processing;reviews;},
  timestamp = {00,750}
}

@ARTICLE{2008_JNL_HumanActionWords_Niebles,
  author = {Niebles, Juan Carlos and Wang, Hongcheng and Fei-Fei, Li},
  title = {Unsupervised learning of human action categories using spatial-temporal
	words},
  journal = {International Journal of Computer Vision},
  year = {2008},
  volume = {79},
  pages = {299-318},
  number = {3},
  abstract = {We present a novel unsupervised learning method for human action categories.
	A video sequence is represented as a collection of spatial-temporal
	words by extracting space-time interest points. The algorithm automatically
	learns the probability distributions of the spatial-temporal words
	and the intermediate topics corresponding to human action categories.
	This is achieved by using latent topic models such as the probabilistic
	Latent Semantic Analysis (pLSA) model and Latent Dirichlet Allocation
	(LDA). Our approach can handle noisy feature points arisen from dynamic
	background and moving cameras due to the application of the probabilistic
	models. Given a novel video sequence, the algorithm can categorize
	and localize the human action(s) contained in the video. We test
	our algorithm on three challenging datasets: the KTH human motion
	dataset, the Weizmann human action dataset, and a recent dataset
	of figure skating actions. Our results reflect the promise of such
	a simple approach. In addition, our algorithm can recognize and localize
	multiple actions in long and complex video sequences containing multiple
	motions. 2008 Springer Science+Business Media, LLC.},
  comment = {recog_action_interestPoint},
  file = {:papers\\2008 JNL, Unsupervised learning of human action categories using spatial-temporal words (Fei, ICJV, 210).pdf:PDF},
  keywords = {Unsupervised learning Algorithms Probability distributions Video streaming
	Word processing},
  owner = {salman},
  timestamp = {00,210}
}

@INPROCEEDINGS{2004_CNF_SURVEYgait_Nixon,
  author = {Nixon, Mark S. and Carter, John N.},
  title = {Advances in automatic gait recognition},
  booktitle = {Proceedings - Sixth IEEE International Conference on Automatic Face
	and Gesture Recognition FGR},
  year = {2004},
  abstract = {Automatic recognition by gait is subject to increasing interest and
	has the unique capability to recognize people at a distance when
	other biometrics are obscured. Its interest is reinforced by the
	longstanding computer vision interest in automated non-invasive analysis
	of human motion. Its recognition capability is supported by studies
	in other domains such as medicine (biomechanics), mathematics and
	psychology which continue to suggest that gait is unique. Further,
	examples of recognition by gait can be found in literature, with
	early reference by Shakespeare concerning recognition by the way
	people walk. Current approaches confirm the early results that suggested
	gait could be used for identification, and now on much larger databases.
	This has been especially influenced by the Human ID at a Distance
	research program with its wide scenario of data and approaches. Gait
	has benefited from the developments in other biometrics and has led
	to new insight particularly in view of covariates. As such, gait
	is an interesting research area, with contributions not only to the
	field of biometrics but also to the stock of new techniques for the
	extraction and description of objects moving within image sequences.},
  comment = {survey, TRK_gait},
  file = {:papers\\2004 CNF, Advances in automatic gait recognition (SURVEY, Carter, AFGR, 46).pdf:PDF},
  keywords = {Biomechanics Computer vision Data reduction Database systems Eigenvalues
	and eigenfunctions Mathematical models Motion estimation},
  owner = {salman},
  timestamp = {00,050}
}

@INPROCEEDINGS{1994_CNF_WalkingFiguresXYT_Niyogi,
  author = {Niyogi, S.A. and Adelson, E.H.},
  title = {Analyzing and recognizing walking figures in XYT},
  booktitle = {Computer Vision and Pattern Recognition, 1994. Proceedings CVPR '94.,
	1994 IEEE Computer Society Conference on},
  year = {1994},
  pages = {469 -474},
  month = {jun},
  abstract = {We describe a novel algorithm for gait analysis. A person walking
	frontoparallel to the image plane generates a characteristic ldquo;braided
	rdquo; pattern in a spatiotemporal (XYT) volume. Our algorithm detects
	this pattern, and fits it with a set of spatiotemporal snakes. The
	snakes can be used to find the bounding contours of the walker. The
	contours vary over time in a manner characteristic of each walker.
	Individual gaits can be recognized by applying standard pattern recognition
	techniques to the contour signals},
  comment = {recog_action, TRK_gait},
  doi = {10.1109/CVPR.1994.323868},
  file = {:papers\\1994 CNF, Analyzing and recognizing walking figures in XYT (Niyogi, Adelson, CVPR, 419).pdf:PDF},
  keywords = { XYT; braided pattern; contour signals; gait analysis; pattern recognition;
	spatiotemporal snakes; spatiotemporal volume; walking figures; biomechanics;
	computer vision; image recognition;},
  timestamp = {00,400}
}

@MISC{2010_WHITE_Fermi_Nvidia,
  author = {Nvidia},
  title = {NVIDIAs Next Generation CUDA Compute Architecture: Fermi},
  howpublished = {Nvidia},
  year = {2010},
  comment = {3D_stereo},
  file = {:papers\\2010 WHITE, NVIDIAs Next Generation CUDA Compute Architecture_ Fermi (Nvidia).pdf:PDF},
  timestamp = {-},
  url = {http://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf}
}

@ARTICLE{1995_JNL_ImagesVQ_Oehler,
  author = {Oehler, K.L. and Gray, R.M.},
  title = {Combining image compression and classification using vector quantization},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1995},
  volume = {17},
  pages = {461 -473},
  number = {5},
  abstract = {We describe a method of combining classification and compression into
	a single vector quantizer by incorporating a Bayes risk term into
	the distortion measure used in the quantizer design algorithm. Once
	trained, the quantizer can operate to minimize the Bayes risk weighted
	distortion measure if there is a model providing the required posterior
	probabilities, or it can operate in a suboptimal fashion by minimizing
	the squared error only. Comparisons are made with other vector quantizer
	based classifiers, including the independent design of quantization
	and minimum Bayes risk classification and Kohonen's LVQ. A variety
	of examples demonstrate that the proposed method can provide classification
	ability close to or superior to learning VQ while simultaneously
	providing superior compression performance},
  comment = {VQ},
  doi = {10.1109/34.391396},
  file = {:papers\\1995 JNL, Combining image compression and classification using vector quantization (Oehler, Gray).pdf:PDF},
  issn = {0162-8828},
  keywords = {Bayes risk classification;Kohonen LVQ;image classification;image coding;image
	compression;posterior probability;squared error;statistical clustering;vector
	quantization;weighted distortion measure;Bayes methods;image classification;image
	coding;statistical analysis;vector quantisation;},
  timestamp = {00,100}
}

@INPROCEEDINGS{2004_CNF_BoostedPFMTT_Okuma,
  author = {Okuma, Kenji and Taleghani, Ali and Freitas, Nando and Little, James
	J. and Lowe, David G.},
  title = {A Boosted Particle Filter: Multitarget Detection and Tracking},
  booktitle = {Computer Vision - ECCV 2004 },
  year = {2004},
  pages = {28--39},
  abstract = {The problem of tracking a varying number of non-rigid objects has
	two major difficulties. First, the observation models and target
	distributions can be highly non-linear and non-Gaussian. Second,
	the presence of a large, varying number of objects creates complex
	interactions with overlap and ambiguities. To surmount these difficulties,
	we introduce a vision system that is capable of learning, detecting
	and tracking the objects of interest. The system is demonstrated
	in the context of tracking hockey players using video sequences.
	Our approach combines the strengths of two successful algorithms:
	mixture particle filters and Adaboost. The mixture particle filter[17]
	is ideally suited to multi-target tracking as it assigns a mixture
	component to each player. The crucial design issues in mixture particle
	filters are the choice of the proposal distribution and the treatment
	of objects leaving and entering the scene. Here, we construct the
	proposal distribution using a mixture model that incorporates information
	from the dynamic models of each player and the detection hypotheses
	generated by Adaboost. The learned Adaboost proposal distribution
	allows us to quickly detect players entering the scene, while the
	filtering process enables us to keep track of the individual players.
	The result of interleaving Adaboost with mixture particle filters
	is a simple, yet powerful and fully automatic multiple object tracking
	system.},
  citeulike-article-id = {6964734},
  citeulike-linkout-0 = {http://www.springerlink.com/content/wyf1nw3xw53xjnf3},
  comment = {trk_people},
  file = {:papers\\2004 CNF, A boosted particle filter_ Multitarget detection and tracking (Okuma, Taleghani, deFreitas, Little, Low, ECCV, 361).pdf:PDF},
  posted-at = {2010-04-24 20:50:33},
  priority = {2},
  timestamp = {00,350},
  url = {http://www.springerlink.com/content/wyf1nw3xw53xjnf3}
}

@ARTICLE{2000_JNL_BayesianHumanInteractions_Oliver,
  author = {Oliver, N. M. and Rosario, B. and Pentland, A. P.},
  title = {A Bayesian computer vision system for modeling human interactions},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {831-843},
  number = {8},
  abstract = {We describe a real-time computer vision and machine learning system
	for modeling and recognizing human behaviors in a visual surveillance
	task. The system deals in particularly with detecting when interactions
	between people occur and classifying the type of interaction. Examples
	of interesting interaction behaviors include following another person,
	altering one's path to meet another, and so forth. Our system combines
	top-down with bottom-up information in a closed feedback loop, with
	both components employing a statistical Bayesian approach. We propose
	and compare two different state-based learning architectures, namely,
	HMMs and CHMMs for modeling behaviors and interactions. Finally,
	a synthetic &ldquo;Alife-style&rdquo; training system is used to
	develop flexible prior models for recognizing human interactions.
	We demonstrate the ability to use these a priori models to accurately
	classify real human behaviors and interactions with no additional
	tuning or training},
  comment = {trk_people},
  file = {:papers\\2000 JNL, A Bayesian computer vision system for modeling human interactions (Oliver, Rosario, Pentland, PAMI, 632).pdf:PDF},
  keywords = {Bayes methods computer vision hidden Markov models image segmentation
	learning systems object recognition real-time systems surveillance
	Bayes method hidden Markov model human behavior recognition machine
	learning pattern recognition people detection visual surveillance},
  owner = {salman},
  timestamp = {00,600}
}

@ARTICLE{2001_JNL_Future3Dvideo_Ollis,
  author = {Ollis, M. and Williamson, T.},
  title = {The future of 3D video},
  journal = {Computer},
  year = {2001},
  volume = {34},
  pages = {97 -99},
  number = {6},
  month = {jun},
  abstract = {The latest developments in 3D video promise to advance the medium
	from a passive experience to a fully interactive one. As 3D video
	makes this transition, the underlying technologies are maturing to
	the point of commercial viability. We've yet to achieve the seamless
	virtual reality experience of the Star Trek holodeck, but today's
	virtual environments indicate that it's only a matter of time until
	we do},
  comment = {3D_stereo},
  doi = {10.1109/2.953470},
  file = {:papers\\2001 JNL, The future of 3D video (Ollis, Williamson).pdf:PDF},
  issn = {0018-9162},
  keywords = {3D video;commercial viability;entertainment computing;future;interactive
	experience;technology maturation;virtual environments;virtual reality;entertainment;interactive
	video;technological forecasting;three-dimensional television;virtual
	reality;},
  timestamp = {-}
}

@INPROCEEDINGS{2000_CNF_MLtemplateMatching_Olson,
  author = {Olson, C. F.},
  title = {Maximum-likelihood template matching},
  booktitle = {Computer Vision and Pattern Recognition, 2000. Proceedings. IEEE
	Conference on},
  year = {2000},
  volume = {2},
  abstract = {In image matching applications such as tracking and stereo matching,
	it is common to use the sum-of-squared-differences (SSD) measure
	to determine the best match for an image template. However, this
	measure is sensitive to outliers and is not robust to template variations.
	We describe a robust measure and efficient search strategy for template
	matching with a binary or greyscale template using a maximum-likelihood
	formulation. In addition to subpixel localization and uncertainty
	estimation, these techniques allow optimal feature selection based
	on minimizing the localization uncertainty. We examine the use of
	these techniques for object recognition, stereo matching, feature
	selection, and tracking},
  comment = {trk_region},
  file = {:papers\\2000 CNF, Maximum-likelihood template matching (Olson, CVPR, 40).pdf:PDF},
  keywords = {image matching maximum likelihood detection feature selection image
	template maximum-likelihood formulation object recognition outliers
	search strategy stereo matching sum-of-squared-differences template
	matching tracking},
  owner = {salman},
  timestamp = {00,040}
}

@INPROCEEDINGS{2005_Misc_KalmanFilterComparison_Orderud,
  author = {Fredrik Orderud},
  title = {Comparison of Kalman Filter Estimation Approaches for State Space
	Models with Nonlinear Measurements},
  booktitle = {Scandinavian Conference on Simulation and Modeling},
  year = {2005},
  comment = {trk_filtering},
  file = {:papers\\2005 CNF,  Comparison of Kalman Filter Estimation Approaches for State Space Models with Nonlinear Measurements (Orderud, SCMM, 12).pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{1997_CNF_PedestrianDetection_Oren,
  author = {Oren, M. and Papageorgiou, C. and Sinha, P. and Osuna, E. and Poggio,
	T.},
  title = {Pedestrian detection using wavelet templates},
  booktitle = {Computer Vision and Pattern Recognition, 1997. Proceedings., 1997
	IEEE Computer Society Conference on},
  year = {1997},
  pages = {193 -199},
  month = {jun},
  abstract = {This paper presents a trainable object detection architecture that
	is applied to detecting people in static images of cluttered scenes.
	This problem poses several challenges. People are highly non-rigid
	objects with a high degree of variability in size, shape, color,
	and texture. Unlike previous approaches, this system learns from
	examples and does not rely on any a priori (hand-crafted) models
	or on motion. The detection technique is based on the novel idea
	of the wavelet template that defines the shape of an object in terms
	of a subset of the wavelet coefficients of the image. It is invariant
	to changes in color and texture and can be used to robustly define
	a rich and complex class of objects such as people. We show how the
	invariant properties and computational efficiency of the wavelet
	template make it an effective tool for object detection},
  comment = {det_pedestrian},
  doi = {10.1109/CVPR.1997.609319},
  file = {:papers\\1997 CNF, Pedestrian detection using wavelet templates (Oren, Papageorgiou, Sinha, Osuna, Poggio, 444).pdf:PDF},
  keywords = {cluttered scenes;color;object detection;pedestrian detection;shape;size;static
	images;texture;trainable object detection architecture;wavelet coefficients;wavelet
	template;wavelet templates;clutter;object detection;wavelet transforms;},
  review = {salman: similar to my approach},
  timestamp = {00,500}
}

@PHDTHESIS{2001_THE_OnlineEnsembleLearning_Oza,
  author = {Nikunj C. Oza},
  title = {Online Ensemble Learning},
  school = {The University of California},
  year = {2001},
  address = {Berkeley, CA},
  month = {Sep},
  abstract = {This thesis presents online versions of the popular bagging and boosting
	algorithms. We demonstrate theoretically and experimentally that
	the online versions perform comparably to their original batch counterparts
	in terms of classification performance. However, our online algorithms
	yield the typical practical benefits of online learning algorithms
	when the amount of training data available is large. Ensemble learning
	algorithms have become extremely popular over the last several years
	because these algorithms, which generate multiple \emph{base} models
	using traditional machine learning algorithms and combine them into
	an \emph{ensemble} model, have often demonstrated significantly better
	performance than single models. Bagging and boosting are two of the
	most popular algorithms because of their good empirical results and
	theoretical support. However, most ensemble algorithms operate in
	batch mode, i.e., they repeatedly read and process the entire training
	set. Typically, they require at least one pass through the training
	set for every base model to be included in the ensemble. The base
	model learning algorithms themselves may require several passes through
	the training set to create each base model. In situations where data
	is being generated continuously, storing data for batch learning
	is impractical, which makes using these ensemble learning algorithms
	impossible. These algorithms are also impractical in situations where
	the training set is large enough that reading and processing it many
	times would be computationally prohibitive. This thesis describes
	online versions of bagging and boosting. Unlike the batch versions,
	our online versions require only one pass through the training examples
	in order regardless of the number of base models to be combined.
	We discuss how we derive the online algorithms from their batch counterparts
	as well as theoretical and experimental evidence that our online
	algorithms perform comparably to the batch versions in terms of classification
	performance. We also demonstrate that our online algorithms have
	the practical advantage of lower running time, especially for larger
	datasets. This makes our online algorithms practical for machine
	learning and data mining tasks where the amount of training data
	available is very large.},
  bib2html_pubtype = {Other},
  bib2html_rescat = {Ensemble Learning},
  comment = {PRML},
  department = {Electrical Engineering and Computer Science},
  file = {:papers\\2001 THE, Online ensemble learning (Oza, Berkeley, 40).pdf:PDF},
  timestamp = {00,050}
}

@ARTICLE{11061297,
  author = {Ozcelik, E. and Karakus, T. and Kursun, E. and Cagiltay, K.},
  title = {An eye-tracking study of how color coding affects multimedia learning},
  journal = {Computers \&amp; Education},
  year = {2009},
  volume = { 53},
  pages = {445 - 53},
  number = { 2},
  note = {color coding;multimedia learning;eye-tracking study;eye movement
	data;perceptually salient information;},
  abstract = {Color coding has been proposed to promote more effective learning.
	However, insufficient evidence currently exists to show how color
	coding leads to better learning. The goal of this study was to investigate
	the underlying cause of the color coding effect by utilizing eye
	movement data. Fifty-two participants studied either a color-coded
	or conventional format of multimedia instruction. Eye movement data
	were collected during the study. The results indicate that color
	coding increased retention and transfer performance. Enhancement
	of learning by color coding was due to efficiency of locating corresponding
	information between illustration and text. Color coding also attracted
	attention of learners to perceptually salient information. [All rights
	reserved Elsevier].},
  address = {UK},
  comment = {trk_color},
  copyright = {Copyright 2010, The Institution of Engineering and Technology},
  file = {:papers\\2009 JNL, An eye-tracking study of how color coding affects multimedia learning (Ozcelik, Karakus, Kursun, Cagiltay).pdf:PDF},
  issn = {0360-1315},
  keywords = {computer aided instruction;human factors;multimedia computing;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1016/j.compedu.2009.03.002}
}

@ARTICLE{7393436,
  author = {Ozyildiz, E. and Krahnstover, N. and Sharma, R.},
  title = {Adaptive texture and color segmentation for tracking moving objects},
  journal = {Pattern Recognition},
  year = {2002},
  volume = { 35},
  pages = {2013 - 29},
  number = { 10},
  note = {adaptive texture segmentation;adaptive color segmentation;moving
	object tracking;real-time object tracking;varying environmental conditions;video
	sequences;multiple cue fusion techniques;computational complexity;computational
	cost;real-time target tracking;autobinomial Gibbs Markov random field;2D
	Gaussian distribution;probabilistic fusion;static images;dynamic
	image sequences;},
  abstract = {Color segmentation is a very popular technique for real-time object
	tracking. However, even with adaptive color segmentation schemes,
	under varying environmental conditions in video sequences, the tracking
	tends to be unreliable. To overcome this problem, many multiple cue
	fusion techniques have been suggested. One of the cues that complements
	color nicely is texture. However, texture segmentation has not been
	used for object tracking mainly because of the computational complexity
	of texture segmentation. This paper presents a formulation for fusing
	texture and color in a manner that makes the segmentation reliable
	while keeping the computational cost low, with the goal of real-time
	target tracking. An autobinomial Gibbs Markov random field is used
	for modeling the texture and a 2D Gaussian distribution is used for
	modeling the color. This allows a probabilistic fusion of the texture
	and color cues and for adapting both the texture and color over time
	for target tracking. Experiments with both static images and dynamic
	image sequences establish the feasibility of the proposed approach},
  address = {UK},
  comment = {trk_color},
  copyright = {Copyright 2002, IEE},
  file = {:papers\\2002 JNL, Adaptive texture and color segmentation for tracking moving objects (Ozyildiz, Krahnstover, Sharma).pdf:PDF},
  issn = {0031-3203},
  keywords = {adaptive signal processing;computational complexity;image colour analysis;image
	segmentation;image sequences;image texture;Markov processes;real-time
	systems;tracking;},
  language = {English},
  timestamp = {00,050},
  url = {http://dx.doi.org/10.1016/S0031-3203(01)00181-9}
}

@ARTICLE{2011_JNL_TRKocclusion_Papadakis,
  author = {Papadakis, Nicolas and Bugeau, Aurelie},
  title = {Tracking with Occlusions via Graph Cuts},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2011},
  volume = {33},
  pages = {144 -157},
  number = {1},
  month = jan.,
  abstract = {This work presents a new method for tracking and segmenting along
	time-interacting objects within an image sequence. One major contribution
	of the paper is the formalization of the notion of visible and occluded
	parts. For each object, we aim at tracking these two parts. Assuming
	that the velocity of each object is driven by a dynamical law, predictions
	can be used to guide the successive estimations. Separating these
	predicted areas into good and bad parts with respect to the final
	segmentation and representing the objects with their visible and
	occluded parts permit handling partial and complete occlusions. To
	achieve this tracking, a label is assigned to each object and an
	energy function representing the multilabel problem is minimized
	via a graph cuts optimization. This energy contains terms based on
	image intensities which enable segmenting and regularizing the visible
	parts of the objects. It also includes terms dedicated to the management
	of the occluded and disappearing areas, which are defined on the
	areas of prediction of the objects. The results on several challenging
	sequences prove the strength of the proposed approach.},
  comment = {TRK_occlusion},
  doi = {10.1109/TPAMI.2010.56},
  file = {:c\:\\salman\\work\\writing\\papers\\2011 JNL, Tracking with Occlusions via Graph Cuts (Papadakis).pdf:PDF},
  issn = {0162-8828},
  timestamp = {-}
}

@INPROCEEDINGS{1998_CNF_GeneralFrameworkObjectDetection_Papageorgiou,
  author = {Papageorgiou, C. P. and Oren, M. and Poggio, T.},
  title = {A general framework for object detection},
  booktitle = {Computer Vision, 1998. Sixth International Conference on},
  year = {1998},
  pages = {555-562},
  abstract = {This paper presents a general trainable framework for object detection
	in static images of cluttered scenes. The detection technique we
	develop is based on a wavelet representation of an object class derived
	from a statistical analysis of the class instances. By learning an
	object class in terms of a subset of an overcomplete dictionary of
	wavelet basis functions, we derive a compact representation of an
	object class which is used as an input to a support vector machine
	classifier. This representation overcomes both the problem of in-class
	variability and provides a low false detection rate in unconstrained
	environments. We demonstrate the capabilities of the technique in
	two domains whose inherent information content differs significantly.
	The first system is face detection and the second is the domain of
	people which, in contrast to faces, vary greatly in color, texture,
	and patterns. Unlike previous approaches, this system learns from
	examples and does not rely on any a priori (hand-crafted) models
	or motion-based segmentation. The paper also presents a motion-based
	extension to enhance the performance of the detection algorithm over
	video sequences. The results presented here suggest that this architecture
	may well be quite general},
  comment = {det},
  file = {:papers\\1998 CNF, A general framework for object detection (Papageorgiou, Oren, Tomaso Poggio, ICCV, 525).pdf:PDF},
  keywords = {learning (artificial intelligence) object detection object recognition
	cluttered scenes static images trainable framework unconstrained
	environments wavelet representation},
  owner = {salman},
  timestamp = {00,500}
}

@ARTICLE{9191501,
  author = {Papanikolopoulos, N. and Veeraraghavan, H. and Schrater, P.},
  title = {Robust target detection and tracking through integration of motion,
	color, and geometry},
  journal = {Computer Vision and Image Understanding},
  year = {2006},
  volume = { 103},
  pages = {121 - 38},
  number = { 2},
  note = {robust target detection;target tracking;vision tracking;computer
	vision;clutter;geometry template;joint probabilistic data association;blob
	cue measurement;outdoor traffic intersection image sequences;},
  abstract = {Vision-based tracking is a basic elementary task in many computer
	vision-based applications such as video surveillance and monitoring,
	sensing and navigation in robotics, video compression, video annotation,
	and many more. However, reliable recovery of targets and their trajectories
	in an uncontrolled environment is affected by a wide range of conditions
	exhibited by the environment such as sudden illumination changes
	and clutter. This work addresses the problem of (i) combining information
	from a set of cues in order to obtain reasonably accurate estimates
	of multiple targets in uncontrolled environments and (ii) a collection
	of data association methods for cues containing less information
	for robust tracking through persistent clutter. Specifically, we
	introduce a novel geometric template constrained data association
	method for robust tracking of point features, while using the Joint
	Probabilistic Data Association (JPDA) method for blob cue measurements.
	Extensive experimental validation of the tracking and the data association
	framework is presented in the work for several real-world outdoor
	traffic intersection image sequences. [All rights reserved Elsevier]},
  address = {USA},
  comment = {trk_color},
  copyright = {Copyright 2006, The Institution of Engineering and Technology},
  file = {:papers\\2006 JNL, Robust target detection and tracking through integration of motion, color, and geometry (Veeraraghavan, Schrater, Papanikolopoulos).pdf:PDF},
  issn = {1077-3142},
  keywords = {clutter;computer vision;image sequences;road traffic;sensor fusion;target
	tracking;},
  language = {English},
  timestamp = {00,020},
  url = {http://dx.doi.org/10.1016/j.cviu.2006.04.003}
}

@ARTICLE{2002_JNL_GeodesicActiveRegionsLevelSetsSegmentation_Paragios,
  author = {Paragios, Nikos and Deriche, Rachid},
  title = {Geodesic active regions and level set methods for supervised texture
	segmentation},
  journal = {International Journal of Computer Vision},
  year = {2002},
  volume = {46},
  pages = {223 - 247},
  number = {3},
  abstract = {This paper presents a novel variational framework to deal with frame
	partition problems in Computer Vision. This framework exploits boundary
	and region-based segmentation modules under a curve-based optimization
	objective function. The task of supervised texture segmentation is
	considered to demonstrate the potentials of the proposed framework.
	The textured feature space is generated by filtering the given textured
	images using isotropic and anisotropic filters, and analyzing their
	responses as multi-component conditional probability density functions.
	The texture segmentation is obtained by unifying region and boundary-based
	information as an improved Geodesic Active Contour Model. The defined
	objective function is minimized using a gradient-descent method where
	a level set approach is used to implement the obtained PDE. According
	to this PDE, the curve propagation towards the final solution is
	guided by boundary and region-based segmentation forces, and is constrained
	by a regularity force. The level set implementation is performed
	using a fast front propagation algorithm where topological changes
	are naturally handled. The performance of our method is demonstrated
	on a variety of synthetic and real textured frames.},
  comment = {seg},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2002 JNL, Geodesic active regions and level set methods for supervised texture (Paragios, Deriche, IJCV, 493).pdf:PDF},
  issn = {09205691},
  key = {Computer vision},
  keywords = {Algorithms;Computational geometry;Computer simulation;Digital filters;Feature
	extraction;Gradient methods;Image analysis;Image enhancement;Image
	segmentation;Optimization;Probability density function;},
  language = {English},
  timestamp = {00,500},
  url = {http://dx.doi.org/10.1023/A:1014080923068}
}

@ARTICLE{2000_JNL_GeodesicActiveContoursLevelSetsTracking_ParagiosDeriche,
  author = {Paragios, N. and Deriche, R.},
  title = {Geodesic active contours and level sets for the detection and tracking
	of moving objects},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {266-280},
  number = {3},
  abstract = {This paper presents a new variational framework for detecting and
	tracking multiple moving objects in image sequences. Motion detection
	is performed using a statistical framework for which the observed
	interframe difference density function is approximated using a mixture
	model. This model is composed of two components, namely, the static
	(background) and the mobile (moving objects) one. Both components
	are zero-mean and obey Laplacian or Gaussian law. This statistical
	framework is used to provide the motion detection boundaries. Additionally,
	the original frame is used to provide the moving object boundaries.
	Then, the detection and the tracking problem are addressed in a common
	framework that employs a geodesic active contour objective function.
	This function is minimized using a gradient descent method. A new
	approach named Hermes is proposed, which exploits aspects from the
	well-known front propagation algorithms and compares favorably to
	them. Very promising experimental results are provided using real
	video sequences},
  comment = {trk_contour},
  file = {:papers\\2000 JNL, Geodesic active contours and level sets for the detection and tracking of moving objects (Paragios, Deriche, PAMI, 613).pdf:PDF},
  keywords = {edge detection image sequences motion estimation optimisation statistical
	analysis target tracking Hermes front propagation geodesic active
	contours level set theory motion detection moving object tracking
	objective function},
  owner = {salman},
  timestamp = {00,600}
}

@ARTICLE{2004_JNL_BNhumanInteractions_Park,
  author = {Park, Sangho and Aggarwal, J. K.},
  title = {A hierarchical Bayesian network for event recognition of human actions
	and interactions},
  journal = {Multimedia Systems},
  year = {2004},
  volume = {10},
  pages = {164-179},
  number = {Compendex},
  abstract = {Recognizing human interactions is a challenging task due to the multiple
	body parts of interacting persons and the concomitant occlusions.
	This paper presents a method for the recognition of two-person interactions
	using a hierarchical Bayesian network (BN). The poses of simultaneously
	tracked body parts are estimated at the low level of the BN, and
	the overall body pose is estimated at the high level of the BN. The
	evolution of the poses of the multiple body parts are processed by
	a dynamic Bayesian network (DBN). The recognition of two-person interactions
	is expressed in terms of semantic verbal descriptions at multiple
	levels: individual body-part motions at low level, single-person
	actions at middle level, and two-person interactions at high level.
	Example sequences of interacting persons illustrate the success of
	the proposed framework. Surveillance - Event recognition - Human
	interaction - Motion - Bayesian network.},
  comment = {recog_action},
  file = {:papers\\2004 JNL, A hierarchical bayesian network for event recognition of human actions and interactions (Park, Aggarwal, MS, 58).pdf:PDF},
  keywords = {Pattern recognition Algorithms Computer vision Context free grammars
	Feature extraction Finite automata Image analysis Image segmentation
	Markov processes Motion estimation Neural networks Semantics},
  owner = {salman},
  timestamp = {00,060}
}

@ARTICLE{2001_JNL_ColorTexture_Paschos,
  author = {Paschos, G.},
  title = {Perceptually uniform color spaces for color texture analysis: an
	empirical evaluation},
  journal = {Image Processing, IEEE Transactions on},
  year = {2001},
  volume = {10},
  pages = {932 -937},
  number = {6},
  month = {jun},
  abstract = {RGB, a nonuniform color space, is almost universally accepted by the
	image processing community as the means for representing color. On
	the other hand, perceptually uniform spaces, such as L*a*b*, as well
	as approximately-uniform color spaces, such as HSV, exist, in which
	measured color differences are proportional to the human perception
	of such differences. This paper compares RGB with L*a*b* and HSV
	in terms of their effectiveness in color texture analysis. There
	has been a limited but increasing amount of work on the color aspects
	of textured images. The results have shown that incorporating color
	into a texture analysis and recognition scheme can be very important
	and beneficial. The presented methodology uses a family of Gabor
	filters specially tuned to measure specific orientations and sizes
	within each color texture. Effectiveness is measured by the classification
	performance of each color space, as well as by classifier-independent
	measures. Experimental results are obtained with a variety of color
	texture Images. Perceptually uniform spaces are shown to outperform
	RGB in many cases },
  comment = {feature},
  doi = {10.1109/83.923289},
  file = {:papers\\2001 JNL, Perceptually uniform color spaces for color texture analysis_ an empirical evaluation (Paschos, PAMI, 81).pdf:PDF},
  issn = {1057-7149},
  keywords = {Gabor filters;HSV;L*a*b*;RGB;approximately-uniform color spaces;classification
	performance;classifier-independent measures;color differences;color
	representation;color space;color texture analysis;image classification;image
	processing;image recognition;nonuniform color space;perceptually
	uniform color spaces;filtering theory;image classification;image
	colour analysis;image recognition;image texture;visual perception;},
  timestamp = {00,080}
}

@ARTICLE{2010_JNL_TRK_region_Patras,
  author = {Patras, I. and Hancock, E.R.},
  title = {Coupled Prediction Classification for Robust Visual Tracking},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2010},
  volume = {32},
  pages = {1553 -1567},
  number = {9},
  month = sept.,
  abstract = {This paper addresses the problem of robust template tracking in image
	sequences. Our work falls within the discriminative framework in
	which the observations at each frame yield direct probabilistic predictions
	of the state of the target. Our primary contribution is that we explicitly
	address the problem that the prediction accuracy for different observations
	varies, and in some cases, can be very low. To this end, we couple
	the predictor to a probabilistic classifier which, when trained,
	can determine the probability that a new observation can accurately
	predict the state of the target (that is, determine the #x201C;relevance
	#x201D; or #x201C;reliability #x201D; of the observation in question).
	In the particle filtering framework, we derive a recursive scheme
	for maintaining an approximation of the posterior probability of
	the state in which multiple observations can be used and their predictions
	moderated by their corresponding relevance. In this way, the predictions
	of the #x201C;relevant #x201D; observations are emphasized, while
	the predictions of the #x201C;irrelevant #x201D; observations are
	suppressed. We apply the algorithm to the problem of 2D template
	tracking and demonstrate that the proposed scheme outperforms classical
	methods for discriminative tracking both in the case of motions which
	are large in magnitude and also for partial occlusions.},
  comment = {TRK_region},
  doi = {10.1109/TPAMI.2009.175},
  file = {:papers\\2010 JNL, Coupled Prediction Classification for Robust Visual Tracking (Patras).pdf:PDF},
  issn = {0162-8828},
  keywords = {2D template tracking;computer vision;coupled prediction classification;discriminative
	tracking;image sequences;particle filtering framework;posterior probability;probabilistic
	classifier;robust template tracking;robust visual tracking;state
	estimation;computer vision;image classification;image sequences;particle
	filtering (numerical methods);probability;state estimation;target
	tracking;},
  timestamp = {-}
}

@INPROCEEDINGS{2009_CNF_SURVEY_surv_Patrick,
  author = {Patrick, R. and Bourbakis, N.},
  title = {Surveillance Systems for Smart Homes: A Comparative Survey},
  booktitle = {Tools with Artificial Intelligence, 2009. ICTAI '09. 21st International
	Conference on},
  year = {2009},
  pages = {248 -252},
  month = nov.,
  abstract = {As the number of older Americans increases and many decide to stay
	in their homes, the need for assistive technologies grows. One such
	technology is an intelligent system of surveillance cameras. While
	these systems can provide many services, we attempted to limit ourselves
	to the evaluation of systems that use cameras to track the locations
	of objects that may be lost or misplaced within a home. We provide
	a comparison of the systems that are currently in operation or are
	being developed and suggest areas where enhancements may produce
	systems that are more effective. Because few systems exist for the
	sole purpose of tracking objects in a home, we also evaluate systems
	that are designed to use multiple cameras to perform general surveillance.
	We go on to show where some principles of these system can be adapted
	to improve the performance in the task of tracking objects within
	a home.},
  comment = {survey, surveillance},
  doi = {10.1109/ICTAI.2009.93},
  file = {:C\:\\salman\\work\\writing\\papers\\2009 CNF, Surveillance Systems for Smart Homes_ A Comparative Survey (Patrick).pdf:PDF},
  issn = {1082-3409},
  keywords = {intelligent system;location tracking;object tracking;smart homes;surveillance
	cameras;surveillance systems;home computing;tracking;video cameras;video
	surveillance;},
  timestamp = {?}
}

@INPROCEEDINGS{1999_CNF_DBNfigureTracking_Pavlovic,
  author = {Pavlovic, V. and Rehg, J.M. and Tat-Jen Cham and Murphy, K.P.},
  title = {A dynamic Bayesian network approach to figure tracking using learned
	dynamic models},
  booktitle = {Computer Vision, 1999. The Proceedings of the Seventh IEEE International
	Conference on},
  year = {1999},
  volume = {1},
  pages = {94 -101 vol.1},
  abstract = {The human figure exhibits complex and rich dynamic behavior that is
	both nonlinear and time-varying. However most work on tracking and
	synthesizing figure motion has employed either simple, generic dynamic
	models or highly specific hand-tailored ones. Recently, a broad class
	of learning and inference algorithms for time-series models have
	been successfully cast in the framework of dynamic Bayesian networks
	(DBNs). This paper describes a novel DBN-based switching linear dynamic
	system (SLDS) model and presents its application to figure motion
	analysis. A key feature of our approach is an approximate Viterbi
	inference technique for overcoming the intractability of exact inference
	in mixed-state DBNs. We present experimental results for learning
	figure dynamics from video data and show promising initial results
	for tracking, interpolation, synthesis, and classification using
	learned models},
  comment = {trk_people},
  doi = {10.1109/ICCV.1999.791203},
  file = {:papers\\1999 CNF, A dynamic Bayesian network approach to figure tracking using learned dynamic models (Pavlovic, Rehg, Cham, Murphy, ICCV, 199).pdf:PDF},
  keywords = {Bayesian network;dynamic Bayesian networks;figure dynamics;figure
	motion analysis;figure tracking;inference algorithms;interpolation;learned
	dynamic models;learning;tracking;video data;belief networks;motion
	estimation;tracking;},
  timestamp = {00,200}
}

@ARTICLE{9197114,
  author = {Li Peihua},
  title = {A clustering-based color model and integral images for fast object
	tracking},
  journal = {Signal Processing: Image Communication},
  year = {2006},
  volume = { 21},
  pages = {676 - 87},
  number = { 8},
  note = {clustering-based color model;integral images;object tracking;K-means
	clustering;multi-channel gray level;Gaussian distribution;color distribution;Bhattacharrya
	distance;bin distribution;exhaustive search;object localization;mean
	shift algorithm;},
  abstract = {The paper presents a clustering-based color model and develops a fast
	algorithm for object tracking. The color model is built upon <i>K</i>-means
	clustering, by which the color space of the object can be partitioned
	adaptively and the histogram bins can be determined accordingly.
	In addition, in each bin the multi-channel gray level is modelled
	as Gaussian distribution. Defined in this way the color model can
	describe accurately the color distribution with very little bins.
	To evaluate similarity between the reference model and the candidate
	model, a similarity measure based on Bhattacharrya distance is introduced
	and its simplified form is derived under assumption that in each
	bin distribution of gray level in different channel is independent
	of each other. Motivated by the paper of Viola and Jones, the <i>Integral
	Images</i> for computation of histogram, mean and variance are proposed,
	with which the similarity measure can be evaluated at negligible
	computational cost. Thus, exhaustive search is made efficiently for
	object localization which guarantees the global maximum be achieved.
	Comparisons with the well-known mean shift algorithm demonstrate
	that the proposed algorithm has better performance while having the
	same (or less) computational cost. [All rights reserved Elsevier]},
  address = {Netherlands},
  comment = {trk_color},
  copyright = {Copyright 2006, The Institution of Engineering and Technology},
  file = {:papers\\2006 JNL, A clustering-based color model and integral images for fast object tracking (Peihua).pdf:PDF},
  issn = {0923-5965},
  keywords = {Gaussian distribution;image colour analysis;pattern clustering;search
	problems;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1016/j.image.2006.06.002}
}

@ARTICLE{1991_JNL_ClosedFormSolutionsModeling_Pent,
  author = {Pentland, A. and Sclaroff, S.},
  title = {Closed-form solutions for physically based shape modeling and recognition},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1991},
  volume = {13},
  pages = {715 -729},
  number = {7},
  month = {jul},
  abstract = {The authors present a closed-form, physically based solution for recovering
	a three-dimensional (3-D) solid model from collections of 3-D surface
	measurements. Given a sufficient number of independent measurements,
	the solution is overconstrained and unique except for rotational
	symmetries. The proposed approach is based on the finite element
	method (FEM) and parametric solid modeling using implicit functions.
	This approach provides both the convenience of parametric modeling
	and the expressiveness of the physically based mesh formulation and,
	in addition, can provide great accuracy at physical simulation. A
	physically based object-recognition method that allows simple, closed-form
	comparisons of recovered 3-D solid models is presented. The performance
	of these methods is evaluated using both synthetic range data with
	various signal-to-noise ratios and using laser rangefinder data},
  comment = {TRK_contour},
  doi = {10.1109/34.85660},
  file = {:papers\\1991 JNL, Closed-form solutions for physically based shape modeling and recognition (Pentland).pdf:PDF},
  issn = {0162-8828},
  keywords = {3-D surface;finite element method;object-recognition;parametric modeling;pattern
	recognition;picture processing;rotational symmetries;shape modeling;shape
	recognition;solid modeling;finite element analysis;pattern recognition;picture
	processing;solid modelling;},
  timestamp = {00,500}
}

@INPROCEEDINGS{2002_CNF_TRKcolor_Perez,
  author = {Perez, P. and Hue, C. and Vermaak, J. and Gangnet, M.},
  title = {Color-based probabilistic tracking},
  booktitle = {Computer Vision - ECCV 2002. 7th European Conference on Computer
	Vision. Proceedings, Part I (Lecture Notes in Computer Science Vol.2350)},
  year = {2002},
  abstract = {Color-based trackers proposed by Bradski (1998), Chen and Lui (2001)
	and Comanicui et al. (2000) have been proved to be robust and versatile
	at a modest computational cost. They are especially appealing for
	tracking tasks where the spatial structure of the tracked objects
	exhibits such a dramatic variability that trackers based on a space-dependent
	appearance reference would break down very quickly. These trackers
	rely on the deterministic search of a window whose color content
	matches a reference histogram color model. Relying on the same principle
	of color histogram distance, but within a probabilistic framework,
	we introduce a new Monte Carlo tracking technique. The use of a particle
	filter allows us to handle better color clutter in the background,
	as well as complete occlusion of the tracked entities over a few
	frames. This probabilistic approach is very flexible and can be extended
	in a number of useful ways. In particular, we introduce: multi-part
	color modeling to capture a rough spatial layout ignored by global
	histograms; incorporation of a background color model when relevant;
	and extension to multiple objects},
  comment = {trk_feature},
  copyright = {Copyright 2002, IEE},
  file = {:papers\\2002 CNF, Color Based Probabilistic Tracking (Perez, Hue, Vermaak, Gangnet, ECCV, 1533).pdf:PDF},
  keywords = {image colour analysis;image sequences;Monte Carlo methods;optical
	tracking;},
  language = {English},
  timestamp = {01,500}
}

@ARTICLE{1999_JNL_KalmanSnakes_Peterfreund,
  author = {Peterfreund, N.},
  title = {Robust tracking of position and velocity with Kalman snakes},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1999},
  volume = {21},
  pages = {564 -569},
  number = {6},
  month = {jun},
  abstract = {A new Kalman-filter based active contour model is proposed for tracking
	of nonrigid objects in combined spatio-velocity space. The model
	employs measurements of gradient-based image potential and of optical-flow
	along the contour as system measurements. In order to improve robustness
	to image clutter and to occlusions an optical-flow based detection
	mechanism is proposed. The method detects and rejects spurious measurements
	which are not consistent with previous estimation of image motion},
  comment = {trk_contour},
  doi = {10.1109/34.771328},
  file = {:papers\\1999 JNL, Robust tracking of position and velocity with Kalman snakes (Peterfreund).pdf:PDF},
  issn = {0162-8828},
  keywords = {Kalman snakes;Kalman-filter based active contour model;combined spatio-velocity
	space;gradient-based image potential;image clutter robustness;image
	motion estimation;nonrigid objects;occlusion robustness;optical-flow
	based detection mechanism;robust position tracking;robust velocity
	tracking;Kalman filters;filtering theory;image sequences;motion estimation;optical
	tracking;},
  timestamp = {00,200}
}

@ARTICLE{1990_JNL_Network_Poggio,
  author = {Poggio, T. and Edelman, S.},
  title = {A network that learns to recognize three-dimensional objects},
  journal = {Nature},
  year = {1990},
  volume = {343},
  pages = {263--266},
  number = {6255},
  file = {:papers\\1990 JNL, A network that learns to recognize three-dimensional objects (Poggio).pdf:PDF},
  issn = {0028-0836}
}

@ARTICLE{1990_JNL_NetworkLearning_Poggio,
  author = {Poggio, T. and Girosi, F.},
  title = {Networks for approximation and learning},
  journal = {Proceedings of the IEEE},
  year = {1990},
  volume = {78},
  pages = {1481 -1497},
  number = {9},
  month = sep,
  abstract = {The problem of the approximation of nonlinear mapping, (especially
	continuous mappings) is considered. Regularization theory and a theoretical
	framework for approximation (based on regularization techniques)
	that leads to a class of three-layer networks called regularization
	networks are discussed. Regularization networks are mathematically
	related to the radial basis functions, mainly used for strict interpolation
	tasks. Learning as approximation and learning as hypersurface reconstruction
	are discussed. Two extensions of the regularization approach are
	presented, along with the approach's corrections to splines, regularization,
	Bayes formulation, and clustering. The theory of regularization networks
	is generalized to a formulation that includes task-dependent clustering
	and dimensionality reduction. Applications of regularization networks
	are discussed},
  comment = {TRK_subspace},
  file = {:papers\\1990 JNL, Networks for Approximation and Learning (Poggio).pdf:PDF},
  issn = {0018-9219},
  keywords = {Bayes formulation;approximation;clustering;dimensionality reduction;hypersurface;interpolation;neural
	networks;nonlinear mapping;regularization networks;splines;three-layer
	networks;approximation theory;learning systems;neural nets;},
  timestamp = {02,300}
}

@INPROCEEDINGS{1994_CNF_GetYourMan_Polana,
  author = {Polana, R. and Nelson, R.},
  title = {Low level recognition of human motion (or how to get your man without
	finding his body parts)},
  booktitle = {Motion of Non-Rigid and Articulated Objects, 1994., Proceedings of
	the 1994 IEEE Workshop on},
  year = {1994},
  pages = {77 -82},
  month = {nov},
  abstract = {The recognition of human movements such as walking, running or climbing
	has been approached previously by tracking a number of feature points
	and either classifying the trajectories directly or matching them
	with a high-level model of the movement. A major difficulty with
	these methods is acquiring and trading the requisite feature points,
	which are generally specific joints such as knees or angles. This
	requires previous recognition and/or part segmentation of the actor.
	We show that the recognition of walking or any repetitive motion
	activity can be accomplished on the basis of bottom up processing,
	which does not require the prior identification of specific parts,
	or classification of the actor. In particular, we demonstrate that
	repetitive motion is such a strong cue, that the moving actor can
	be segmented, normalized spatially and temporally, and recognized
	by matching against a spatiotemporal template of motion features.
	We have implemented a real-time system that can recognize and classify
	repetitive motion activities in normal gray-scale image sequences},
  comment = {recog_action},
  doi = {10.1109/MNRAO.1994.346251},
  file = {:papers\\1994 CNF, Low level recognition of human motion (or how to get your man without finding his body parts)  (Polana, Nelson, MNRAO, 207).pdf:PDF},
  keywords = { bottom up processing; feature points; high-level model; human motion;
	human movements; low level recognition; motion features; moving actor;
	normal gray-scale image sequences; real-time system; repetitive motion
	activity; spatially normalized; spatiotemporal template; trajectories;
	biomechanics; image recognition; image sequences; motion estimation;
	real-time systems;},
  review = {action recognition},
  timestamp = {00,200}
}

@ARTICLE{1997_JNL_RecognizingPeriodicMotion_Polana,
  author = {Polana, R. and Nelson, R. C.},
  title = {Detection and recognition of periodic, nonrigid motion},
  journal = {International Journal of Computer Vision},
  year = {1997},
  volume = {23},
  pages = {261-82},
  abstract = {The recognition of nonrigid motion, particularly that arising from
	human movement has typically made use of high-level parametric models
	representing the various body parts and their connections to each
	other. Such model-based recognition has been successful in some cases;
	however, the methods are often difficult to apply to real-world scenes,
	and are severely limited in their generalizability. The first problem
	arises from the difficulty of acquiring and tracking the requisite
	model parts, usually specific joints such as knees, elbows or ankles.
	This generally requires some prior high-level understanding and segmentation
	of the scene, or initialization by a human operator. The second problem
	with generalization is due to the fact that the human model is not
	much good for dogs or birds, and for each new type of motion, a new
	model must be hand-crafted. We show that the recognition of human
	or animal locomotion, and, in fact, any repetitive activity can be
	done using low-level, non-parametric representations. Such an approach
	has the advantage that the same underlying representation is used
	for all examples, and no individual tailoring of models or prior
	scene understanding is required. We show in particular, that repetitive
	motion is such a strong cue, that the moving actor can be segmented,
	normalized spatially and temporally, and recognized by matching against
	a spatio-temporal template of motion features. We have implemented
	a real-time system that can recognize and classify repetitive motion
	activities in normal gray-scale image sequences. Results on a number
	of real-world sequences are described},
  comment = {recog_action_periodicity},
  file = {:papers\\1997 JNL, Detection and recognition of periodic, nonrigid motion (Polana, Nelson, IJCV, 134).pdf:PDF},
  keywords = {computer vision image recognition image sequences motion estimation
	real-time systems},
  owner = {salman},
  timestamp = {00,150}
}

@ARTICLE{1988_JNL_SDE_Poor,
  author = {Poor, H.V.},
  title = {Fine quantization in signal detection and estimation},
  journal = {Information Theory, IEEE Transactions on},
  year = {1988},
  volume = {34},
  pages = {960 -972},
  number = {5},
  month = sep,
  abstract = {The performance lost to data quantization in signal detection and
	estimation procedures is considered. The performance is measured
	by f-divergences, which are useful indices of discrimination information
	between statistical hypotheses. Some properties of the f -divergences
	are briefly discussed, and a general result for the loss in divergence
	due to uniform data quantization is obtained. Several applications
	of this result in specific problems of signal detection and estimation
	being developed, and some numerical results that illustrate the asymptotic
	behavior of the divergence in these applications are given. The divergence
	lost to finite nonuniform quantization through the use of a companding
	model is considered, and the asymptotically optimum nonuniform quantizer
	within this class is derived. Some interesting problems that remain
	open are discussed},
  comment = {SP},
  doi = {10.1109/18.21219},
  file = {:\\papers\\1988 JNL, Fine Quantization in Signal Detection and Estimation (Poor).pdf:PDF},
  issn = {0018-9448},
  keywords = {companding model;data quantization;f-divergences;finite nonuniform
	quantization;signal detection;signal estimation;analogue-digital
	conversion;compandors;signal detection;},
  timestamp = {00,050}
}

@ARTICLE{2007_JNL_HumanMotion_Poppe,
  author = {Poppe, R.},
  title = {Vision-based human motion analysis: an overview},
  journal = {Computer Vision and Image Understanding},
  year = {2007},
  volume = {108},
  pages = {4-18},
  abstract = {Markerless vision-based human motion analysis has the potential to
	provide an inexpensive, non-obtrusive solution for the estimation
	of body poses. The significant research effort in this domain has
	been motivated by the fact that many application areas, including
	surveillance, human-computer interaction and automatic annotation,
	will benefit from a robust solution. In this paper, we discuss the
	characteristics of human motion analysis. We divide the analysis
	into a modeling and an estimation phase. Modeling is the construction
	of the likelihood function, estimation is concerned with finding
	the most likely pose given the likelihood surface. We discuss model-free
	approaches separately. This taxonomy allows us to highlight trends
	in the domain and to point out limitations of the current state of
	the art. [All rights reserved Elsevier].},
  comment = {survey},
  file = {:papers\\2007 JNL, Vision-based human motion analysis_ an overview (SURVEY, Poppe, CVIU, 73).pdf:PDF},
  keywords = {computer vision image motion analysis pose estimation},
  timestamp = {00,070}
}

@INPROCEEDINGS{12,
  author = {Prati, A. and Cucchiara, R. and Mikic, I. and Trivedi, M.M.},
  title = {Analysis and detection of shadows in video streams: a comparative
	evaluation},
  booktitle = {Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings
	of the 2001 IEEE Computer Society Conference on},
  year = {2001},
  volume = {2},
  pages = { II-571 - II-576 vol.2},
  abstract = { Robustness to changes in illumination conditions as well as viewing
	perspectives is an important requirement for many computer vision
	applications. One of the key factors in enhancing the robustness
	of dynamic scene analysis is that of accurate and reliable means
	for shadow detection. Shadow detection is critical for correct object
	detection in image sequences. Many algorithms have been proposed
	in the literature that deal with shadows. However, a comparative
	evaluation of the existing approaches is still lacking. In this paper,
	the full range of problems underlying the shadow detection is identified
	and discussed. We classify the proposed solutions to this problem
	using a taxonomy of four main classes, deterministic model and non-model
	based, and statistical parametric and nonparametric. Novel quantitative
	(detection and discrimination accuracy) and qualitative metrics (scene
	and object independence, flexibility to shadow situations and robustness
	to noise) are proposed to evaluate these classes of algorithms on
	a benchmark suite of indoor and outdoor video sequences.},
  comment = {color_shadow},
  doi = {10.1109/CVPR.2001.991013},
  file = {:papers\\2001 CNF, Analysis and detection of shadows in video streams_ a comparative evaluation (Prati).pdf:PDF},
  issn = {1063-6919 },
  keywords = { computer vision; deterministic model based method; deterministic
	nonmodel based method; dynamic scene analysis; illumination change
	robustness; image sequences; indoor video sequences; object detection;
	outdoor video sequences; qualitative metrics; quantitative metrics;
	shadow analysis; shadow detection; statistical nonparametric method;
	statistical parametric method; video streams; viewing perspective
	change robustness; computer vision; image sequences; object detection;
	video signal processing;},
  timestamp = {00,060}
}

@INPROCEEDINGS{1998_CNF_PRML_Provost,
  author = {Provost, F. and Fawcett, T. and Kohavi, R.},
  title = {The case against accuracy estimation for comparing induction algorithms},
  booktitle = {Machine Learning. Proceedings of the Fifteenth International Conference
	(ICML'98)},
  year = {1998},
  pages = {445 - 53},
  address = {San Francisco, CA, USA},
  note = {accuracy estimation;induction algorithms;classification accuracy;data
	sets;ROC analysis;machine learning;},
  abstract = {We analyze critically the use of classification accuracy to compare
	classifiers on natural data sets, providing a thorough investigation
	using ROC analysis, standard machine learning algorithms, and standard
	benchmark data sets. The results raise serious concerns about the
	use of accuracy for comparing classifiers and draw into question
	the conclusions that can be drawn from such studies. In the course
	of the presentation, we describe and demonstrate what we believe
	to be the proper use of ROC analysis for comparative studies in machine
	learning research. We argue that this methodology is preferable both
	for making practical choices and for drawing scientific conclusions},
  comment = {PRML},
  copyright = {Copyright 1999, IEE},
  file = {:papers\\1998 CNF, The case against accuracy estimation for comparing induction algorithms (Provost).pdf:PDF},
  keywords = {learning by example;pattern classification;},
  language = {English},
  timestamp = {00,570}
}

@INPROCEEDINGS{2010_CNF_TRKsubs_Qian,
  author = {Cheng Qian and Shuchang Xu and Sanyuan Zhang},
  title = {Robust visual tracking via weighted incremental subspace learning},
  booktitle = {Future Computer and Communication (ICFCC), 2010 2nd International
	Conference on},
  year = {2010},
  volume = {2},
  pages = {V2-26 -V2-29},
  month = may,
  abstract = {A method is proposed for tracking objects in face with varying viewpoints
	and partial occlusions. A low-dimensional subspace is built to model
	the appearance of the target. And each image sample is presented
	as a coefficient vector in the subspace. A collection of image patches
	are sampled as the candidates of the object image region in the current
	frame, and their likelihoods of being the object are evaluated resorting
	to the reconstructions from the corresponding coefficient vectors.
	The image patch with highest likelihood is selected as the object
	image region. Finally, the subspace is incrementally updated based
	on the coefficient vectors, which are assigned with the temporal
	weights to enhance the adaptability of the tracker. Experimental
	results show that our proposed method can locate the target accurately
	even when the target undergoes changes in viewpoints and partial
	occlusions.},
  comment = {TRK_subspace},
  doi = {10.1109/ICFCC.2010.5497295},
  file = {:papers\\2010 CNF, Robust visual tracking via weighted incremental subspace learning (Qian).pdf:PDF},
  keywords = {coefficient vector;image patches;partial occlusions;robust visual
	tracking;weighted incremental subspace learning;computer graphics;image
	reconstruction;learning (artificial intelligence);object detection;vectors;}
}

@ARTICLE{7559381,
  author = {Liu Qingshan and Ma Songde and Lu Hanqing},
  title = {Head tracking using shapes and adaptive color histograms},
  journal = {Journal of Computer Science and Technology (English Language Edition)},
  year = {2002},
  volume = { 17},
  pages = {859 - 864},
  number = { 6},
  note = {adaptive color histogram;head tracking;computer vision;real-time
	system;elliptical shape;nonparametric technique;mean shift algorithm;histogram
	matching;},
  abstract = {A new method is presented for tracking a person's head in real-time.
	The head is shaped as an ellipse, and the adaptively modified RGB
	color histogram is used to represent the tracked object (head). The
	method is composed of two parts. First, a robust nonparametric technique,
	called the mean shift algorithm, is adopted for histogram matching
	to estimate the head's location in the current frame. Second, a local
	search is performed after histogram matching to maximize the normalized
	gradient magnitude around the boundary of the elliptical head, so
	that a more accurate location and the best scale size of the head
	can be obtained. The method is demonstrated to be a real-time tracker
	and robust to clutter, scale variation, occlusion, rotation and camera
	motion, for several test sequences},
  address = {China},
  comment = {trk_color},
  copyright = {Copyright 2003, IEE},
  file = {:papers\\2002 JNL, Head tracking using shapes and adaptive color histograms (Qingshan, Songde, Hanqing).pdf:PDF},
  issn = {1000-9000},
  keywords = {computer vision;image colour analysis;image matching;real-time systems;target
	tracking;},
  language = {English},
  timestamp = {-}
}

@ARTICLE{1989_JNL_TutorialHMM_Rabiner,
  author = {Rabiner, L. R.},
  title = {A tutorial on hidden Markov models and selected applications in speech
	recognition},
  journal = {Proceedings of the IEEE},
  year = {1989},
  volume = {77},
  pages = {257-286},
  number = {2},
  comment = {tutorial},
  file = {:papers\\1989 JNL, A Tutorial on Hidden Markov Models and Selected Applilcations in Speech Recognition (Rabiner, RSR, 10749).pdf:PDF},
  keywords = {Markov processes speech recognition balls-in-urns system coin-tossing
	discrete Markov chains ergodic models hidden Markov models hidden
	states left-right models probabilistic function},
  owner = {salman},
  timestamp = {11,000}
}

@ARTICLE{2005_JNL_SURVEYchangeDetection_Radke,
  author = {Radke, R.J. and Andra, S. and Al-Kofahi, O. and Roysam, B.},
  title = {Image change detection algorithms: a systematic survey},
  journal = {Image Processing, IEEE Transactions on},
  year = {2005},
  volume = {14},
  pages = {294-307},
  number = {3},
  month = {March },
  abstract = {Detecting regions of change in multiple images of the same scene taken
	at different times is of widespread interest due to a large number
	of applications in diverse disciplines, including remote sensing,
	surveillance, medical diagnosis and treatment, civil infrastructure,
	and underwater sensing. This paper presents a systematic survey of
	the common processing steps and core decision rules in modern change
	detection algorithms, including significance and hypothesis testing,
	predictive models, the shading model, and background modeling. We
	also discuss important preprocessing methods, approaches to enforcing
	the consistency of the change mask, and principles for evaluating
	and comparing the performance of change detection algorithms. It
	is hoped that our classification of algorithms into a relatively
	small number of categories will provide useful guidance to the algorithm
	designer.},
  comment = {trk_BG},
  doi = {10.1109/TIP.2004.838698},
  issn = {1057-7149},
  keywords = {image classificationhypothesis testing, illumination invariance, image
	change detection algorithm, predictive model, shading model, significance
	testing, systematic survey},
  timestamp = {00,350}
}

@ARTICLE{1991_JNL_MotionCorrespondence_Rangarajan,
  author = {Rangarajan, Krishnan and Shah, Mubarak},
  title = {Establishing motion correspondence},
  journal = {CVGIP: Image Underst.},
  year = {1991},
  volume = {54},
  pages = {56--73},
  number = {1},
  address = {Orlando, FL, USA},
  comment = {trk_correspondence},
  doi = {http://dx.doi.org/10.1016/1049-9660(91)90075-Z},
  file = {:papers\\1991 JNL, Establishing motion correspondence (Rangarajan, Mubarakshah, CVGIP, 127).pdf:PDF},
  issn = {1049-9660},
  publisher = {Academic Press, Inc.},
  timestamp = {00,100}
}

@ARTICLE{2001_JNL_ProbabilisticDataAssociation_Rasmussen,
  author = {Rasmussen, C. and Hager, G. D.},
  title = {Probabilistic data association methods for tracking complex visual
	objects},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2001},
  volume = {23},
  pages = {560-576},
  number = {6},
  abstract = {We describe a framework that explicitly reasons about data association
	to improve tracking performance in many difficult visual environments.
	A hierarchy of tracking strategies results from ascribing ambiguous
	or missing data to: 1) noise-like visual occurrences, 2) persistent,
	known scene elements (i.e., other tracked objects), or 3) persistent,
	unknown scene elements. First, we introduce a randomized tracking
	algorithm adapted from an existing probabilistic data association
	filter (PDAF) that is resistant to clutter and follows agile motion.
	The algorithm is applied to three different tracking modalities-homogeneous
	regions, textured regions, and snakes-and extensibly defined for
	straightforward inclusion of other methods. Second, we add the capacity
	to track multiple objects by adapting to vision a joint PDAF which
	oversees correspondence choices between same-modality trackers and
	image features. We then derive a related technique that allows mixed
	tracker modalities and handles object overlaps robustly. Finally,
	we represent complex objects as conjunctions of cues that are diverse
	both geometrically (e.g., parts) and qualitatively (e.g., attributes).
	Rigid and hinge constraints between part trackers and multiple descriptive
	attributes for individual parts render the whole object more distinctive,
	reducing susceptibility to mistracking. Results are given for diverse
	objects such as people, microscopic cells, and chess pieces},
  comment = {trk_correspondence},
  file = {:papers\\2001 JNL, Probabilistic data association methods for tracking complex visual objects (Rasmussen, Hager, PAMI, 218).pdf:PDF},
  keywords = {filtering theory image processing noise probability randomised algorithms
	tracking PDAF agile motion ambiguous data clutter resistance complex
	visual object tracking hinge constraints homogeneous regions image
	features joint PDAF missing data mistracking susceptibility multiple
	descriptive attributes noise-like visual occurrences object overlaps
	occlusion probabilistic data association filter probabilistic data
	association methods randomized tracking algorithm rigid constraints
	snakes textured regions tracking strategy hierarchy},
  owner = {salman},
  timestamp = {00,200}
}

@INPROCEEDINGS{1995_CNF_Tracking_Rehg,
  author = {Rehg, J. M. and Kanade, T.},
  title = {Model-based tracking of self-occluding articulated objects},
  booktitle = {Proceedings of IEEE International Conference on Computer Vision,
	20-23 June 1995},
  year = {1995},
  abstract = {Computer sensing of hand and limb motion is an important problem for
	applications in human computer interaction and computer graphics.
	We describe a framework for local trading of self occluding motion,
	in which one part of an object obstructs the visibility of another.
	Our approach uses a kinematic model to predict occlusions and windowed
	templates to track partially occluded objects. We present offline
	3D tracking results for hand motion with significant self occlusion},
  comment = {recog_pose},
  file = {:papers\\1995 CNF, Model-based tracking of self-occluding articulated objects (Rehg, Kanade, ICCV, 349).pdf:PDF},
  keywords = {computer graphics kinematics motion estimation user interfaces},
  timestamp = {00,400}
}

@INPROCEEDINGS{1994_CNF_DigitEyes_Rehg,
  author = {Rehg, J. M. and Kanade, T.},
  title = {DigitEyes: vision-based hand tracking for human-computer interaction},
  booktitle = {Motion of Non-Rigid and Articulated Objects, 1994., Proceedings of
	the 1994 IEEE Workshop on},
  year = {1994},
  abstract = {Computer sensing of hand and limb motion is an important problem for
	applications in human-computer interaction (HCI), virtual reality,
	and athletic performance measurement. Commercially available sensors
	are invasive, and require the user to wear gloves or targets. We
	have developed a noninvasive vision-based hand tracking system, called
	DigitEyes. Employing a kinematic hand model, the DigitEyes system
	has demonstrated tracking performance at speeds of up to 10 Hz, using
	line and point features extracted from gray scale images of unadorned,
	unmarked hands. We describe an application of our sensor to a 3D
	mouse user-interface problem},
  comment = {recog_pose},
  file = {:papers\\1994 CNF, DigitEyes_ vision-based hand tracking for human-computer interaction (Rehg, Kanade, MNRAO, 140).pdf:PDF},
  keywords = {computer vision feature extraction human factors image recognition
	mouse controllers (computers) user interfaces 3D mouse user-interface
	problem DigitEyes athletic performance measurement computer sensing
	gloves gray scale images hand motion human-computer interaction kinematic
	hand model limb motion sensors tracking performance virtual reality
	vision-based hand tracking},
  timestamp = {00,150}
}

@INPROCEEDINGS{1994_CNF_HighDOF_RehgKanade,
  author = {Rehg, J. M. and Kanade, T.},
  title = {Visual tracking of high DOF articulated structures: an application
	to human hand tracking},
  booktitle = {Proceedings of Third European Conference on Computer Vision, Volume
	II, 2-6 May 1994},
  year = {1994},
  abstract = {Passive sensing of human hand and limb motion is important for a wide
	range of applications from human-computer interaction to athletic
	performance measurement. High degree of freedom articulated mechanisms
	like the human hand are difficult to track because of their large
	state space and complex image appearance. This article describes
	a model-based hand tracking system, called DigitEyes, that can recover
	the state of a 27 DOF hand model from ordinary gray scale images
	at speeds of up to 10 Hz},
  comment = {trk_people},
  file = {:papers\\1994 CNF, Visual tracking of high DOF articulated structures_ an application to human hand tracking (Rehg, Kanade, ECCV, 332).pdf:PDF},
  keywords = {image processing motion estimation tracking},
  owner = {salman},
  timestamp = {00,300}
}

@ARTICLE{1979_JNL_MTT_Reid,
  author = { Reid, D.},
  title = {An algorithm for tracking multiple targets},
  journal = {Automatic Control, IEEE Transactions on},
  year = {1979},
  volume = {24},
  pages = { 843 - 854},
  number = {6},
  month = {dec},
  abstract = { An algorithm for tracking multiple targets in a cluttered enviroment
	is developed. The algorithm is capable of initiating tracks, accounting
	for false or missing reports, and processing sets of dependent reports.
	As each measurement is received, probabilities are calculated for
	the hypotheses that the measurement came from previously known targets
	in a target file, or from a new target, or that the measurement is
	false. Target states are estimated from each such data-association
	hypothesis using a Kalman filter. As more measurements are received,
	the probabilities of joint hypotheses are calculated recursively
	using all available information such as density of unknown targets,
	density of false targets, probability of detection, and location
	uncertainty. This branching technique allows correlation of a measurement
	with its source based on subsequent, as well as previous, data. To
	keep the number of hypotheses reasonable, unlikely hypotheses are
	eliminated and hypotheses with similar target estimates are combined.
	To minimize computational requirements, the entire set of targets
	and measurements is divided into clusters that are solved independently.
	In an illustrative example of aircraft tracking, the algorithm successfully
	tracks targets over a wide range of conditions.},
  comment = {trk_correspondence},
  file = {:papers\\1979 JNL, An algorithm  for tracking multiple targets (Reid, TAC, 1048).pdf:PDF},
  issn = {0018-9286},
  keywords = { Bayes procedures; Tracking;},
  timestamp = {01,000}
}

@ARTICLE{10176026,
  author = {Ying Ren and Chin Seng Chua},
  title = {Bilateral learning for color-based tracking},
  journal = {Image and Vision Computing},
  year = {2008},
  volume = { 26},
  pages = {1530 - 9},
  number = { 11},
  note = {bilateral learning;color model adaptation;color-based object tracking;illumination
	changes;color-based object detection;unsupervised learning problem;image
	sequence;},
  abstract = {This paper addresses the issue of color model adaptation and color-based
	object tracking in a dynamic scene. Under different environmental
	conditions such as illumination changes, a static color model is
	inadequate for the purpose of color-based object detection and tracking.
	Color model adaptation is required and this has to be integrated
	into the tracking procedure within the spatial domain. To track a
	target in both the color and spatial domains, a bilateral learning
	(BL) approach is proposed in this paper. Formulated as an unsupervised
	learning problem, the adaptations of the color and spatial models
	are fitted into an EM framework by updating in the color and image
	spaces alternately. This results in the adaptation of the color model
	and the localization of the target along the image sequence. Experimental
	results show the effectiveness and efficacy of the proposed approach
	for color model adaptation and object tracking under illumination
	changes and environmental noise in real time.[All rights reserved
	Elsevier].},
  address = {Netherlands},
  comment = {trk_color},
  copyright = {Copyright 2008, The Institution of Engineering and Technology},
  file = {:papers\\2008 JNL, Bilateral learning for color-based tracking (Ren, Chua).pdf:PDF},
  issn = {0262-8856},
  keywords = {image colour analysis;image sequences;object detection;target tracking;unsupervised
	learning;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1016/j.imavis.2008.04.023}
}

@BOOK{1987_BOOK_StochasticSimulation_Ripley,
  title = {Stochastic Simulation (Wiley Series in Probability and Statistics)},
  publisher = {Wiley-Interscience},
  year = {2006},
  author = {Ripley, Brian D.},
  edition = {1},
  month = {March},
  comment = {PRML_book},
  day = {10},
  file = {:papers\\1987 BOOK, Stochastic Simulation (Ripley, Wiley).djvu:Djvu},
  howpublished = {Paperback},
  isbn = {0470009608},
  posted-at = {2010-04-21 18:42:05},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0470009608}
}

@INPROCEEDINGS{2000_CNF_ProbabilisticBGforTracking_Rittscher,
  author = {J. Rittscher and J. Kato and S. Joga and A. Blake},
  title = {A Probabilistic Background Model for Tracking},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year = {2000},
  pages = {336--350},
  comment = {trk_BG},
  file = {:papers\\2000 CNF, A probabilistic background model for tracking (Rittscher, Kato, Joga, Blake, ECCV, 119).pdf:PDF},
  timestamp = {00,100}
}

@INPROCEEDINGS{2005_CNF_SegmentationAndShape_Rittscher,
  author = {Rittscher, J. and Tu, P.H. and Krahnstoever, N.},
  title = {Simultaneous estimation of segmentation and shape},
  booktitle = {Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
	Society Conference on},
  year = {2005},
  volume = {2},
  abstract = { The main focus of this work is the integration of feature grouping
	and model based segmentation into one consistent framework. The algorithm
	is based on partitioning a given set of image features using a likelihood
	function that is parameterized on the shape and location of potential
	individuals in the scene. Using a variant of the EM formulation,
	maximum likelihood estimates of both the model parameters and the
	grouping are obtained simultaneously. The resulting algorithm performs
	global optimization and generates accurate results even when decisions
	can not be made using local context alone. An important feature of
	the algorithm is that the number of people in the scene is not modeled
	explicitly. As a result no prior knowledge or assumed distributions
	are required. The approach is shown to be robust with respect to
	partial occlusion, shadows, clutter, and can operate over a large
	range of challenging view angles including those that are parallel
	to the ground plane. Comparisons with existing crowd segmentation
	systems are made and the utility of coupling crowd segmentation with
	a temporal tracking system is demonstrated.},
  comment = {seg},
  doi = {10.1109/CVPR.2005.323},
  file = {:papers\\2005 CNF, Simultaneous estimation of segmentation and shape (Rittscher, Tu, Krahnstoever, CVPR, 37).pdf:PDF},
  issn = {1063-6919 },
  keywords = { EM formulation; image features; image segmentation; maximum likelihood
	estimation; model based segmentation; partial occlusion; feature
	extraction; hidden feature removal; image segmentation; maximum likelihood
	estimation;},
  timestamp = {00,040}
}

@ARTICLE{2005_JNL_DiscriminativeFeaturesTracking_CollinsYanxiLeordeanu,
  author = {Robert, T. Collins and Yanxi, Liu and Leordeanu, M.},
  title = {Online selection of discriminative tracking features},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2005},
  volume = {27},
  pages = {1631-1643},
  number = {10},
  abstract = {This paper presents an online feature selection mechanism for evaluating
	multiple features while tracking and adjusting the set of features
	used to improve tracking performance. Our hypothesis is that the
	features that best discriminate between object and background are
	also best for tracking the object. Given a set of seed features,
	we compute log likelihood ratios of class conditional sample densities
	from object and background to form a new set of candidate features
	tailored to the local object/background discrimination task. The
	two-class variance ratio is used to rank these new features according
	to how well they separate sample distributions of object and background
	pixels. This feature evaluation mechanism is embedded in a mean-shift
	tracking system that adaptively selects the top-ranked discriminative
	features for tracking. Examples are presented that demonstrate how
	this method adapts to changing appearances of both tracked object
	and scene background. We note susceptibility of the variance ratio
	feature selection method to distraction by spatially correlated background
	clutter and develop an additional approach that seeks to minimize
	the likelihood of distraction.},
  comment = {trk_feature},
  file = {:papers\\2005 JNL, Online selection of discriminative tracking  features (Collins, PAMI, 335).pdf:PDF},
  keywords = {clutter feature extraction maximum likelihood estimation background
	clutter background discrimination task discriminative tracking features
	log likelihood ratios mean-shift tracking system object discrimination
	task online feature selection Algorithms Artificial Intelligence
	Computer Systems Discriminant Analysis Image Enhancement Image Interpretation,
	Computer-Assisted Information Storage and Retrieval Motion Movement
	Online Systems Pattern Recognition, Automated Subtraction Technique
	Video Recording},
  owner = {salman},
  timestamp = {00,350}
}

@ARTICLE{1994_JNL_Recog_shape_Rohr,
  author = {Rohr, K.},
  title = {Towards model-based recognition of human movements in image sequences},
  journal = {CVGIP: Image Understanding},
  year = {1994},
  volume = {59},
  pages = {94-115},
  abstract = {The interpretation of the movements of articulated bodies in image
	sequences is one of the most challenging problems in computer vision.
	The author introduces a model-based approach for the recognition
	of pedestrians. He represents the human body by a 3D-model consisting
	of cylinders, whereas for modelling the movement of walking he uses
	data from medical motion studies. The estimation of model parameters
	in consecutive images is done by applying a Kalman filter. Experimental
	results are shown for synthetic as well as for real image data},
  comment = {recog_action},
  file = {:papers\\1994 JNL, Towards model-based recognition of human movements in image sequences (Rohr, CVGIP, 354).pdf:PDF},
  keywords = {computer vision image sequences Kalman filters motion estimation parameter
	estimation},
  owner = {salman},
  timestamp = {00,300}
}

@ARTICLE{1994_JNL_ActiveContours_Ronfard,
  author = {Ronfard, R.},
  title = {Region based strategies for active contour models},
  journal = {International Journal of Computer Vision},
  year = {1994},
  volume = {13},
  pages = {229-251},
  number = {2},
  comment = {trk_contour},
  owner = {salman},
  timestamp = {00,350}
}

@INPROCEEDINGS{1999_CNF_3DtrajectoryRecoveryTracking_Rosales,
  author = {Rosales, R. and Sclaroff, S.},
  title = {3D trajectory recovery for tracking multiple objects and trajectory
	guided recognition of actions},
  booktitle = {Computer Vision and Pattern Recognition, 1999. IEEE Computer Society
	Conference on.},
  year = {1999},
  volume = {2},
  pages = {123 Vol. 2},
  abstract = {A mechanism is proposed that integrates low-level (image processing),
	mid-level (recursive 3D trajectory estimation), and high level (action
	recognition) processes. It is assumed that the system observes multiple
	moving objects via a single, uncalibrated video camera. A novel extended
	Kalman filter formulation is used in estimating the relative 3D motion
	trajectories up to a scale factor. The recursive estimation process
	provides a prediction and error measure that is exploited in higher-level
	stages of action recognition. Conversely, higher-level mechanisms
	provide feedback that allows the system to reliable segment and maintain
	the tracking of moving objects before, during, and after occlusion.
	The 3D trajectory, occlusion, and segmentation information are utilized
	in extracting stabilized views of the moving object. Trajectory-guided
	recognition (TGR) is proposed as a new and efficient method for adaptive
	classification of action. The TGR approach is demonstrated using
	&ldquo;motion history images&rdquo; that are then recognized via
	a mixture of Gaussian classifier. The system was tested in recognizing
	various dynamic human outdoor activities; e.g., running, walking,
	roller blading, and cycling. Experiments with synthetic data sets
	are used to evaluate stability of the trajectory estimator with respect
	to noise},
  comment = {recog_action},
  file = {:papers\\1999 CNF, 3D trajectory recovery for tracking multiple objects and trajectory guided recognition of actions  (Rosales, Sclaroff, CVPR, 109).pdf:PDF},
  keywords = {image recognition motion estimation 3D trajectory recovery Kalman
	filter formulation adaptive classification motion history images
	multiple objects trajectory guided recognition},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{2008_JNL_subspaceTRK_Ross,
  author = {Ross, David and Lim, Jongwoo and Lin, Ruei-Sung and Yang, Ming-Hsuan},
  title = {Incremental Learning for Robust Visual Tracking},
  journal = {International Journal of Computer Vision},
  year = {2008},
  volume = {77},
  pages = {125-141},
  note = {10.1007/s11263-007-0075-7},
  abstract = {Visual tracking, in essence, deals with non-stationary image streams
	that change over time. While most existing algorithms are able to
	track objects well in controlled environments, they usually fail
	in the presence of significant variation of the objects appearance
	or surrounding illumination. One reason for such failures is that
	many algorithms employ fixed appearance models of the target. Such
	models are trained using only appearance data available before tracking
	begins, which in practice limits the range of appearances that are
	modeled, and ignores the large volume of information (such as shape
	changes or specific lighting conditions) that becomes available during
	tracking. In this paper, we present a tracking method that incrementally
	learns a low-dimensional subspace representation, efficiently adapting
	online to changes in the appearance of the target. The model update,
	based on incremental algorithms for principal component analysis,
	includes two important features: a method for correctly updating
	the sample mean, and a forgetting factor to ensure less modeling
	power is expended fitting older observations. Both of these features
	contribute measurably to improving overall tracking performance.
	Numerous experiments demonstrate the effectiveness of the proposed
	tracking algorithm in indoor and outdoor environments where the target
	objects undergo large changes in pose, scale, and illumination.},
  affiliation = {University of Toronto 10 Kings College Road Toronto ON M55 3G4 Canada},
  comment = {TRK_subspace},
  file = {:papers\\2008 JNL, Incremental Learning for Robust Visual Tracking (Ross).pdf:PDF},
  issn = {0920-5691},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Netherlands},
  timestamp = {00,130},
  url = {http://dx.doi.org/10.1007/s11263-007-0075-7}
}

@INPROCEEDINGS{2004_SubspaceTRK_Ross,
  author = {Ross, D. and Jongwoo Lim and Ming-Hsuan Yang},
  title = {Adaptive probabilistic visual tracking with incremental subspace
	update},
  booktitle = {Computer Vision - ECCV 2004. 8th European Conference on Computer
	Vision. Proceedings (Lecture Notes in Comput. Sci. Vol.3022)},
  year = {2004},
  volume = {Vol.2},
  pages = {470 - 82},
  address = {Berlin, Germany},
  abstract = {Visual tracking, in essence, deals with non-stationary data streams
	that change over time. While most existing algorithms are able to
	track objects well in controlled environments, they usually fail
	if there is a significant change in object appearance or surrounding
	illumination. The reason being that these visual tracking algorithms
	operate on the premise that the models of the objects being tracked
	are invariant to internal appearance change or external variation
	such as lighting or viewpoint. Consequently most tracking algorithms
	do not update the models once they are built or learned at the outset.
	In this paper, we present an adaptive probabilistic tracking algorithm
	that updates the models using an incremental update of eigenbasis.
	To track objects in two views, we use an effective probabilistic
	method for sampling affine motion parameters with priors and predicting
	its location with a maximum a posteriori estimate. Borne out by experiments,
	we demonstrate the proposed method is able to track objects well
	under large lighting, pose and scale variation with close to real-time
	performance},
  comment = {TRK_subspace},
  copyright = {Copyright 2004, IEE},
  file = {:papers\\2004 CNF, Adaptive probabilistic visual tracking with incremental subspace (Ross).pdf:PDF},
  journal = {Computer Vision - ECCV 2004. 8th European Conference on Computer
	Vision. Proceedings (Lecture Notes in Comput. Sci. Vol.3022)},
  keywords = {adaptive estimation;Bayes methods;computer vision;eigenvalues and
	eigenfunctions;image sequences;maximum likelihood estimation;motion
	estimation;probability;singular value decomposition;target tracking;video
	signal processing;},
  language = {English},
  timestamp = {00,060}
}

@ARTICLE{1996_JNL_StatisticalMosaicsTracking_Rowe,
  author = {Rowe, S. and Blake, A.},
  title = {Statistical mosaics for tracking},
  journal = {Image and Vision Computing},
  year = {1996},
  volume = { 14},
  pages = {549 - 64},
  number = { 8},
  abstract = {A method of robust feature-detection is proposed for visual tracking
	with a pan-tilt head. Even with good foreground models, the tracking
	process is liable to be disrupted by strong features in the background.
	Previous researchers have shown that the disruption can be somewhat
	suppressed by the use of image-subtraction. Building on this idea,
	a more powerful statistical model of background intensity is proposed
	in which a Gaussian mixture distribution is fitted to each of the
	pixels on a `virtual' image plane. A fitting algorithm of the `Expectation-Maximisation'
	type proves to be particularly effective here. Practical tests with
	contour tracking show marked improvement over image subtraction methods.
	Since the burden of computation is off-line, the online tracking
	process can run in real-time, at video field-rate},
  address = {Netherlands},
  comment = {trk},
  copyright = {Copyright 1996, IEE},
  file = {:papers\\1996 JNL, Statistical mosaics for tracking (Rowe, Blake, IVC, 44).pdf:PDF},
  issn = {0262-8856},
  keywords = {computational geometry;computer vision;},
  language = {English},
  timestamp = {00,050},
  url = {http://dx.doi.org/10.1016/0262-8856(96)01103-1}
}

@ARTICLE{1999_JNL_Gaussian_roweis,
  author = {Roweis, S. and Ghahramani, Z.},
  title = {A unifying review of linear Gaussian models},
  journal = {Neural computation},
  year = {1999},
  volume = {11},
  pages = {305--345},
  number = {2},
  comment = {00,500},
  file = {:papers\\1999 JNL, A Unifying Review of Linear Gaussian Models (Roweis).pdf:PDF},
  publisher = {MIT Press}
}

@ARTICLE{1998_JNL_NNfaceDetection_Rowley,
  author = {Rowley, H. A. and Baluja, S. and Kanade, T.},
  title = {Neural network-based face detection},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1998},
  volume = {20},
  pages = {23-38},
  number = {1},
  abstract = {We present a neural network-based upright frontal face detection system.
	A retinally connected neural network examines small windows of an
	image and decides whether each window contains a face. The system
	arbitrates between multiple networks to improve performance over
	a single network. We present a straightforward procedure for aligning
	positive face examples for training. To collect negative examples,
	we use a bootstrap algorithm, which adds false detections into the
	training set as training progresses. This eliminates the difficult
	task of manually selecting nonface training examples, which must
	be chosen to span the entire space of nonface images. Simple heuristics,
	such as using the fact that faces rarely overlap in images, can further
	improve the accuracy. Comparisons with several other state-of-the-art
	face detection systems are presented, showing that our system has
	comparable performance in terms of detection and false-positive rates},
  comment = {det_face},
  file = {:papers\\1998 JNL, Neural network-based face detection (Rowley, Baluja, Kanade, PAMI, 2637).pdf:PDF},
  keywords = {computer vision face recognition filtering theory image processing
	equipment neural nets accuracy bootstrap algorithm detection rates
	false detections false-positive rates neural network-based upright
	frontal face detection system retinally connected neural network},
  owner = {salman},
  timestamp = {02,600}
}

@BOOK{2003_BOOK_AI_Russell,
  title = {Artificial Intelligence: A Modern Approach},
  publisher = {Prentice-Hall, Englewood Cliffs, NJ},
  year = {2003},
  author = {Stuart Russell and Peter Norvig},
  edition = {2nd edition},
  comment = {PRML_book},
  file = {:papers\\2003 BOOK, Artificial Intelligence - A Modern Approach (Russell, Norvig, 2ed).pdf:PDF},
  timestamp = {-}
}

@ARTICLE{10576847,
  author = {Sabeti, L. and Parvizi, E. and Wu, Q.M.J.},
  title = {Visual tracking using color cameras and time-of-flight range imaging
	sensors},
  journal = {Journal of Multimedia},
  year = {2008},
  volume = { 3},
  pages = {28 - 36},
  number = { 2},
  note = {color cameras;time-of-flight range imaging sensors;particle filter-based
	visual trackers;},
  abstract = {This work proposes two particle filter-based visual trackers - one
	using output images from a color camera and the other using images
	from a time-of-flight range imaging sensor. These proposed trackers
	were compared in order to identify the advantages and drawbacks of
	utilizing output images from the color camera as opposed to output
	from the time-of-flight range imaging sensor for the most efficient
	visual tracking. This paper is also unique in its novel mixture of
	efficient methods to produce two stable and reliable human trackers
	using the two cameras.},
  address = {Finland},
  comment = {trk_color},
  copyright = {Copyright 2009, The Institution of Engineering and Technology},
  file = {:papers\\2008 JNL, Visual tracking using color cameras and time-of-flight range imaging sensors (Sabeti, Parvizi, Wu).pdf:PDF},
  issn = {1796-2048},
  keywords = {cameras;image sensors;particle filtering (numerical methods);particle
	track visualisation;tracking;},
  language = {English},
  timestamp = {-}
}

@ARTICLE{1984_JNL_PVQ_Sabin,
  author = {Sabin, M. and Gray, R.},
  title = {Product code vector quantizers for waveform and voice coding},
  journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  year = {1984},
  volume = {32},
  pages = { 474 - 488},
  number = {3},
  abstract = { Memory and computation requirements imply fundamental limitations
	on the quality that can be achieved in vector quantization systems
	used for speech waveform coding and linear predictive voice coding
	(LPC). One approach to reducing storage and computation requirements
	is to organize the set of reproduction vectors as the Cartesian product
	of a vector codebook describing the shape of each reproduction vector
	and a scalar codebook describing the gain or energy. Such shape-gain
	vector quantizers can be applied both to waveform coding using a
	quadratic-error distortion measure and to voice coding using an Itakura-Saito
	distortion measure. In each case, the minimum distortion reproduction
	vector can be found by first selecting a shape code-word, and then,
	based on that choice, selecting a gain codeword. Several algorithms
	are presented for the design of shape-gain vector quantizers based
	on a traning sequence of data or a probabilistic model. The algorithms
	are used to design shape-gain vector quantizers for both the waveform
	coding and voice coding application. The quantizers are simulated,
	and their performance is compared to that of previously reported
	vector quantization systems.},
  comment = {VQ},
  file = {:papers\\1984 JNL, Product code vector quantizers for waveform and voice coding (Sabin, Gray).pdf:PDF},
  issn = {0096-3518},
  timestamp = {00,100}
}

@ARTICLE{2009_JNL_MPEG4JointSourceChannelDistortion_Sabir,
  author = {Sabir, M.F. and Heath, R.W. and Bovik, A.C.},
  title = {Joint Source-Channel Distortion Modeling for MPEG-4 Video},
  journal = {Image Processing, IEEE Transactions on},
  year = {2009},
  volume = {18},
  pages = {90-105},
  number = {1},
  month = {Jan. },
  doi = {10.1109/TIP.2008.2005819},
  issn = {1057-7149},
  keywords = {channel estimation, code standards, combined source-channel coding,
	data compression, error statistics, multimedia communication, quantisation
	(signal), rate distortion theory, video coding, video streamingJSCC
	methods, MPEG-4 compressed video streams, channel bit error rates,
	channel distortion estimation, image communication, joint source-channel
	distortion modeling, multimedia communication, operational rate-distortion
	curves, quantization, source coding rates, video communication}
}

@ARTICLE{2006_WHITE_stab_Sachs,
  author = {Sachs, D. and Nasiri, S. and Goehl, D.},
  title = {Image stabilization technology overview},
  journal = {InvenSense Whitepaper},
  year = {2006}
}

@ARTICLE{2000_JNL_SURVEYimageRetrieval_Safar,
  author = {Safar, M. and Shahabi, C. and Sun, X.},
  title = {Image retrieval by shape: a comparative study},
  journal = {Multimedia and Expo, 2000. ICME 2000. 2000 IEEE International Conference
	on},
  year = {2000},
  volume = {1},
  pages = {141-144 vol.1},
  abstract = {Besides traditional applications (e.g. CAD/CAM and trademark registry),
	new multimedia applications, such as structured video, animation
	and the MPEG-7 standard, require the storage and management of well-defined
	objects. We focus on shape-based object retrieval and conduct a comparison
	study on four such techniques: Fourier descriptors (FD), the grid-based
	(GB) method, Delaunay triangulation (DT) and MBC-TPVAS (minimum bounding
	circles/touch-point vertex-angle sequence). Our results show that
	the similarity retrieval accuracy of our method (TVPAS) is as good
	as the other methods, while it has the lowest computational cost
	for generating the shape signatures of the objects. Moreover, it
	has low storage requirements and a comparable computation cost to
	compute the similarity between two shape signatures. In addition,
	TPVAS requires no normalization of the objects and is the only method
	that has direct support for RST (rotation/scaling/translation) query
	types. In this paper, we also introduce a new shape description taxonomy},
  doi = {10.1109/ICME.2000.869564},
  keywords = {edge detection, image retrieval, multimedia databasesDelaunay triangulation,
	Fourier descriptors, MBC-TPVAS, MPEG-7 standard, RST query types,
	animation, computational cost, grid-based method, minimum bounding
	circles, multimedia applications, shape description taxonomy, shape
	signature generation, shape-based image retrieval, shape-based object
	retrieval, similarity retrieval accuracy, structured video, touch-point
	vertex-angle sequence}
}

@ARTICLE{1999_JNL_PCAvideoAnalysis_Zakhor,
  author = {Sahouria, E. and Zakhor, A.},
  title = {Content analysis of video using principal components},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  year = {1999},
  volume = {9},
  pages = {1290-8},
  abstract = {We use principal component analysis (PCA) to reduce the dimensionality
	of features of video frames for the purpose of content description.
	This low-dimensional description makes practical the direct use of
	all the frames of a video sequence in later analysis. The PCA representation
	circumvents or eliminates several of the stumbling blocks in current
	analysis methods and makes new analyses feasible. We demonstrate
	this with two applications. The first accomplishes high-level scene
	description without shot detection and key-frame selection. The second
	uses the time sequences of motion data from every frame to classify
	sports sequences},
  comment = {analysis},
  file = {:papers\\1999 JNL, Content Analysis of Video Using Principal Components (Sahouria, Zakhor, TCSVT, 114).pdf:PDF},
  keywords = {image motion analysis image representation image sequences principal
	component analysis video signal processing},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{1990_JNL_PointCorresp_Salari,
  author = {Salari, V. and Sethi, I. K.},
  title = {Feature point correspondence in the presence of occlusion},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1990},
  volume = {12},
  pages = {87-91},
  number = {1},
  comment = {trk_correspondence},
  file = {:papers\\1990 JNL, Feature point correspondence in the presence of occlusion (Salari, Sethi, PAMI, 113).pdf:PDF},
  keywords = {iterative methods optimisation pattern recognition feature point correspondence
	feature point detection iterative optimization multiple frames occlusion},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{2004_JNL_VelocityTransform_SatoAggarwal,
  author = {Sato, Koichi and Aggarwal, J. K.},
  title = {Temporal spatio-velocity transform and its application to tracking
	and interaction},
  journal = {Computer Vision and Image Understanding},
  year = {2004},
  volume = {96},
  pages = {100-128},
  number = {2},
  abstract = {This paper describes the temporal spatio-velocity (TSV) transform
	for extracting pixel velocities from binary image sequences. The
	TSV transform is derived from the Hough transform over windowed spatio-temporal
	images. We present the methodology of the transform and its implementation
	in an iterative computational form. The intensity at each pixel in
	the TSV image represents a measure of the likelihood of occurrence
	of a pixel with instantaneous velocity in the current position. Binarization
	of the TSV image extracts blobs based on the similarity of velocity
	and position. The TSV transform provides an efficient way to remove
	noise by focusing on stable velocities, and constructs noise-free
	blobs. We apply the transform to tracking human figures in a sidewalk
	environment and extend its use to an interaction recognition system.
	The system performs background subtraction to separate the foreground
	image from the background, extracts standing human objects and generates
	a one-dimensional binary image sequence. The TSV transform takes
	the one-dimensional image sequence and yields the TSV images. Thresholding
	of the TSV image generates the human blobs. We obtain the human trajectories
	by associating the segmented blobs over time using blob features.
	We analyze the motion-state transitions of human interactions, which
	we consider to be combinations of ten simple interaction units (SIUs).
	Our system recognizes the 10 SIUs by analyzing the shape of the human
	trajectory. We illustrate the TSV transform and its application to
	real images for human segmentation, tracking and interaction classification.},
  comment = {trk_people},
  file = {:papers\\2004 JNL, Temporal spatio-velocity transform and its application to tracking and interaction (Sato, Aggarwal, CVIU, 28).pdf:PDF},
  keywords = {Temporal spatio-velocity transform Hough transform Spatio-temporal
	Windowing Human segmentation Tracking Interaction recognition},
  owner = {salman},
  timestamp = {-}
}

@BOOK{2005_BOOK_DataCompression_Sayood,
  title = {Introduction to Data Compression},
  publisher = {Morgan Kaufmann},
  year = {2005},
  author = {Sayood, Khalid},
  edition = {3},
  abstract = {{Khalid Sayood's textbook-style <I>Introduction to Data Compression</I>
	is the definitive guide to all kinds of compression schemes. Early
	chapters establish the mathematics involved in basic compression
	techniques, including lossless and lossy compression as well as the
	fundamentals of information theory that lay the groundwork for common
	forms of compression. (The book contains all the relevant formulas,
	although those who don't need such mathematical detail will still
	be able to understand the book.)<p> A good portion of the book examines
	various compression schemes, their strengths and weaknesses, and
	what content they work best for. <I>Introduction to Data Compression</I>
	begins with lossless compression schemes, which lose no information
	during the compression/decompression process. Huffman Coding, a well-established
	compression scheme, and arithmetic and dictionary coding also receive
	excellent treatment. In addition, the author takes on lossless compression
	for images.<p> For lossy compression, Sayood discusses schemes that
	use quantization, where a range of values is compressed in some way.
	He also describes scalar, vector, and differential encoding and fractal
	compression. A final chapter looks at video encryption (which often
	combines techniques from earlier chapters). Many of the compression
	schemes include examples from image and sound files, but the book
	considers a wide variety of video schemes too. This rich and confidently
	written text collates a lot of research and can serve as both textbook
	and source for designers who need a readable and mathematically solid
	introduction to data compression.} {Each edition of Introduction
	to Data Compression has widely been considered the best introduction
	and reference text on the art and science of data compression, and
	the third edition continues in this tradition. Data compression techniques
	and technology are ever-evolving with new applications in image,
	speech, text, audio, and video. The third edition includes all the
	cutting edge updates the reader will need during the work day and
	in class. <br><br>Khalid Sayood provides an extensive introduction
	to the theory underlying todays compression techniques with detailed
	instruction for their applications using several examples to explain
	the concepts. Encompassing the entire field of data compression Introduction
	to Data Compression, includes lossless and lossy compression, Huffman
	coding, arithmetic coding, dictionary techniques, context based compression,
	scalar and vector quantization. Khalid Sayood provides a working
	knowledge of data compression, giving the reader the tools to develop
	a complete and concise compression package upon completion of his
	book.<br><br>* New content added on the topic of audio compression
	including a description of the mp3 algorithm<br>* New video coding
	standard and new facsimile standard explained<br>* Completely explains
	established and emerging standards in depth including JPEG 2000,
	JPEG-LS, MPEG-2, Group 3 and 4 faxes, JBIG 2, ADPCM, LPC, CELP, and
	MELP<br>* Source code provided via companion web site that gives
	readers the opportunity to build their own algorithms, choose and
	implement techniques in their own applications}},
  comment = {IT_book},
  day = {15},
  file = {:papers\\2005 BOOK, Introduction to Data Compression (Sayood, MorganKaufmann 3rd ed).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {012620862X},
  posted-at = {2010-04-15 15:24:13},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/012620862X}
}

@ARTICLE{9136286,
  author = {Schiele, B.},
  title = {Model-free tracking of cars and people based on color regions},
  journal = {Image and Vision Computing},
  year = {2006},
  volume = { 24},
  pages = {1172 - 8},
  number = { 11},
  note = {object extraction;image sequence;static camera;wearable computing;model-free
	tracking;color image region;image segmentation;surveillance;},
  abstract = {This paper exploits a simple but general technique to extract object
	models from arbitrary image sequences. Such object models can be
	used to structure and index the image sequence. The algorithm extracts
	and tracks homogenous regions, which may correspond to objects or
	object parts. By grouping similar moving regions, the algorithm constructs
	models of potential objects. As such, the approach is model-free
	in the sense that it does not use a priori models to detect, track,
	and segment objects. On the contrary, the ultimate goal of the approach
	is to build such models automatically from image sequences. In this
	paper, the approach is applied to an image sequence taken by a static
	camera overlooking a parking lot. Due to the general formulation
	of the approach it can be used to extract any object from image sequences
	including cars and people. Tracking results for cars and people are
	reported. For evaluation purposes all participants of the PETS 2000
	workshop<sup>1</sup> were given the same image sequences. [All rights
	reserved Elsevier]},
  address = {Netherlands},
  comment = {trk_color},
  copyright = {Copyright 2006, The Institution of Engineering and Technology},
  file = {:papers\\2006 JNL, Model-free tracking of cars and people based on color regions (Schiele).pdf:PDF},
  issn = {0262-8856},
  keywords = {feature extraction;image colour analysis;image segmentation;image
	sequences;object detection;optical tracking;surveillance;},
  language = {English},
  timestamp = {00,020},
  url = {http://dx.doi.org/10.1016/j.imavis.2005.06.003}
}

@ARTICLE{2010_JNL_TRK_deform_Schoenemann,
  author = {Schoenemann, T. and Cremers, D.},
  title = {A Combinatorial Solution for Model-Based Image Segmentation and Real-Time
	Tracking},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2010},
  volume = {32},
  pages = {1153 -1164},
  number = {7},
  month = july,
  abstract = {We propose a combinatorial solution to determine the optimal elastic
	matching of a deformable template to an image. The central idea is
	to cast the optimal matching of each template point to a corresponding
	image pixel as a problem of finding a minimum cost cyclic path in
	the three-dimensional product space spanned by the template and the
	input image. We introduce a cost functional associated with each
	cycle, which consists of three terms: a data fidelity term favoring
	strong intensity gradients, a shape consistency term favoring similarity
	of tangent angles of corresponding points, and an elastic penalty
	for stretching or shrinking. The functional is normalized with respect
	to the total length to avoid a bias toward shorter curves. Optimization
	is performed by Lawler's Minimum Ratio Cycle algorithm parallelized
	on state-of-the-art graphics cards. The algorithm provides the optimal
	segmentation and point correspondence between template and segmented
	curve in computation times that are essentially linear in the number
	of pixels. To the best of our knowledge, this is the only existing
	globally optimal algorithm for real-time tracking of deformable shapes.},
  comment = {TRK},
  doi = {10.1109/TPAMI.2009.79},
  file = {:papers\\2010 JNL, A Combinatorial Solution for Model-Based Image Segmentation and Real-Time Tracking (Schoenemann).pdf:PDF},
  issn = {0162-8828},
  keywords = {Lawler minimum ratio cycle;combinatorial solution;cost functional;data
	fidelity term;deformable shapes tracking;elastic penalty;image pixel;minimum
	cost cyclic path;model-based image segmentation;optimal matching;shape
	consistency term;combinatorial mathematics;image matching;image segmentation;minimisation;},
  timestamp = {-}
}

@ARTICLE{1999_JNL_TRKocclusion_Scholl,
  author = {Brian J. Scholl and Zenon W. Pylyshyn},
  title = {Tracking Multiple Items Through Occlusion: Clues to Visual Objecthood,
	},
  journal = {Cognitive Psychology},
  year = {1999},
  volume = {38},
  pages = {259 - 290},
  number = {2},
  abstract = {In three experiments, subjects attempted to track multiple items as
	they moved independently and unpredictably about a display. Performance
	was not impaired when the items were briefly (but completely) occluded
	at various times during their motion, suggesting that occlusion is
	taken into account when computing enduring perceptual objecthood.
	Unimpaired performance required the presence of accretion and deletion
	cues along fixed contours at the occluding boundaries. Performance
	was impaired when items were present on the visual field at the same
	times and to the same degrees as in the occlusion conditions, but
	disappeared and reappeared in ways which did not implicate the presence
	of occluding surfaces (e.g., by imploding and exploding into and
	out of existence instead of accreting and deleting along a fixed
	contour). Unimpaired performance didnotrequire visible occluders
	(i.e., Michotte'stunnel effect) or globally consistent occluder positions.
	We discuss implications of these results for theories of objecthood
	in visual attention.},
  comment = {TRK_occlusion},
  issn = {0010-0285},
  timestamp = {00,200}
}

@ARTICLE{1986_JNL_ImageFlowConstraint_Schunck,
  author = {Schunck, B. G.},
  title = {The image flow constraint equation},
  journal = {Computer Vision, Graphics, and Image Processing},
  year = {1986},
  volume = {35},
  pages = {20-46},
  number = {Copyright 1986, IEE},
  abstract = {The image flow field is the velocity field in the image plane due
	to the motion of the observer or objects in the scene, or due to
	apparent motion. There is a fundamental equation describing image
	flow that is called the image flow constraint equation in this paper.
	A new derivation of the image flow constraint equation is presented
	that shows that the equation is valid at image irradiance discontinuities
	and motion boundaries. A clearer discussion of the conditions for
	validity of the image flow constraint equation is presented. The
	image flow constraint equation is reformulated in polar coordinates.
	Several advantages of this representation are discussed. The polar
	representation is compared with directional selectivity. It is shown
	that in the case where the image is sampled in time to form an image
	sequence, the image flow constraint equation is incorrect at occluding
	boundaries to a far greater degree than has been assumed},
  comment = {?},
  file = {:papers\\1986 JNL, The image flow constraint equation (Schunck, CVGIP, 84).pdf:PDF},
  keywords = {picture processing},
  timestamp = {00,100}
}

@INPROCEEDINGS{2002_CNF_FastTemplateMatching_SchweitzerBellWu,
  author = {Schweitzer, H. and Bell, J. W. and Wu, F.},
  title = {Very fast template matching},
  booktitle = {Computer Vision - ECCV 2002. 7th European Conference on Computer
	Vision. Proceedings, Part IV, 28-31 May 2002},
  year = {2002},
  abstract = {Template matching by normalized correlations is a common technique
	for determine the existence and compute the location of a shape within
	an image. In many cases the run time of computer vision applications
	is dominated by repeated computation of template matching, applied
	to locate multiple templates in varying scale and orientation. A
	straightforward implementation of template matching for an image
	size n and a template size k requires order of kn operations. There
	are fast algorithms that require order of n log n operations. We
	describe a new approximation scheme that requires order n operations.
	It is based on the idea of "Integral-Images", introduced by Viola
	and Jones (2001)},
  comment = {trk_region},
  file = {:papers\\2002 CNF Very fast template matching (Schweitzer, Bell, Wu, ECCV, 22).pdf:PDF},
  keywords = {computational complexity computer vision image matching},
  timestamp = {-}
}

@ELECTRONIC{PRES_CoveringPacking_Schurmann,
  author = {Achill Schurmann},
  title = {The Sphere Covering Problem},
  howpublished = {http://fma2.math.uni-magdeburg.de/~achill/mfo/slides_achill_lecture_III.pdf},
  file = {:papers\\presentation_Packing_vs_covering.pdf:PDF},
  owner = {salman},
  timestamp = {2011.04.11}
}

@MISC{2007_VID_kernel_Scholkpof,
  author = {Bernhard Schlkopf},
  title = {Introduction to kernel methods},
  howpublished = {\url{http://videolectures.net/mlss07_scholkopf_intkmet/}},
  year = {2007}
}

@INPROCEEDINGS{2004_CNF_ProbabilisticTrackingMultipleFeatures_Serby,
  author = {Serby, D. and Meier, E. K. and Van Gool, L.},
  title = {Probabilistic object tracking using multiple features},
  booktitle = {Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International
	Conference on},
  year = {2004},
  volume = {2},
  pages = {184-187 Vol.2},
  comment = {trk},
  file = {:papers\\2004 CNF, Probabilistic object tracking using multiple features (Serby, Meier, Vangool, ICPR, 29).pdf:PDF},
  keywords = {image sequences object detection probability affine transformations
	nonGaussian estimation problems nonlinear estimation problems particle
	filter framework probabilistic object tracking},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{1987_JNL_FeatureTrajectories_Sethi,
  author = {Sethi, Ishwar K. and Jain, Ramesh},
  title = {Finding Trajectories of Feature Points in a Monocular Image Sequence},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1987},
  volume = {PAMI-9},
  pages = {56-73},
  number = {1},
  abstract = {Identifying the same physical point in more than one image, the correspondence
	problem, is vital in motion analysis. Most research for establishing
	correspondence uses only two frames of a sequence to solve this problem.
	By using a sequence of frames, it is possible to exploit the fact
	that due to inertia the motion of an object cannot change instantaneously.
	By using smoothness of motion, it is possible to solve the correspondence
	problem for arbitrary motion of several nonrigid objects in a scene.
	We formulate the correspondence problem as an optimization problem
	and propose an iterative algorithm to find trajectories of points
	in a monocular image sequence. A modified form of this algorithm
	is useful in case of occlusion also. We demonstrate the efficacy
	of this approach considering synthetic, laboratory, and real scenes.},
  comment = {trk_correspondence},
  file = {:papers\\1987 JNL, Finding Trajectories of Feature Points in a Monocular Image Sequence (Sethi, Jain, PAMI, 327).pdf:PDF},
  owner = {salman},
  timestamp = {00,300}
}

@ARTICLE{2005_JNL_PointTRK_Shafique,
  author = {Shafique, K. and Shah, M.},
  title = {A noniterative greedy algorithm for multiframe point correspondence},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2005},
  volume = {27},
  pages = {51 -65},
  number = {1},
  month = {jan. },
  abstract = {This work presents a framework for finding point correspondences in
	monocular image sequences over multiple frames. The general problem
	of multiframe point correspondence is NP-hard for three or more frames.
	A polynomial time algorithm for a restriction of this problem is
	presented and is used as the basis of the proposed greedy algorithm
	for the general problem. The greedy nature of the proposed algorithm
	allows it to be used in real-time systems for tracking and surveillance,
	etc. In addition, the proposed algorithm deals with the problems
	of occlusion, missed detections, and false positives by using a single
	noniterative greedy optimization scheme and, hence, reduces the complexity
	of the overall algorithm as compared to most existing approaches
	where multiple heuristics are used for the same purpose. While most
	greedy algorithms for point tracking do not allow the entry and exit
	of the points from the scene, this is not a limitation for the proposed
	algorithm. Experiments with real and synthetic data over a wide range
	of scenarios and system parameters are presented to validate the
	claims about the performance of the proposed algorithm.},
  comment = {trk_correspondence},
  doi = {10.1109/TPAMI.2005.1},
  file = {:papers\\2005 JNL, A noniterative greedy algorithm for multiframe point correspondence (Shafique, Mubarakshah, PAMI, 79).pdf:PDF},
  issn = {0162-8828},
  keywords = {NP-hard problems;monocular image sequences;multiframe point correspondence;multiple
	heuristics;noniterative greedy algorithm;occlusion;optimization;point
	tracking algorithm;polynomial time algorithm;real time systems;surveillance;computational
	complexity;greedy algorithms;image sequences;optimisation;polynomials;real-time
	systems;tracking;Algorithms;Artificial Intelligence;Computer Graphics;Computer
	Simulation;Image Enhancement;Image Interpretation, Computer-Assisted;Information
	Storage and Retrieval;Models, Biological;Models, Statistical;Movement;Numerical
	Analysis, Computer-Assisted;Pattern Recognition, Automated;Reproducibility
	of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Subtraction
	Technique;User-Computer Interface;Video Recording;},
  timestamp = {00,080}
}

@ARTICLE{2003_JNL_SURVEYbeh_Shah,
  author = {Shah, M.},
  title = {Understanding human behavior from motion imagery},
  journal = {Machine Vision and Applications},
  year = {2003},
  volume = {14},
  pages = {210-14},
  number = {Copyright 2004, IEE},
  abstract = {Computer vision is gradually making the transition from image understanding
	to video understanding. This is due to the enormous success in analyzing
	sequences of images that has been achieved in recent years. The main
	shift in the paradigm has been from recognition followed by reconstruction
	(shape from X) to motion-based recognition. Since the most videos
	are about people, this work has focused on the analysis of human
	motion. In this paper, I present my perspective on understanding
	human behavior. Automatically understanding human behavior from motion
	imagery involves extraction of relevant visual information from a
	video sequence, representation of that information in a suitable
	form, and interpretation of visual information for the purpose of
	recognition and learning about human behavior. Significant progress
	has been made in human tracking over the last few years. As compared
	with tracking, not much progress has been made in understanding human
	behavior, and the issue of representation has largely been ignored.
	I present my opinion on possible reasons and hurdles for slower progress
	in understanding human behavior, briefly present our work in tracking,
	representation, and recognition, and comment on the next steps in
	all three areas},
  comment = {survey},
  file = {:papers\\2003 JNL, Understanding human behavior from motion imagery (SURVEY, Mubarak Shah, MVA, 21).pdf:PDF},
  keywords = {behavioural sciences computing computer vision human factors image
	motion analysis image representation image sequences},
  owner = {salman},
  review = {No experiments, just nice discussion.},
  timestamp = {-}
}

@ARTICLE{1948_JNL_Comm_Shannon,
  author = {Shannon, C.E.},
  title = {A mathematical theory of communication},
  journal = {Bell System Technical Journal},
  year = {1948},
  volume = {27},
  pages = {379 - 423},
  abstract = {The effect of noise in modifying the established relations in communication
	channels is examined first in the case of discrete systems (e.g.
	morse code) and then extended to continuous (e.g. television) and
	mixed (e.g. p.c.m.) systems. A definition is adopted for the signal
	which can afterwards embrace noise as well. The signal is derived
	from English by a stochastic process, in which a "model" of English
	is produced as a sequence of symbols governed by a set of probabilities
	as to the occurrence of single letters, groups of letters, etc. Information
	is measured in terms of the binary digit or "bit" which provides
	the barest intelligence of the "yes-no" variety. Messages are considered
	merely as samples of the vast resources of language and hence admit
	of a statistical treatment. The existence of preferred structures
	in language allows the sample to be further reduced by codifying.
	The max. compression possible in a language when encoded into the
	same alphabet is called the relative entropy <i>r</I> and a consistent
	definition of this measure is given. 1-<i>r</I> is the redundancy
	and some interesting sidelights are thrown on its significance in
	English prose and crossword puzzles. The capacity of a discrete channel
	is defined and it is shown that encoding the information cannot increase
	the rate of transmission beyond the ratio of capacity to entropy.
	Noise is then considered as a chance variable like the message and
	represented by a stochastic process. The average ambiguity of the
	resulting signal is measured by the average, weighted entropy of
	the message and called the equivocation. Ways are considered of transmitting
	the signal which are optimal in combating noise. It is shown that
	encoding the information can never reduce the equivocation below
	the difference between the entropy and the channel capacity. The
	concept is introduced of "matching" a source to a channel and an
	example of efficient coding is worked out. The treatment of continuous
	systems is prefaced by a full mathematical discussion of the implications
	involved in extending the previous definitions and the geometrical
	representation of a communication system is introduced. The capacity
	of a continuous channel is then calculated under various conditions
	of noise disturbance in terms of the s./n. ratio. Finally a criterion
	is deduced for the fidelity of transmission and a definition given
	for the rate of transmission. A more general and rigorous mathematical
	approach to the central definitions of communication theory is appended.},
  comment = {VQ},
  copyright = {Copyright 2004, IEE},
  file = {:papers\\1948 JNL, A mathematical theory of communication (Shannon).pdf:PDF},
  keywords = {information theory;},
  timestamp = {37,000}
}

@INCOLLECTION{1959_CNF_RD_Shannon,
  author = {Shannon, C. E.},
  title = {Coding theorems for a discrete source with a fidelity criterion},
  booktitle = {IRE Nat. Conv. Rec., Pt. 4},
  year = {1959},
  pages = {142--163},
  citeulike-article-id = {603526},
  comment = {SP},
  keywords = {classics, information-theory, rate-distortion, source-coding},
  posted-at = {2006-04-26 20:34:47},
  priority = {0},
  timestamp = {00,600}
}

@INPROCEEDINGS{2003_CNF_RecognitionGroupActivities_Shaogang,
  author = {Shaogang, Gong and Tao, Xiang},
  title = {Recognition of group activities using dynamic probabilistic networks},
  booktitle = {ICCV 2003: 9th International Conference on Computer Vision, 13-16
	Oct. 2003},
  year = {2003},
  abstract = {Dynamic Probabilistic Networks (DPNs) are exploited for modeling the
	temporal relationships among a set of different object temporal events
	in the scene for a coherent and robust scene-level behaviour interpretation.
	In particular, we develop a Dynamically Multi-Linked Hidden Markov
	Model (DML-HMM) to interpret group activities involving multiple
	objects captured in an outdoor scene. The model is based on the discovery
	of salient dynamic interlinks among multiple temporal events using
	DPNs. Object temporal events are detected and labeled using Gaussian
	Mixture Models with automatic model order selection. A DML-HMM is
	built using Schwarz's Bayesian Information Criterion based factorisation
	resulting in its topology being intrinsically determined by the underlying
	causality and temporal order among different object events. Our experiments
	demonstrate that its performance on modelling group activities in
	a noisy outdoor scene is superior compared to that of a Multi-Observation
	Hidden Markov Model (MOHMM), a Parallel Hidden Markov Model (PaHMM)
	and a Coupled Hidden Markov Model (CHMM)},
  comment = {recog_action_appearance},
  file = {:papers\\2003 CNF, Recognition of group activities using dynamic probabilistic networks (Xiang, ICCV, 119).pdf:PDF},
  keywords = {Bayes methods causality computer vision Gaussian processes hidden
	Markov models image recognition object detection probability topology},
  owner = {salman},
  timestamp = {00,100}
}

@BOOK{2001_BOOK_CV_Shapiro,
  title = {Computer Vision},
  publisher = {{Prentice Hall}},
  year = {2001},
  author = {Shapiro, Linda G. and Stockman, George C.},
  month = {January},
  comment = {CV_book},
  day = {23},
  file = {:papers\\2000 BOOK, Computer Vision (Shapiro, Stockman).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0130307963},
  keywords = {book, computervision},
  posted-at = {2008-10-11 02:58:33},
  priority = {3},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0130307963}
}

@ARTICLE{2005_JNL_BayesianObjectDetection_Sheikh,
  author = {Yaser Sheikh and Mubarak Shah},
  title = {Bayesian Modeling of Dynamic Scenes for Object Detection},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2005},
  volume = {27},
  pages = {1778-1792},
  address = {Los Alamitos, CA, USA},
  comment = {TRK_detection},
  doi = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2005.213},
  file = {:papers\\2005 JNL, Bayesian Modeling of Dynamic Scenes for Object Detection (Yaser Shiekh, Mubarak Shah).pdf:PDF},
  issn = {0162-8828},
  publisher = {IEEE Computer Society},
  timestamp = {00,120}
}

@ARTICLE{2000_JNL_NormalizedCuts_ShiMalik,
  author = {Shi, Jianbo and Malik, J.},
  title = {Normalized cuts and image segmentation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {888-905},
  number = {8},
  abstract = {We propose a novel approach for solving the perceptual grouping problem
	in vision. Rather than focusing on local features and their consistencies
	in the image data, our approach aims at extracting the global impression
	of an image. We treat image segmentation as a graph partitioning
	problem and propose a novel global criterion, the normalized cut,
	for segmenting the graph. The normalized cut criterion measures both
	the total dissimilarity between the different groups as well as the
	total similarity within the groups. We show that an efficient computational
	technique based on a generalized eigenvalue problem can be used to
	optimize this criterion. We applied this approach to segmenting static
	images, as well as motion sequences, and found the results to be
	very encouraging},
  comment = {seg},
  file = {:papers\\2000 JNL, Normalized cuts and image segmentation (PAMI, 3541).pdf:PDF},
  keywords = {computer vision eigenvalues and eigenfunctions graph theory image
	segmentation image sequences dissimilarity eigenvalues graph partitioning
	normalized cut perceptual grouping similarity},
  owner = {salman},
  timestamp = {03,500}
}

@INPROCEEDINGS{1994_CNF_FeaturesToTrack_ShiTomasi,
  author = {Shi, Jianbo and Tomasi, C.},
  title = {Good features to track},
  booktitle = {Computer Vision and Pattern Recognition, 1994. Proceedings CVPR '94.,
	1994 IEEE Computer Society Conference on},
  year = {1994},
  abstract = {No feature-based vision system can work unless good features can be
	identified and tracked from frame to frame. Although tracking itself
	is by and large a solved problem, selecting features that can be
	tracked well and correspond to physical points in the world is still
	hard. We propose a feature selection criterion that is optimal by
	construction because it is based on how the tracker works, and a
	feature monitoring method that can detect occlusions, disocclusions,
	and features that do not correspond to points in the world. These
	methods are based on a new tracking algorithm that extends previous
	Newton-Raphson style search methods to work under affine image transformations.
	We test performance with several simulations and experiments},
  comment = {trk_point},
  file = {:papers\\1994 CNF, Good Features to Track (Shi, Tomasi, CVPR, 2540).pdf:PDF},
  keywords = {computer vision feature extraction tracking Newton-Raphson style search
	methods affine image transformations disocclusions feature monitoring
	feature selection feature-based occlusions performance tracker vision
	system},
  owner = {salman},
  timestamp = {02,500}
}

@INPROCEEDINGS{2004_CNF_TrackingLooseLimbedPeople_Sigal,
  author = {Sigal, L. and Bhatia, S. and Roth, S. and Black, M.J. and Isard,
	M.},
  title = {Tracking loose-limbed people},
  booktitle = {Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings
	of the 2004 IEEE Computer Society Conference on},
  year = {2004},
  abstract = {We pose the problem of 3D human tracking as one of inference in a
	graphical model. Unlike traditional kinematic tree representations,
	our model of the body is a collection of loosely-connected limbs.
	Conditional probabilities relating the 3D pose of connected limbs
	are learned from motion-captured training data. Similarly, we learn
	probabilistic models for the temporal evolution of each limb (forward
	and backward in time). Human pose and motion estimation is then solved
	with non-parametric belief propagation using a variation of particle
	filtering that can be applied over a general loopy graph. The loose-limbed
	model and decentralized graph structure facilitate the use of low-level
	visual cues. We adopt simple limb and head detectors to provide "bottom-up"
	information that is incorporated into the inference process at every
	time-step; these detectors permit automatic initialization and aid
	recovery from transient tracking failures. We illustrate the method
	by automatically tracking a walking person in video imagery using
	four calibrated cameras. Our experimental apparatus includes a marker-based
	motion capture system aligned with the coordinate frame of the calibrated
	cameras with which we quantitatively evaluate the accuracy of our
	3D person tracker.},
  comment = {trk_people},
  doi = {10.1109/CVPR.2004.1315063},
  file = {:papers\\2004 CNF, Tracking Loose-Limbed People (Sigal, Bhatia, Roth, Black, Isard, CVPR, 237).pdf:PDF},
  issn = {1063-6919 },
  keywords = { 3D human tracking; calibrated cameras; conditional probabilities;
	decentralized graph structure; detectors; graphical model; inference
	process; kinematic tree representation; loopy graph; loose limbed
	people; marker based motion capture system; motion estimation; nonparametric
	belief propagation; particle filtering; video imaging; belief networks;
	filtering theory; graph theory; inference mechanisms; motion estimation;
	probability; tracking;},
  timestamp = {00,250}
}

@ARTICLE{1992_JNL_ShiftableMultiscaleTransforms_Simoncelli,
  author = {Simoncelli, E.P. and Freeman, W.T. and Adelson, E.H. and Heeger,
	D.J.},
  title = {Shiftable multiscale transforms},
  journal = {Information Theory, IEEE Transactions on},
  year = {1992},
  volume = {38},
  pages = {587 -607},
  number = {2},
  month = {mar},
  abstract = {One of the major drawbacks of orthogonal wavelet transforms is their
	lack of translation invariance: the content of wavelet subbands is
	unstable under translations of the input signal. Wavelet transforms
	are also unstable with respect to dilations of the input signal and,
	in two dimensions, rotations of the input signal. The authors formalize
	these problems by defining a type of translation invariance called
	shiftability. In the spatial domain, shiftability corresponds to
	a lack of aliasing; thus, the conditions under which the property
	holds are specified by the sampling theorem. Shiftability may also
	be applied in the context of other domains, particularly orientation
	and scale. Jointly shiftable transforms that are simultaneously shiftable
	in more than one domain are explored. Two examples of jointly shiftable
	transforms are designed and implemented: a 1-D transform that is
	jointly shiftable in position and scale, and a 2-D transform that
	is jointly shiftable in position and orientation. The usefulness
	of these image representations for scale-space analysis, stereo disparity
	measurement, and image enhancement is demonstrated},
  comment = {SP},
  doi = {10.1109/18.119725},
  file = {:papers\\1992 JNL, Shiftable multi-scale transforms (Simoncelli, Freeman, Adelson, Heeger, TIT, 940).pdf:PDF},
  issn = {0018-9448},
  keywords = {1-D transform;image enhancement;jointly shiftable;multiscale transforms;orientation
	domain;orthogonal wavelet transforms;sampling theorem;scale domain;scale-space
	analysis;shiftability;shiftable transforms;signal analysis;spatial
	domain;stereo disparity measurement;translation invariance;wavelet
	subbands;filtering and prediction theory;picture processing;signal
	processing;transforms;},
  timestamp = {01,000}
}

@ARTICLE{2008_JNL_TRKsubs_Skocaj,
  author = {Danijel Skocaj and Ales Leonardis},
  title = {Incremental and robust learning of subspace representations},
  journal = {Image and Vision Computing},
  year = {2008},
  volume = {26},
  pages = {27 - 38},
  number = {1},
  note = {Cognitive Vision-Special Issue},
  comment = {TRK_subspace},
  doi = {DOI: 10.1016/j.imavis.2005.07.028},
  file = {:papers\\2008 JNL, Incremental and robust learning of subspace representations (Skocaj).pdf:PDF},
  issn = {0262-8856},
  keywords = {Subspace learning},
  url = {http://www.sciencedirect.com/science/article/B6V09-4JTR904-1/2/8563159139086c93de084859503002a8}
}

@ARTICLE{2006_JNL_HumanMotionRecognition_Sminchisescu,
  author = {Sminchisescu, C. and Kanaujia, A. and Metaxas, D.},
  title = {Conditional models for contextual human motion recognition},
  journal = {Computer Vision and Image Understanding},
  year = {2006},
  volume = {104},
  pages = {210-20},
  abstract = {We describe algorithms for recognizing human motion in monocular video
	sequences, based on discriminative conditional random fields (CRFs)
	and maximum entropy Markov models (MEMMs). Existing approaches to
	this problem typically use generative structures like the hidden
	Markov model (HMM). Therefore, they have to make simplifying, often
	unrealistic assumptions on the conditional independence of observations
	given the motion class labels and cannot accommodate rich overlapping
	features of the observation or long-term contextual dependencies
	among observations at multiple timesteps. This makes them prone to
	myopic failures in recognizing many human motions, because even the
	transition between simple human activities naturally has temporal
	segments of ambiguity and overlap. The correct interpretation of
	these sequences requires more holistic, contextual decisions, where
	the estimate of an activity at a particular timestep could be constrained
	by longer windows of observations, prior and even posterior to that
	timestep. This would not be computationally feasible with a HMM which
	requires the enumeration of a number of observation sequences exponential
	in the size of the context window. In this work we follow a different
	philosophy: instead of restrictively modeling the complex image generation
	process - the observation, we work with models that can unrestrictedly
	take it as an input, hence condition on it. Conditional models like
	the proposed CRFs seamlessly represent contextual dependencies and
	have computationally attractive properties: they support efficient,
	exact recognition using dynamic programming, and their parameters
	can be learned using convex optimization. We introduce conditional
	graphical models as complementary tools for human motion recognition
	and present an extensive set of experiments that show not only how
	these can successfully classify diverse human activities like walking,
	jumping, running, picking or dancing, but also how they can discriminate
	among subtle motion styles like normal walks and wander walks. [All
	rights reserved Elsevier]},
  comment = {recog_action_shape},
  file = {:papers\\2006 JNL, Conditional models for contextual human motion recognition (Sminchisescu, Kanaujia, Metaxas, CVIU, 97).pdf:PDF},
  keywords = {dynamic programming hidden Markov models image motion analysis image
	recognition image sequences maximum entropy methods video signal
	processing},
  owner = {salman},
  timestamp = {00,100}
}

@INPROCEEDINGS{2003_CNF_KinematicHumanTracking_Sminchisescu,
  author = {Sminchisescu, C. and Triggs, B.},
  title = {Kinematic jump processes for monocular 3D human tracking},
  booktitle = {Computer Vision and Pattern Recognition, 2003. Proceedings. 2003
	IEEE Computer Society Conference on},
  year = {2003},
  volume = {1},
  abstract = {A major difficulty for 3D (three-dimensional) human body tracking
	from monocular image sequences is the near nonobservability of kinematic
	degrees of freedom that generate motion in depth. For known link
	(body segment) lengths, the strict nonobservabilities reduce to twofold
	'forwards/backwards flipping' ambiguities for each link. These imply
	2# links formal inverse kinematics solutions for the full model,
	and hence linked groups of O(2# links) local minima in the model-image
	matching cost function. Choosing the wrong minimum leads to rapid
	mistracking, so for reliable tracking, rapid methods of investigating
	alternative minima within a group are needed. Previous approaches
	to this have used generic search methods that do not exploit the
	specific problem structure. Here, we complement these by using simple
	kinematic reasoning to enumerate the tree of possible forwards/backwards
	flips, thus greatly speeding the search within each linked group
	of minima. Our methods can be used either deterministically, or within
	stochastic 'jump-diffusion' style search processes. We give experimental
	results on some challenging monocular human tracking sequences, showing
	how the new kinematic-flipping based sampling method improves and
	complements existing ones.},
  comment = {trk_people},
  file = {:papers\\2003 CNF, Kinematic Jump Processes For Monocular 3D Human Tracking (Sminchisescu, Triggs, CVPR, 174).pdf:PDF},
  issn = {1063-6919 },
  keywords = { 3D human body tracking; constrained optimization; covariance scaled
	sampling; formal inverse kinematics; high-dimensional searching;
	iterative inverse kinematics; jump-diffusion searching; kinematic
	ambiguity; kinematic jump process; kinematic reasoning; kinematic-flipping;
	model-image matching cost function; monocular image sequence; motion
	generation; particle filtering; three-dimensional; image sequences;
	kinematics; object detection; solid modelling; target tracking;},
  timestamp = {00,175}
}

@ARTICLE{1992_JNL_BayesianStatisticsWoTears_Smith,
  author = {Smith, A. F. M. and Gelfand, A. E.},
  title = {Bayesian Statistics without Tears: A Sampling-Resampling Perspective},
  journal = {The American Statistician},
  year = {1992},
  volume = {46},
  pages = {84--88},
  number = {2},
  abstract = {Even to the initiated, statistical calculations based on Bayes's Theorem
	can be daunting because of the numerical integrations required in
	all but the simplest applications. Moreover, from a teaching perspective,
	introductions to Bayesian statistics-if they are given at all-are
	circumscribed by these apparent calculational difficulties. Here
	we offer a straightforward sampling-resampling perspective on Bayesian
	inference, which has both pedagogic appeal and suggests easily implemented
	calculation strategies.},
  comment = {tutorial},
  copyright = {Copyright  1992 American Statistical Association},
  file = {:papers\\1992 JNL, Bayesian Statistics Without Tears_ a sampling-resampling perspective (Smith, Gelfand, ASA, 533).pdf:PDF},
  issn = {00031305},
  jstor_articletype = {primary_article},
  jstor_formatteddate = {May, 1992},
  publisher = {American Statistical Association},
  timestamp = {00,500},
  url = {http://www.jstor.org/stable/2684170}
}

@TECHREPORT{2002_TUT_PCA_Smith,
  author = {Lindsay Smith},
  title = {Tutorial on PCA},
  year = {2002},
  comment = {tutorial},
  file = {:papers\\2002 TUT, Tutorial on PCA (TUTORIAL, Lindsay Smith).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{1996_JNL_DefectColorTextures_Song,
  author = {Song, K. Y. and Kittler, J. and Petrou, M.},
  title = {Defect detection in random colour textures},
  journal = {Image and Vision Computing},
  year = {1996},
  volume = {14},
  pages = {667-683},
  number = {Compendex},
  abstract = {Machine vision and automatic surface inspection has been an active
	field of research during the last few years. However, very little
	research has been contributed to the area of defect detection in
	textured images, especially for the case of random textures. In this
	paper, we propose a novel algorithm that uses colour and texture
	information to solve the problem. A new colour clustering scheme
	based on human colour perception is developed. The algorithm is training
	based and produces very promising results on defect detection in
	random textured images and in particular, granite images.},
  comment = {det},
  file = {:papers\\1996 JNL, Defect detection in random color textures (Song, Kittler, Petrou, IVC, 50).pdf:PDF},
  keywords = {Image analysis Algorithms Automation Color vision Computer vision
	Problem solving Surface structure Textures},
  timestamp = {00,050}
}

@ARTICLE{7980521,
  author = {Sotelo, M.A. and Rodriguez, F.J. and Magdalena, L. and Bergasa, L.M.
	and Boquete, L.},
  title = {A color vision-based lane tracking system for autonomous driving
	on unmarked roads},
  journal = {Autonomous Robots},
  year = {2004},
  volume = { 16},
  pages = {95 - 116},
  number = { 1},
  note = {unmarked roads;autonomous driving;color vision-based lane tracking
	system;road surface detection system;vehicle stability;intelligent
	transportation systems;urban roads;BABIECA prototype vehicle;navigation
	missions;road texture;illumination conditions;weather conditions;unsupervised
	road segmentation algorithm;},
  abstract = {This work describes a color vision-based system intended to perform
	stable autonomous driving on unmarked roads. Accordingly, this implies
	the development of an accurate road surface detection system that
	ensures vehicle stability. Although this topic has already been documented
	in the technical literature by different research groups, the vast
	majority of the already existing intelligent transportation systems
	are devoted to assisted driving of vehicles on marked extra urban
	roads and highways. The complete system was tested on the BABIECA
	prototype vehicle, which was autonomously driven for hundred of kilometers
	accomplishing different navigation missions on a private circuit
	that emulates an urban quarter. During the tests, the navigation
	system demonstrated its robustness with regard to shadows, road texture,
	and weather and changing illumination conditions},
  address = {Netherlands},
  comment = {trk_color},
  copyright = {Copyright 2004, IEE},
  file = {:papers\\2004 JNL, A color vision-based lane tracking system for autonomous driving on unmarked roads (Sotelo et al.).pdf:PDF},
  issn = {0929-5593},
  keywords = {image segmentation;mobile robots;navigation;road vehicles;roads;robot
	vision;stability;tracking;},
  language = {English},
  timestamp = {00,030},
  url = {http://dx.doi.org/10.1023/B:AURO.0000008673.96984.28}
}

@ARTICLE{1990_JNL_PNN_Specht,
  author = {Specht, Donald F.},
  title = {Probabilistic neural networks},
  journal = {Neural Networks},
  year = {1990},
  volume = {3},
  pages = {109 - 118},
  number = {1},
  abstract = {By replacing the sigmoid activation function often used in neural
	networks with an exponential function, a probabilistic neural network
	(PNN) that can compute nonlinear decision boundaries which approach
	the Bayes optimal is formed. Alternate activation functions having
	similar properties are also discussed. A fourlayer neural network
	of the type proposed can map any input pattern to any number of classifications.
	The decision boundaries can be modified in real-time using new data
	as they become available, and can be implemented using artificial
	hardware #neurons# that operate entirely in parallel. Provision is
	also made for estimating the probability and reliability of a classification
	as well as making the decision. The technique offers a tremendous
	speed advantage for problems in which the incremental adaptation
	time of back propagation is a significant fraction of the total computation
	time. For one application, the PNN paradigm was 200,000 times faster
	than back-propagation.},
  comment = {PRML},
  doi = {DOI: 10.1016/0893-6080(90)90049-Q},
  file = {:papers\\1990 JNL, Probabilistic neural networks (Specht).pdf:PDF},
  issn = {0893-6080},
  keywords = {Neural network},
  timestamp = {01,800},
  url = {http://www.sciencedirect.com/science/article/B6T08-485RHV9-67/2/6184d2a09369bbb2e1313d627600fc70}
}

@ARTICLE{1992_JNL_BoundaryFinding_Staib,
  author = {Staib, L.H. and Duncan, J.S.},
  title = {Boundary finding with parametrically deformable models},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1992},
  volume = {14},
  pages = {1061-1075},
  number = {11},
  month = {Nov},
  abstract = {Segmentation using boundary finding is enhanced both by considering
	the boundary as a whole and by using model-based global shape information.
	The authors apply flexible constraints, in the form of a probabilistic
	deformable model, to the problem of segmenting natural 2-D objects
	whose diversity and irregularity of shape make them poorly represented
	in terms of fixed features or form. The parametric model is based
	on the elliptic Fourier decomposition of the boundary. Probability
	distributions on the parameters of the representation bias the model
	to a particular overall shape while allowing for deformations. Boundary
	finding is formulated as an optimization problem using a maximum
	a posteriori objective function. Results of the method applied to
	real and synthetic images are presented, including an evaluation
	of the dependence of the method on prior information and image quality},
  doi = {10.1109/34.166621},
  issn = {0162-8828},
  keywords = {Fourier analysis, image recognition, image segmentation, optimisation,
	probability2D object segmentation, boundary finding, elliptic Fourier
	decomposition, flexible constraints, image recognition, objective
	function, optimization, probabilistic deformable model, probability
	distribution}
}

@INPROCEEDINGS{1989_CNF_Contours_Staib,
  author = {Staib, L.H. and Duncan, J.S.},
  title = {Parametrically deformable contour models},
  booktitle = {Computer Vision and Pattern Recognition, 1989. Proceedings CVPR '89.,
	IEEE Computer Society Conference on},
  year = {1989},
  pages = {98 -103},
  month = {jun},
  abstract = {Segmentation using boundary finding is enhanced both by considering
	the boundary as a whole and by using model-based shape information.
	Flexible constraints, in the form of a probabilistic deformable model,
	are applied to the problem of segmenting natural objects whose diversity
	and irregularity of shape makes them poorly represented in terms
	of fixed features of forms. The parametric model is based on the
	elliptic Fourier decomposition of the boundary. The segmentation
	problem is solved as an optimization problem, where the best match
	between the boundary (as defined by the parameter vector) and the
	image data is found. Initial experimentation shows good results on
	a variety of images},
  comment = {trk_contour},
  doi = {10.1109/CVPR.1989.37834},
  file = {:papers\\1989 CNF, Parametrically deformable contour models (Staib, Duncan).pdf:PDF},
  keywords = {elliptic Fourier decomposition;feature extraction;model-based shape
	information;optimization;parameter vector;parametrically deformable
	contour model;pattern recognition;picture processing;segmentation;Fourier
	analysis;optimisation;pattern recognition;picture processing;},
  timestamp = {00,080}
}

@ARTICLE{2000_JNL_MG_Stauffer,
  author = {Stauffer, Chris and Eric, W. and Eric},
  title = {Learning Patterns of Activity Using Real-Time Tracking},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2000},
  volume = {22},
  pages = {747-757},
  abstract = {this paper, we focus on motion tracking and show how one can use observed
	motion to learn patterns of activity in a site},
  comment = {trk_people},
  file = {:papers\\2000 JNL, Learning patterns of activity using real-time tracking (Stauffer, Grimson, PAMI, 1472).pdf:PDF},
  owner = {salman},
  timestamp = {01,500}
}

@INPROCEEDINGS{1999_CNF_RealTimeTracking_Stauffer,
  author = {Stauffer, C. and Grimson, W.E.L.},
  title = {Adaptive background mixture models for real-time tracking},
  booktitle = {Computer Vision and Pattern Recognition, 1999. IEEE Computer Society
	Conference on.},
  year = {1999},
  volume = {2},
  pages = {252 Vol. 2},
  abstract = {A common method for real-time segmentation of moving regions in image
	sequences involves ldquo;background subtraction ldquo;, or thresholding
	the error between an estimate of the image without moving objects
	and the current image. The numerous approaches to this problem differ
	in the type of background model used and the procedure used to update
	the model. This paper discusses modeling each pixel as a mixture
	of Gaussians and using an on-line approximation to update the model.
	The Gaussian, distributions of the adaptive mixture model are then
	evaluated to determine which are most likely to result from a background
	process. Each pixel is classified based on whether the Gaussian distribution
	which represents it most effectively is considered part of the background
	model. This results in a stable, real-time outdoor tracker which
	reliably deals with lighting changes, repetitive motions from clutter,
	and long-term scene changes. This system has been run almost continuously
	for 16 months, 24 hours a day, through rain and snow},
  comment = {trk_people},
  doi = {10.1109/CVPR.1999.784637},
  file = {:papers\\1999 CNF, Adaptive background mixture models for real-time tracking (Stauffer, Grimson, CVPR, 1963).pdf:PDF},
  keywords = {adaptive background mixture models;background subtraction;image sequences;real-time
	segmentation;real-time tracking;thresholding;image segmentation;image
	sequences;real-time systems;tracking;},
  timestamp = {02,000}
}

@INPROCEEDINGS{2001_CNF_TopologyFreeHMMsBG_Stenger,
  author = {Stenger, B. and Ramesh, V. and Paragios, N. and Coetzee, F. and Buhmann,
	J. M.},
  title = {Topology free hidden Markov models: application to background modeling},
  booktitle = {Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International
	Conference on},
  year = {2001},
  volume = {1},
  pages = {294-301 vol.1},
  abstract = {Hidden Markov models (HMMs) are increasingly being used in computer
	vision for applications such as: gesture analysis, action recognition
	from video, and illumination modeling. Their use involves an off-line
	learning step that is used as a basis for on-line decision making
	(i.e. a stationarity assumption on the model parameters). But, real-world
	applications are often non-stationary in nature. This leads to the
	need for a dynamic mechanism to learn and update the model topology
	as well as its parameters. This paper presents a new framework for
	HMM topology and parameter estimation in an online, dynamic fashion.
	The topology and parameter estimation is posed as a model selection
	problem with an MDL prior. Online modifications to the topology are
	made possible by incorporating a state splitting criterion. To demonstrate
	the potential of the algorithm, the background modeling problem is
	considered. Theoretical validation and real experiments are presented},
  comment = {trk_BG},
  file = {:papers\\2001 CNF, Topology free hidden Markov models_ application to background modeling (Stenger, Ramesh, Paragios, Coetzee, Buhmann, ICCV, 125).pdf:PDF},
  keywords = {computer vision hidden Markov models parameter estimation action recognition
	background modeling gesture analysis illumination modeling model
	selection problem off-line learning step state splitting criterion
	topology free hidden Markov models},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{8617872,
  author = {Stern, H. and Efros, B.},
  title = {Adaptive color space switching for tracking under varying illumination},
  journal = {Image and Vision Computing},
  year = {2005},
  volume = { 23},
  pages = {353 - 64},
  number = {3},
  note = {color space models;color distribution models;face detection;video
	processing;adaptive color tracking systems;adaptive color space switching
	algorithm;color segmentation;face tracking;},
  abstract = {Many studies use color space models (CSM) and color distribution models
	(CDM) for detection of faces in an image. We develop a procedure
	that adaptively switches CSMs throughout the processing of a video.
	We show that this works in environments with varying types of illumination.
	In addition, a new performance measure for evaluating tracking algorithms
	is proffered. Extensive testing of the procedure found that switching
	between the color spaces resulted in increased tracking performance
	when compared to using single CSMs throughout. The methodology developed
	can be used to find the optimal CSM-CDM combination in the design
	of adaptive color tracking systems. The adaptive color space switching
	algorithm has linear computational time complexity O(<i>S</i>), at
	each iteration, where <i>S</i> is the picture size in pixels. [All
	rights reserved Elsevier]},
  address = {Netherlands},
  comment = {trk_color},
  copyright = {Copyright 2005, IEE},
  file = {:papers\\2005 JNL, Adaptive color space switching for tracking under varying illumination (Stern, Efros).pdf:PDF},
  issn = {0262-8856},
  keywords = {computational complexity;face recognition;feature extraction;image
	colour analysis;image segmentation;tracking;video signal processing;},
  language = {English},
  timestamp = {00,030},
  url = {http://dx.doi.org/10.1016/j.imavis.2004.09.005}
}

@INPROCEEDINGS{2002_CNF_ColorSpaceSwitchingFaceTracking_SternEfros,
  author = {Stern, Helman and Efros, Boris.},
  title = {Adaptive color space switching for face tracking in multi-colored
	lighting environments},
  booktitle = {Automatic Face and Gesture Recognition, 2002. Proceedings. Fifth
	IEEE International Conference on},
  year = {2002},
  pages = {249 -254},
  month = {may},
  abstract = {There are many studies that use color space models (CSM) for detection
	of faces in an image. Most researchers a priori select a given CSM,
	and proceed to use the selected model for color segmentation of the
	face by constructing a color distribution model (CDM). There is limited
	work on finding the overall best CSM. We develop a procedure to adaptively
	change the CSM throughout the processing of a video. We show that
	this works in environments where the face moves through multi-positioned
	light sources with varying types of illumination. A test of the procedure
	using the 2D color space models; RG, rg, HS, YQ and CbCr found that
	switching between the color spaces resulted in increased tracking
	performance. In addition, we have proposed a new performance measure
	for evaluating color-tracking algorithms, which include both accuracy
	and robustness of the tracking window. The methodology developed
	can be used to find the optimal CSM-CDM combination in adaptive color
	tracking systems.},
  comment = {trk_face},
  doi = {10.1109/AFGR.2002.1004162},
  file = {:papers\\2002 CNF, Adaptive Color Space Switching for Face Tracking in Multi-Colored Lighting (Stern, Efros, AFGR, 58).pdf:PDF},
  keywords = {2D color space models;adaptive color space switching;adaptive color
	tracking systems;color distribution model;color segmentation;face
	detection;face tracking;illumination;image processing;multicolored
	lighting environments;tracking performance;video processing;face
	recognition;image colour analysis;image segmentation;lighting;object
	detection;performance evaluation;},
  timestamp = {00,050}
}

@BOOK{2003_BOOK_LinearAlgebra_Strang,
  title = {Linear Algebra and Its Applications},
  publisher = {Harcourth Brace Jovanovich},
  year = {1988},
  author = {Gilbert Strang},
  file = {:papers\\1988 BOOK, Linear Algebra and Its Applications, 3rd ed with solution manual (Strang).pdf:PDF},
  isbn = {0155510053}
}

@INPROCEEDINGS{1994_CNF_MLPMHT_Streit,
  author = {Streit, R. L. and Luginbuhl, T. E.},
  title = {Maximum likelihood method for probabilistic multi-hypothesis tracking},
  booktitle = {Signal and Data Processing of Small Targets},
  year = {1994},
  abstract = {In a multi-target multi-measurement environment, knowledge of the
	measurement-to-track assignments is typically unavailable to the
	tracking algorithm. In this paper, a strictly probabilistic approach
	to the measurement-to-track assignment problem is taken. Measurements
	are not assigned to tracks as in traditional multi-hypothesis (MHT)
	algorithms; instead, the probability that each measurement belongs
	to each track is estimated using a maximum likelihood (ML) algorithm
	derived by the method of expectation-maximization (EM). These measurement-to-track
	probability estimates are intrinsic to the multi-target tracker called
	the probabilistic multi-hypothesis tracking (PMHT) algorithm. Unlike
	MHT algorithms, the PMHT algorithm does not maintain explicit hypothesis
	lists. The PMHT algorithm is computationally practical because it
	requires neither enumeration of measurement-to-track assignments
	nor pruning},
  comment = {trk_correspondence},
  file = {:papers\\1994 JNL, Maximum likelihood method for probabilistic multi-hypothesis tracking (Streit, Luginbuhl, SPIE, 92).pdf:PDF},
  keywords = {Bayes methods maximum likelihood estimation probability random processes
	target tracking},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{2009_JNL_Content_Su,
  author = {Su, Po-Chyi and Chen, Chun-Chieh and Chang, Hong-Min},
  title = {Towards Effective Content Authentication for Digital Videos by Employing
	Feature Extraction and Quantization},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  year = {2009},
  volume = {19},
  pages = {668 -677},
  number = {5},
  abstract = {A content authentication scheme for digital videos is proposed in
	this paper. In order to prevent the content from being unnoticeably
	tampered with by using digital editing facilities, we employ the
	approach of scalar/vector quantization on the reliable features extracted
	from video frame blocks to form the authentication code, which is
	transmitted along with the video. The resulting authentication code
	is sensitive to malicious modifications of video data but resilient
	to allowed lossy compression processes, such as H.264/advanced video
	coding (AVC). The integrity of video content can thus be guaranteed
	if the extracted feature matches the transmitted authentication code.
	Experimental results will show the feasibility of the proposed scheme.},
  comment = {analysis},
  doi = {10.1109/TCSVT.2009.2017404},
  file = {:papers\\2009 JNL, Towards Effective Content Authentication for Digital Videos by Employing Feature Extraction and Quantization (Su, Chen, Chang).pdf:PDF},
  issn = {1051-8215},
  keywords = {H.264/advanced video coding;authentication code;content authentication
	scheme;digital editing facilities;digital videos;feature extraction;lossy
	compression processes;scalar/vector quantisation;video data;video
	frame blocks;feature extraction;quantisation (signal);video coding;video
	signal processing;},
  timestamp = {-}
}

@ARTICLE{20094212384624,
  author = {Sugandi, Budi and Kim, Hyoungseop and Tan, Joo Kooi and Ishikawa,
	Seiji},
  title = {A moving object tracking based on color information employing a particle
	filter algorithm},
  journal = {Artificial Life and Robotics},
  year = {2009},
  volume = {14},
  pages = {39 - 42},
  number = {1},
  note = {Bhattacharya distance;Color histogram;Color information;Estimation
	problem;Moving object tracking;Moving objects;Nonlinear and non-Gaussian;Object
	positions;Object tracking;Particle filter;Particle filter algorithms;Posterior
	probability;State space model;Tracked objects;},
  abstract = {In this article, we present a new algorithm to track a moving object
	based on color information employing a particle filter algorithm.
	Recently, a particle filter has been proven very successful for nonlinear
	and non-Gaussian estimation problems. It approximates a posterior
	probability density of the state, such as the object position, by
	using samples which are called particles. The probability distribution
	of the state of the tracked object is approximated by a set of particles,
	where each state is denoted as the hypothetical state of the tracked
	object and its weight. The particles are propagated according to
	a state space model. Here, the state is treated as the position of
	the object. The weight is considered as the likelihood of each particle.
	For this likelihood, we consider the similarity between the color
	histogram of the tracked object and the region around the position
	of each particle. The Bhattacharya distance is used to measure this
	similarity. Finally, the mean state of the particles is treated as
	the estimated position of the object. Experiments were performed
	to confirm the effectiveness of this method to track a moving object.
	&copy; International Symposium on Artificial Life and Robotics (ISAROB).
	2009.},
  address = {1-11-11 Kudan-kita, Chiyoda-ku, No. 2 Funato Bldg., Tokyo, 102-0073,
	Japan},
  comment = {trk_color},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2009 JNL, A moving object tracking based on color information employing a particle filter algorithm (Sugandi, Kim, Tan, Ishikawa).pdf:PDF},
  issn = {14335298},
  key = {Color},
  keywords = {Air filters;Nonlinear filtering;Probability density function;Probability
	distributions;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1007/s10015-009-0718-6}
}

@INPROCEEDINGS{2002_CNF_RecognizingTrackingHumanAction_Sullivan,
  author = {Sullivan, J. and Carlsson, S.},
  title = {Recognizing and tracking human action},
  booktitle = {Computer Vision - ECCV 2002. 7th European Conference on Computer
	Vision. Proceedings, 28-31 May 2002},
  year = {2002},
  abstract = {Human activity can be described as a sequence of 3D body postures.
	The traditional approach to recognition and 3D reconstruction of
	human activity has been to track motion in 3D, mainly using advanced
	geometric and dynamic models. In this paper we reverse this process.
	View based activity recognition serves as an input to a human body
	location tracker with the ultimate goal of 3D reanimation in mind.
	We demonstrate that specific human actions can be detected from single
	frame postures in a video sequence. By recognizing the image of a
	person's posture as corresponding to a particular key frame from
	a set of stored key frames, it is possible to map body locations
	from the key frames to actual frames. This is achieved using a shape
	matching algorithm based on qualitative similarity that computes
	point to point correspondence between shapes, together with information
	about appearance. As the mapping is from fixed key frames, our tracking
	does not suffer from the problem of having to reinitialise when it
	gets lost. It is effectively a closed loop. We present experimental
	results both for recognition and tracking for a sequence of a tennis
	player},
  comment = {recog_action_shape},
  file = {:papers\\2002 CNF, Recognizing and tracking human action (Sullivan, Carlsson, ECCV, 95).pdf:PDF},
  keywords = {computer animation computer vision image motion analysis image reconstruction
	image sequences optical tracking stereo image processing video signal
	processing},
  owner = {salman},
  timestamp = {00,100}
}

@ARTICLE{2003_JNL_SVMPCA_Suykens,
  author = {Suykens, J.A.K. and Van Gestel, T. and Vandewalle, J. and De Moor,
	B.},
  title = {A support vector machine formulation to PCA analysis and its kernel
	version},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2003},
  volume = {14},
  pages = { 447 - 450},
  number = {2},
  month = {mar.},
  abstract = {In this paper, we present a simple and straightforward primal-dual
	support vector machine formulation to the problem of principal component
	analysis (PCA) in dual variables. By considering a mapping to a high-dimensional
	feature space and application of the kernel trick (Mercer theorem),
	kernel PCA is obtained as introduced by Scholkopf et al. (2002).
	While least squares support vector machine classifiers have a natural
	link with the kernel Fisher discriminant analysis (minimizing the
	within class scatter around targets +1 and -1), for PCA analysis
	one can take the interpretation of a one-class modeling problem with
	zero target value around which one maximizes the variance. The score
	variables are interpreted as error variables within the problem formulation.
	In this way primal-dual constrained optimization problem interpretations
	to the linear and kernel PCA analysis are obtained in a similar style
	as for least square-support vector machine classifiers.},
  comment = {def_classification},
  doi = {10.1109/TNN.2003.809414},
  file = {:papers\\2003 JNL, A support vector machine formulation to PCA analysis and its kernel version.pdf:PDF},
  issn = {1045-9227},
  keywords = { error variables; high-dimensional feature space; kernel methods;
	least squares; pattern classification; primal-dual constrained optimization;
	principal component analysis; score variables; support vector machine;
	least squares approximations; neural nets; optimisation; pattern
	classification; principal component analysis;},
  timestamp = {00,060}
}

@ARTICLE{1979_JNL_PRML_Swets,
  author = {Swets, John A. and Pickett, Ronald M. and Whitehead, Susan F. and
	Getty, David J. and Schnur, James A. and Swets, Joel B. and Freeman,
	Barbara A.},
  title = {Assessment of Diagnostic Technologies},
  journal = {Science},
  year = {1979},
  volume = {205},
  pages = {pp. 753-759},
  number = {4408},
  abstract = {A general protocol for rigorous evaluation of diagnostic systems in
	medicine was applied successfully in a comparative study of two radiologic
	techniques. Accuracies of computed tomography and radionuclide scanning
	in detecting, localizing, and diagnosing brain lesions were assessed
	with a sample of patients in whom tumor had been suspected. The principal
	means of analysis was the "relative operating characteristic," which
	is unique in providing a measure of accuracy that is largely independent
	of decision biases. Computed tomography was found to be substantially
	more accurate than radionuclide scanning.},
  comment = {PRML},
  copyright = {Copyright  1979 American Association for the Advancement of Science},
  file = {:papers\\1979 JNL, Assessment of Diagnostic Technologies (Swets).pdf:PDF},
  issn = {00368075},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Aug. 24, 1979},
  language = {English},
  publisher = {American Association for the Advancement of Science},
  series = {New Series},
  timestamp = {00,175},
  url = {http://www.jstor.org/stable/1748586}
}

@ARTICLE{1997_JNL_SplineImageRegistration_Szeliski,
  author = {Szeliski, Richard and Coughlan, James},
  title = {Spline-based image registration},
  journal = {International Journal of Computer Vision},
  year = {1997},
  volume = {22},
  pages = {199--218},
  number = {3},
  abstract = {The problem of image registration subsumes a number of problems and
	techniques in multiframe image analysis, including the computation
	of optic flow (general pixel-based motion), stereo correspondence,
	structure from motion, and feature tracking. We present a new registration
	algorithm based on spline representations of the displacement field
	which can be specialized to solve all of the above mentioned problems.
	In particular, we show how to compute local flow, global (parametric)
	flow, rigid flow resulting from camera egomotion, and multiframe
	versions of the above problems. Using a spline-based description
	of the flow removes the need for overlapping correlation windows,
	and produces an explicit measure of the correlation between adjacent
	flow estimates. We demonstrate our algorithm on multiframe image
	registration and the recovery of 3D projective scene geometry. We
	also provide results on a number of standard motion sequences.},
  address = {Dordrecht, Netherlands},
  comment = {registration},
  file = {:papers\\1997 JNL, Spline-Based Image Registration (Szeliski, Coughlan, IJCV, 325).pdf:PDF},
  issn = {09205691},
  keywords = {Computer vision, Algorithms, Computational geometry, Computational
	methods, Image analysis, Optical flows, Parameter estimation, Three
	dimensional computer graphics},
  owner = {salman},
  publisher = {Kluwer Academic Publishers},
  timestamp = {00,300}
}

@ARTICLE{2008_JNL_MRF_Szeliski,
  author = {Szeliski, R. and Zabih, R. and Scharstein, D. and Veksler, O. and
	Kolmogorov, V. and Agarwala, A. and Tappen, M. and Rother, C.},
  title = {A Comparative Study of Energy Minimization Methods for Markov Random
	Fields with Smoothness-Based Priors},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2008},
  volume = {30},
  pages = {1068 -1080},
  number = {6},
  month = {june },
  abstract = {Among the most exciting advances in early vision has been the development
	of efficient energy minimization algorithms for pixel-labeling tasks
	such as depth or texture computation. It has been known for decades
	that such problems can be elegantly expressed as Markov random fields,
	yet the resulting energy minimization problems have been widely viewed
	as intractable. Algorithms such as graph cuts and loopy belief propagation
	(LBP) have proven to be very powerful: For example, such methods
	form the basis for almost all the top-performing stereo methods.
	However, the trade-offs among different energy minimization algorithms
	are still not well understood. In this paper, we describe a set of
	energy minimization benchmarks and use them to compare the solution
	quality and runtime of several common energy minimization algorithms.
	We investigate three promising methods-graph cuts, LBP, and tree-reweighted
	message passing-in addition to the well-known older iterated conditional
	mode (ICM) algorithm. Our benchmark problems are drawn from published
	energy functions used for stereo, image stitching, interactive segmentation,
	and denoising. We also provide a general-purpose software interface
	that allows vision researchers to easily switch between optimization
	methods. The benchmarks, code, images, and results are available
	at http://vision.middlebury.edu/MRF/.},
  comment = {CV},
  doi = {10.1109/TPAMI.2007.70844},
  file = {:papers\\2008 JNL, A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors (Szeliski, et al.).pdf:PDF},
  issn = {0162-8828},
  keywords = {Markov random fields;depth computation;early vision;energy minimization
	methods;graph cuts;image denoising;image stitching;interactive segmentation;iterated
	conditional mode algorithm;loopy belief propagation;optimization
	methods;pixel-labeling tasks;smoothness-based priors;software interface;stereo
	methods;texture computation;tree-reweighted message passing;Markov
	processes;belief networks;energy consumption;image denoising;image
	segmentation;image texture;iterative methods;message passing;random
	processes;stereo image processing;trees (mathematics);Algorithms;Artificial
	Intelligence;Image Enhancement;Image Interpretation, Computer-Assisted;Markov
	Chains;Models, Statistical;Pattern Recognition, Automated;Reproducibility
	of Results;Sensitivity and Specificity;},
  timestamp = {-}
}

@INPROCEEDINGS{2006_CNF_Survey2D3D_Tam,
  author = {Tam, W.J. and Liang Zhang},
  title = {3D-TV Content Generation: 2D-to-3D Conversion},
  booktitle = {Multimedia and Expo, 2006 IEEE International Conference on},
  year = {2006},
  pages = {1869 -1872},
  month = {9-12},
  comment = {survey, 3D_stereo},
  doi = {10.1109/ICME.2006.262919},
  file = {:papers\\2006 CNF, 3D-TV Content Generation_ 2D-to-3D Conversion (Tam, Zhang).pdf:PDF},
  keywords = {2D-to-3D conversion;3D display;HDTV;backward-compatible transmission;depth
	extraction;distribution system;high-definition television;human visual
	system;image based rendering;stereoscopic three-dimensional television;feature
	extraction;high definition television;rendering (computer graphics);stereo
	image processing;three-dimensional displays;three-dimensional television;},
  timestamp = {-}
}

@PATENT{2008_PAT_DepthMapFromColor_Tam,
  nationality = {US},
  number = {0247670},
  year = {2008},
  yearfiled = {2008},
  author = {Wa James Tam and Carlos Vazquez},
  title = {Generation of a Depth Map from a Monoscopic Color Image for Rendering
	Stereoscopic Still and Video Images},
  url = {http://www.google.com/patents/about?id=HUquAAAAEBAJ&dq=Generation+of+a+Depth+Map+from+a+Monoscopic+Color+Image+for+Rendering+Stereoscopic+Still+and+Video+Images},
  file = {:papers\\2008 PAT, Generation of a Depth Map from a Monoscopic Color Image for Rendering Stereoscopic Still and Video Images (Tam, Vazquez).pdf:PDF},
  owner = {saslam},
  timestamp = {2010.05.20}
}

@INPROCEEDINGS{2004_CNF_Survey3DshapeRetrieval,
  author = {Johan W.H. Tangelder and Remco C. Veltkamp},
  title = {A Survey of Content Based 3D Shape Retrieval Methods},
  booktitle = {SMI '04: Proceedings of the Shape Modeling International},
  year = {2004},
  comment = {survey},
  doi = {http://dx.doi.org/10.1109/SMI.2004.10},
  file = {:papers\\2004 CNF, A survey of content based 3D shape retrieval methods (SURVEY, Tangelder, Veltkamp, SMA, 275).pdf:PDF},
  isbn = {0-7695-2075-8},
  timestamp = {00,275}
}

@ARTICLE{2002_JNL_TrackingBayesianEstimationDynamicLayerTaoSawhneyKumar,
  author = {Tao, Hai and Sawhney, Harpreet S. and Kumar, Rakesh},
  title = {Object tracking with bayesian estimation of dynamic layer representations},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2002},
  volume = {24},
  pages = {75-89},
  number = {Compendex},
  abstract = {Decomposing video frames into coherent two-dimensional motion layers
	is a powerful method for representing videos. Such a representation
	provides an intermediate description that enables applications such
	as object tracking, video summarization and visualization, video
	insertion, and sprite-based video compression. Previous work on motion
	layer analysis has largely concentrated on two-frame or multiframe
	batch formulations. The temporal coherency of motion layers and the
	domain constraints on shapes have not been exploited. This paper
	introduces a complete dynamic motion layer representation in which
	spatial and temporal constraints on shape, motion, and layer appearance
	are modeled and estimated in a maximum a posteriori (MAP) framework
	using the generalized expectation-maximization (EM) algorithm. In
	order to limit the computational complexity of tracking arbitrarily
	shaped layer ownership, we propose a shape prior that parameterizes
	the representation of shape and prevents motion layers from evolving
	into arbitrary shapes. In this work, a Gaussian shape prior is chosen
	to specifically develop a near real-time tracker for vehicle tracking
	in aerial videos. However, the general idea of using a parametric
	shape representation as part of the state of a tracker is a powerful
	one that can be extended to other domains as well. Based on the dynamic
	layer representation, an iterative algorithm is developed for continuous
	object tracking over time. The proposed method has been successfully
	applied in an airborne vehicle tracking system. Its performance is
	compared with that of a correlation-based tracker and a motion change-based
	tracker to demonstrate the advantages of the new method. Examples
	of tracking when the backgrounds are cluttered and the vehicles undergo
	various rigid motions and complex interactions such as passing, turning,
	and stop-and-go demonstrate the strength of the complete dynamic
	layer representation.},
  comment = {trk_people_cars},
  file = {:papers\\2002 JNL, Object tracking with bayesian estimation of dynamic layer representations (Tao, Sawhney, Kumar, PAMI, 155).pdf:PDF},
  keywords = {Motion estimation Aerial photography Aerospace applications Algorithms
	Computational complexity Image analysis Image segmentation Iterative
	methods Object recognition Tracking (position)},
  owner = {salman},
  timestamp = {00,160}
}

@ARTICLE{2004_JNL_BoostingImageRetrieval_Tieu,
  author = {Tieu, Kinh and Viola, Paul},
  title = {Boosting Image Retrieval},
  journal = {International Journal of Computer Vision},
  year = {2004},
  volume = {56},
  pages = {17 - 36},
  number = {1-2},
  abstract = {We present an approach for image retrieval using a very large number
	of highly selective features and efficient learning of queries. Our
	approach is predicated on the assumption that each image is generated
	by a sparse set of visual "causes" and that images which are visually
	similar share causes. We propose a mechanism for computing a very
	large number of highly selective features which capture some aspects
	of this causal structure (in our implementation there are over 46,000
	highly selective features). At query time a user selects a few example
	images, and the AdaBoost algorithm is used to learn a classification
	function which depends on a small number of the most appropriate
	features. This yields a highly efficient classification function.
	In addition we show that the AdaBoost framework provides a natural
	mechanism for the incorporation of relevance feedback. Finally we
	show results on a wide variety of image queries.},
  comment = {PRML},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2004 JNL, Boosting image retrival (Tieu, Viola, IJCV, 418).pdf:PDF},
  issn = {09205691},
  key = {Image retrieval},
  keywords = {Ada (programming language);Algorithms;Classification (of information);Database
	systems;Feedback;Functions;Human engineering;Learning systems;Query
	languages;Vectors;Vision;},
  language = {English},
  timestamp = {00,400},
  url = {http://dx.doi.org/10.1023/B:VISI.0000004830.93820.78}
}

@PATENT{2003_PAT_RVM_Tipping,
  nationality = {US},
  number = {6,633,857},
  year = {2003},
  yearfiled = {1999},
  author = {Tipping, M.},
  title = {Relevance},
  owner = {salman},
  timestamp = {2010.11.03}
}

@ARTICLE{2001_JNL_RVM_Tipping,
  author = {Tipping, M.E.},
  title = {Sparse Bayesian learning and the relevance vector machine},
  journal = {Journal of Machine Learning Research},
  year = {2001},
  volume = { 1},
  pages = {211 - 44},
  number = {3},
  abstract = {Introduces a general Bayesian framework for obtaining sparse solutions
	to regression and classification tasks utilising models that are
	linear in the parameters. Although this framework is fully general,
	we illustrate our approach with a particular specialisation that
	we denote the 'relevance vector machine' (RVM), a model of identical
	functional form to the popular and state-of-the-art support vector
	machine (SVM). We demonstrate that, by exploiting a probabilistic
	Bayesian learning framework, we can derive accurate prediction models
	which typically utilise dramatically fewer basis functions than a
	comparable SVM while offering a number of additional advantages.
	These include the benefits of probabilistic predictions, automatic
	estimation of 'nuisance' parameters and the facility to utilise arbitrary
	basis functions (e.g. non-Mercer kernels). We detail the Bayesian
	framework and associated learning algorithm for the RVM, and give
	some illustrative examples of its application along with some comparative
	benchmarks. We offer some explanation for the exceptional degree
	of sparsity obtained, and discuss and demonstrate some of the advantageous
	features and potential extensions of Bayesian relevance learning},
  address = {USA},
  comment = {PRML},
  copyright = {Copyright 2002, IEE},
  file = {:papers\\2001 JNL, Sparse bayesian learning and the relevance vector machine (Tipping, JMLR, 1295).pdf:PDF},
  issn = {1532-4435},
  keywords = {Bayes methods;learning (artificial intelligence);learning automata;pattern
	classification;statistical analysis;},
  language = {English},
  timestamp = {01,300},
  url = {http://dx.doi.org/10.1162/15324430152748236}
}

@ARTICLE{2008_JNL_MixturePCA_Tipping,
  author = {Tipping, M.E. and Bishop, C.M.},
  title = {Mixtures of probabilistic principal component analyzers},
  journal = {Neural Computation},
  year = {1999/02/15},
  volume = { 11},
  pages = {443 - 82},
  number = { 2},
  note = {probabilistic principal component analyzers;principal component analysis;data
	processing;data compression;data visualization;data complexity;local
	linear PCA projections;probability density;maximum likelihood framework;gaussian
	latent variable model;mixture model;expectation-maximization algorithm;clustering;density
	modeling;local dimensionality reduction;image compression;handwritten
	digit recognition;},
  abstract = {Principal component analysis (PCA) is one of the most popular techniques
	for processing, compressing, and visualizing data, although its effectiveness
	is limited by its global linearity. While nonlinear variants of PCA
	have been proposed, an alternative paradigm is to capture data complexity
	by a combination of local linear PCA projections. However, conventional
	PCA does not correspond to a probability density, and so there is
	no unique way to combine PCA models. Therefore, previous attempts
	to formulate mixture models for PCA have been ad hoc to some extent.
	In this article, PCA is formulated within a maximum likelihood framework,
	based on a specific form of gaussian latent variable model. This
	leads to a well-defined mixture model for probabilistic principal
	component analyzers, whose parameters can be determined using an
	expectation-maximization algorithm. The article discusses the advantages
	of this model in the context of clustering, density modeling, and
	local dimensionality reduction, and demonstrates its application
	to image compression and handwritten digit recognition},
  address = {USA},
  comment = {PRML_PCA},
  copyright = {Copyright 1999, IEE},
  file = {:papers\\1999 JNL, Mixtures of probabilistic principal component analyzers (Tipping, Bishop).pdf:PDF},
  issn = {0899-7667},
  keywords = {data compression;handwritten character recognition;image coding;maximum
	likelihood estimation;principal component analysis;probability;},
  language = {English},
  timestamp = {00,900},
  url = {http://dx.doi.org/10.1162/089976699300016728}
}

@ARTICLE{1999_JNL_PPCA_Tipping,
  author = {Tipping, M.E. and Bishop, C.M.},
  title = {Probabilistic principal component analysis},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year = {1999},
  volume = {61},
  pages = {611--622},
  number = {3},
  comment = {PRML_PCA},
  file = {:papers\\1999 JNL, Probabilistic principal component analysis (Tipping, Bishop).pdf:PDF},
  issn = {1467-9868},
  publisher = {John Wiley \& Sons},
  timestamp = {00,800}
}

@TECHREPORT{1991_TECH_PointFeatures_Tomasi,
  author = {Tomasi, Carlo and Kanade, Takeo},
  title = {Detection and Tracking of Point Features},
  institution = {Technical Report CMU-CS-91-132, Carnegie Mellon University},
  year = {1991},
  comment = {trk_point},
  file = {:papers\\1991 TECH, Detection and Tracking of Point Features (Tomasi, Kanade).pdf:PDF},
  owner = {salman},
  timestamp = {01,000}
}

@ARTICLE{2003_JNL_ContextualPrimingDetection_Torralba,
  author = {Torralba, Antonio},
  title = {Contextual priming for object detection},
  journal = {International Journal of Computer Vision},
  year = {2003},
  volume = {53},
  pages = {169 - 191},
  number = {2},
  abstract = {There is general consensus that context can be a rich source of information
	about an object's identity, location and scale. In fact, the structure
	of many real-world scenes is governed by strong configurational rules
	akin to those that apply to a single object. Here we introduce a
	simple framework for modeling the relationship between context and
	object properties based on the correlation between the statistics
	of low-level features across the entire scene and the objects that
	it contains. The resulting scheme serves as an effective procedure
	for object priming, context driven focus of attention and automatic
	scale-selection on real-world scenes.},
  comment = {det},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2003 JNL, Contextual Priming for Object Detection (Torralba, IJCV, 224).pdf:PDF},
  issn = {09205691},
  key = {Object recognition},
  keywords = {Database systems;Image analysis;Information analysis;Statistical methods;},
  language = {English},
  timestamp = {00,230},
  url = {http://dx.doi.org/10.1023/A:1023052124951}
}

@INPROCEEDINGS{2002_CNF_SpaceTimeTracking_Torresani,
  author = {Torresani, L. and Bregler, C.},
  title = {Space-time tracking},
  booktitle = {Computer Vision - ECCV 2002. 7th European Conference on Computer
	Vision. Proceedings, 28-31 May 2002},
  year = {2002},
  abstract = {We propose a new tracking technique that is able to capture non-rigid
	motion by exploiting a space-time rank constraint. Most tracking
	methods use a prior model in order to deal with challenging local
	features. The model usually has to be trained on carefully hand-labeled
	example data before the tracking algorithm can be used. Our new model-free
	tracking technique can overcome such limitations. This can be achieved
	in redefining the problem. Instead of first training a model and
	then tracking the model parameters, we are able to derive trajectory
	constraints first, and then estimate the model. This reduces the
	search space significantly and allows for a better feature disambiguation
	that would not be possible with traditional trackers. We demonstrate
	that sampling in the trajectory space, instead of in the space of
	shape configurations, allows us to track challenging footage without
	use of prior models},
  comment = {trk},
  file = {:papers\\2002 CNF, Space-time tracking (Torresani, Bregler, ECCV, 58).pdf:PDF},
  keywords = {image motion analysis image sequences optical tracking video signal
	processing},
  timestamp = {00,060}
}

@INPROCEEDINGS{1999_CNF_Wallflower_Toyama,
  author = {Toyama, Kentaro and Krumm, John and Brumitt, Barry and Meyers, Brian},
  title = {WALLFLOWER: Principles and practice of background maintenance},
  booktitle = {Proceedings of the 7th IEEE International Conference on Computer
	Vision (ICCV'99)},
  year = {1999},
  abstract = {Background maintenance is a frequent element of video surveillance
	systems. We develop Wallflower, a three-component system for background
	maintenance: the pixel-level component performs Wiener filtering
	to make probabilistic predictions of the expected background; the
	region-level component fills in homogeneous regions of foreground
	objects; and the frame-level component detects sudden, global changes
	in the image and swaps in better approximations of the background.
	We compare our system with 8 other background subtraction algorithms.
	Wallflower is shown to outperform previous algorithms by handling
	a greater set of the difficult situations that can occur. Finally,
	we analyze the experimental results and propose normative principles
	for background maintenance.},
  comment = {trk_BG},
  file = {:papers\\1999 CNF, Wallflower_ principles and practice of background maintenance (ICCV, Toyama, 793).pdf:PDF},
  keywords = {Computer vision Algorithms Approximation theory Signal filtering and
	prediction},
  owner = {salman},
  timestamp = {00,800}
}

@ARTICLE{2006_JNL_SURVEYtrk_Trucco,
  author = {Trucco, E. and Plakas, K.},
  title = {Video Tracking: A Concise Survey},
  journal = {Oceanic Engineering, IEEE Journal of},
  year = {2006},
  volume = {31},
  pages = {520 -529},
  number = {2},
  month = april,
  abstract = {This paper addresses video tracking, the problem of following moving
	targets automatically over a video sequence, and brings three main
	contributions. First, we give a concise introduction to video tracking
	in computer vision, including design requirements and a review of
	recent techniques, with some details of selected algorithms. Second,
	we give an overview of 28 recent papers on subsea video tracking
	and related motion analysis problems, arguably capturing the state-of-the-art
	of subsea video tracking. We summarize key features in a comparative,
	at-a-glance table, and discuss this work in comparison to the state-of-the-art
	in computer vision. Third, we identify well-proven computer vision
	techniques not yet embraced by the subsea research community, suggesting
	useful research directions for the subsea video processing community},
  comment = {SURVEYtrk},
  doi = {10.1109/JOE.2004.839933},
  file = {:C\:\\salman\\work\\writing\\papers\\2006 JNL, Video Tracking_ A Concise Survey (Trucco).pdf:PDF},
  issn = {0364-9059},
  keywords = {computer vision;image processing;motion analysis;subsea video tracking;underwater
	vision;computer vision;image motion analysis;oceanographic techniques;target
	tracking;},
  timestamp = {?}
}

@ARTICLE{1987_JNL_CameraCalibration_Tsai,
  author = {Tsai, R.},
  title = {A versatile camera calibration technique for high-accuracy 3D machine
	vision metrology using off-the-shelf TV cameras and lenses},
  journal = {Robotics and Automation, IEEE Journal of},
  year = {1987},
  volume = {3},
  pages = {323 -344},
  number = {4},
  month = {august },
  abstract = {A new technique for three-dimensional (3D) camera calibration for
	machine vision metrology using off-the-shelf TV cameras and lenses
	is described. The two-stage technique is aimed at efficient computation
	of camera external position and orientation relative to object reference
	coordinate system as well as the effective focal length, radial lens
	distortion, and image scanning parameters. The two-stage technique
	has advantage in terms of accuracy, speed, and versatility over existing
	state of the art. A critical review of the state of the art is given
	in the beginning. A theoretical framework is established, supported
	by comprehensive proof in five appendixes, and may pave the way for
	future research on 3D robotics vision. Test results using real data
	are described. Both accuracy and speed are reported. The experimental
	results are analyzed and compared with theoretical prediction. Recent
	effort indicates that with slight modification, the two-stage calibration
	can be done in real time.},
  comment = {3D_camera},
  doi = {10.1109/JRA.1987.1087109},
  file = {:papers\\1987 JNL, A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses (Tsai).pdf:PDF},
  issn = {0882-4967},
  keywords = {Calibration;Machine vision;Measurement;},
  timestamp = {03,600}
}

@ARTICLE{2004_JNL_RateDistortionVsDatabases_Truncel,
  author = {Tuncel, E. and Koulgi, P. and Rose, K.},
  title = {Rate-distortion approach to databases: storage and content-based
	retrieval},
  journal = {Information Theory, IEEE Transactions on},
  year = {2004},
  volume = {50},
  pages = {953-967},
  number = {6},
  abstract = {This paper investigates the relationship between rate-distortion theory
	and efficient content-based data retrieval from high-dimensional
	databases. We consider database design as the encoding of a data
	object sequence, and retrieval from the database as the decoding
	of the sequence using side information (i.e., the query) available
	only at the decoder. We show that, in this setting, the optimal asymptotic
	tradeoff between the search time R<sub>s</sub> (bits per data object
	read from the storage device) and the expected search accuracy D<sub>s</sub>
	(relevance of the retrieved data set) is given by the Wyner-Ziv solution
	with a side-information-dependent distortion measure. Moreover, the
	data indexing and retrieval problem is, in general, inseparable from
	the data compression problem. Data items selected by the search procedure,
	which can be stored in the disk with a limited total rate of R<sub>r</sub>
	&ge; R<sub>s</sub>, need to be presented at a prescribed expected
	reconstruction quality D<sub>r</sub>. This is, hence, a problem of
	scalable source coding or successive refinement, albeit with differing
	layer distortion measures to quantify search and reconstruction quality,
	respectively. We derive a single-letter characterization of all achievable
	quadruples {R<sub>s</sub>,R<sub>r</sub>,D<sub>s</sub>,D<sub>r</sub>},
	and prove conditions for "successive refinability" without rate loss.
	Finally, we show that the special case D<sub>s</sub>=D<sub>r</sub>=0
	is nontrivial and of practical interest in this context, as it can
	impose "acceptable" search and reconstruction qualities for each
	individual data item and for the entire query space with high probability,
	in contradistinction with standard average distortion requirements.
	The region of achievable {R<sub>s</sub>,R<sub>r</sub>} is obtained
	by adapting Rimoldi's characterization to a new regular scalable
	coding problem.},
  comment = {VQ_RVQ},
  file = {:papers\\2004 JNL, Rate-distortion approach to databases_ storage and content-based retrieval (Truncel, TIT, 11).pdf:PDF},
  keywords = {content-based retrieval data compression database indexing distortion
	measurement rate distortion theory source coding Rimoldis characterization
	Wyner-Ziv solution approximate similarity searching content-based
	data retrieval data indexing data object sequence encoding decoder
	high-dimensional databases query space probability rate-distortion
	theory scalable source coding side-information-dependent distortion
	measure single-letter characterization successive refinement},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{1991_CNF_Eigenfaces_Turk,
  author = {Turk, M.A. and Pentland, A.P.},
  title = {Face recognition using eigenfaces},
  booktitle = {Computer Vision and Pattern Recognition, 1991. Proceedings CVPR '91.,
	IEEE Computer Society Conference on},
  year = {1991},
  pages = {586 -591},
  month = jun,
  abstract = {An approach to the detection and identification of human faces is
	presented, and a working, near-real-time face recognition system
	which tracks a subject's head and then recognizes the person by comparing
	characteristics of the face to those of known individuals is described.
	This approach treats face recognition as a two-dimensional recognition
	problem, taking advantage of the fact that faces are normally upright
	and thus may be described by a small set of 2-D characteristic views.
	Face images are projected onto a feature space (`face space') that
	best encodes the variation among known face images. The face space
	is defined by the `eigenfaces', which are the eigenvectors of the
	set of faces; they do not necessarily correspond to isolated features
	such as eyes, ears, and noses. The framework provides the ability
	to learn to recognize new faces in an unsupervised manner},
  comment = {TRK_subspace},
  file = {:papers\\1991 CNF, Face recognition using eigenfaces (Turk).pdf:PDF},
  keywords = {eigenfaces;eigenvectors;face images;face recognition system;face space;feature
	space;human faces;two-dimensional recognition;unsupervised learning;computerised
	pattern recognition;eigenvalues and eigenfunctions;},
  timestamp = {02,500}
}

@ARTICLE{1991_JNL_Eigenfaces_Turk,
  author = {Turk, M., Pentland, A.},
  title = {Eigenfaces for recognition},
  journal = {Journal of cognitive neuroscience},
  year = {1991},
  volume = {3},
  pages = {71--86},
  number = {1},
  comment = {TRK_subspace},
  file = {:papers\\1991 JNL, Eigenfaces for Recognition (Turk).pdf:PDF},
  timestamp = {07,500}
}

@ARTICLE{1991_JNL_Recog_Ullman,
  author = {Ullman, S. and Basri, R.},
  title = {Recognition by linear combinations of models},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1991},
  volume = {13},
  pages = {992 -1006},
  number = {10},
  month = oct,
  abstract = {An approach to visual object recognition in which a 3D object is represented
	by the linear combination of 2D images of the object is proposed.
	It is shown that for objects with sharp edges as well as with smooth
	bounding contours, the set of possible images of a given object is
	embedded in a linear space spanned by a small number of views. For
	objects with sharp edges, the linear combination representation is
	exact. For objects with smooth boundaries, it is an approximation
	that often holds over a wide range of viewing angles. Rigid transformations
	(with or without scaling) can be distinguished from more general
	linear transformations of the object by testing certain constraints
	placed on the coefficients of the linear combinations. Three alternative
	methods of determining the transformation that matches a model to
	a given image are proposed},
  comment = {TRK_subspace},
  doi = {10.1109/34.99234},
  file = {:papers\\1991 JNL, Recognition by linear combinations of models (Ullman).pdf:PDF},
  issn = {0162-8828},
  keywords = {image combination;linear combinations;model combination;rigid transformations;sharp
	edges;smooth bounding contours;visual object recognition;computerised
	pattern recognition;computerised picture processing;},
  timestamp = {00,800}
}

@ARTICLE{2001_JNL_ImageIndexing_Vailaya,
  author = {Vailaya, A. and Figueiredo, M.A.T. and Jain, A.K. and Hong-Jiang
	Zhang},
  title = {Image classification for content-based indexing},
  journal = {Image Processing, IEEE Transactions on},
  year = {2001},
  volume = {10},
  pages = {117 -130},
  number = {1},
  abstract = {Grouping images into (semantically) meaningful categories using low-level
	visual features is a challenging and important problem in content-based
	image retrieval. Using binary Bayesian classifiers, we attempt to
	capture high-level concepts from low-level image features under the
	constraint that the test image does belong to one of the classes.
	Specifically, we consider the hierarchical classification of vacation
	images; at the highest level, images are classified as indoor or
	outdoor; outdoor images are further classified as city or landscape;
	finally, a subset of landscape images is classified into sunset,
	forest, and mountain classes. We demonstrate that a small vector
	quantizer (whose optimal size is selected using a modified MDL criterion)
	can be used to model the class-conditional densities of the features,
	required by the Bayesian methodology. The classifiers have been designed
	and evaluated on a database of 6931 vacation photographs. Our system
	achieved a classification accuracy of 90.5% for indoor/outdoor, 95.3%
	for city/landscape, 96.6% for sunset/forest and mountain, and 96%
	for forest/mountain classification problems. We further develop a
	learning method to incrementally train the classifiers as additional
	data become available. We also show preliminary results for feature
	reduction using clustering techniques. Our goal is to combine multiple
	two-class classifiers into a single hierarchical classifier},
  comment = {VQ},
  doi = {10.1109/83.892448},
  file = {:papers\\2001 JNL, Image classification for content-based indexing (Vailaya, Figueiredo, Jain, Zhang).pdf:PDF},
  issn = {1057-7149},
  keywords = {binary Bayesian classifiers;city;class-conditional densities;classification
	accuracy;clustering techniques;content-based image retrieval;content-based
	indexing;feature reduction;forest;hierarchical classification;image
	classification;indoor images;landscape;learning method;low-level
	image features;low-level visual features;modified MDL criterion;mountain;multiple
	two-class classifiers;outdoor images;semantically meaningful categories;sunset;test
	image;vacation images;vacation photographs database;vector quantizer;Bayes
	methods;content-based retrieval;feature extraction;image classification;image
	retrieval;pattern clustering;vector quantisation;},
  timestamp = {00,370}
}

@INPROCEEDINGS{2004_CNF_SigmaPointKalman_Merwe,
  author = {Van Der Merwe, Rudolph and Wan, Eric A.},
  title = {Sigma-point Kalman filters for integrated navigation},
  booktitle = {Proceedings of the Annual Meeting - Institute of Navigation},
  year = {2004},
  pages = {641 - 654},
  abstract = {Core to integrated navigation systems is the concept of fusing noisy
	observations from GPS, Inertial Measurement Units (IMU), and other
	available sensors. The current industry standard and most widely
	used algorithm for this purpose is the extended Kalman filter (EKF)
	[6]. The EKF combines the sensor measurements with predictions coming
	from a model of vehicle motion (either dynamic or kinematic), in
	order to generate an estimate of the current navigational state (position,
	velocity, and attitude). This paper points out the inherent shortcomings
	in using the EKF and presents, as an alternative, a family of improved
	derivativeless nonlinear Kalman filters called sigma-point Kalman
	filters (SPKF). We demonstrate the improved state estimation performance
	of the SPKF by applying it to the problem of loosely coupled GPS/INS
	integration. A novel method to account for latency in the GPS updates
	is also developed for the SPKF (such latency compensation is typically
	inaccurate or not practical with the EKF). A UAV (rotor-craft) test
	platform is used to demonstrate the results. Performance metrics
	indicate an approximate 30% error reduction in both attitude and
	position estimates relative to the baseline EKF implementation.},
  comment = {trk_filtering},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2004 CNF, Sigma-Point Kalman Filters for Integrated Navigation (Merwe, Wan, Navigation, 64).pdf:PDF},
  key = {Kalman filtering},
  keywords = {Accelerometers;Global positioning system;Mathematical models;Pattern
	recognition;Random processes;Sensors;Signal processing;Signal receivers;Signal
	to noise ratio;Velocity measurement;},
  language = {English},
  timestamp = {00,060}
}

@BOOK{1999_BOOK_NatureStatisticalLearning_Vapnik,
  title = {The Nature of Statistical Learning Theory (Information Science and
	Statistics)},
  publisher = {Springer},
  year = {1999},
  author = {Vapnik, Vladimir},
  edition = {2nd},
  month = {November},
  abstract = {{The aim of this book is to discuss the fundamental ideas which lie
	behind the statistical theory of learning and generalization. It
	considers learning as a general problem of function estimation based
	on empirical data. Omitting proofs and technical details, the author
	concentrates on discussing the main results of learning theory and
	their connections to fundamental problems in statistics. These include:
	* the setting of learning problems based on the model of minimizing
	the risk functional from empirical data * a comprehensive analysis
	of the empirical risk minimization principle including necessary
	and sufficient conditions for its consistency * non-asymptotic bounds
	for the risk achieved using the empirical risk minimization principle
	* principles for controlling the generalization ability of learning
	machines using small sample sizes based on these bounds * the Support
	Vector methods that control the generalization ability when estimating
	function using small sample size. The second edition of the book
	contains three new chapters devoted to further development of the
	learning theory and SVM techniques. These include: * the theory of
	direct method of learning based on solving multidimensional integral
	equations for density, conditional probability, and conditional density
	estimation * a new inductive principle of learning. Written in a
	readable and concise style, the book is intended for statisticians,
	mathematicians, physicists, and computer scientists. Vladimir N.
	Vapnik is Technology Leader AT\&T Labs-Research and Professor of
	London University. He is one of the founders of statistical learning
	theory, and the author of seven books published in English, Russian,
	German, and Chinese.}},
  comment = {PRML_book},
  day = {19},
  file = {:papers\\1999 BOOK, The Nature Of Statistical Learning Theory (Vapnik, Springer 2nd ed).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0387987800},
  posted-at = {2010-04-15 15:48:54},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387987800}
}

@INPROCEEDINGS{2003_CNF_DynamicsOfInteractingObjects_Vaswani,
  author = {Vaswani, N. and Roy Chowdhury, A. and Chellappa, R.},
  title = {Activity recognition using the dynamics of the configuration of interacting
	objects},
  booktitle = {Computer Vision and Pattern Recognition, 2003. Proceedings. 2003
	IEEE Computer Society Conference on},
  year = {2003},
  volume = {2},
  pages = {II - 633-40 vol.2},
  month = {june},
  abstract = { Monitoring activities using video data is an important surveillance
	problem. A special scenario is to learn the pattern of normal activities
	and detect abnormal events from a very low resolution video where
	the moving objects are small enough to be modeled as point objects
	in a 2D plane. Instead of tracking each point separately, we propose
	to model an activity by the polygonal 'shape' of the configuration
	of these point masses at any time t, and its deformation over time.
	We learn the mean shape and the dynamics of the shape change using
	hand-picked location data (no observation noise) and define an abnormality
	detection statistic for the simple case of a test sequence with negligible
	observation noise. For the more practical case where observation
	(point locations) noise is large and cannot be ignored, we use a
	particle filter to estimate the probability distribution of the shape
	given the noisy observations up to the current time. Abnormality
	detection in this case is formulated as a change detection problem.
	We propose a detection strategy that can detect both 'drastic' and
	'slow' abnormalities. Our framework can be directly applied for object
	location data obtained using any type of sensors - visible, radar,
	infrared or acoustic.},
  comment = {recog_action},
  doi = {10.1109/CVPR.2003.1211526},
  file = {:papers\\2003 CNF, Activity recognition using the dynamics of the configuration of interacting objects (Vaswani, Chowdhury, Chellappa, CVPR, 76).pdf:PDF},
  issn = {1063-6919 },
  keywords = { 2D plane; abnormal event; abnormality detection statistic; acoustic
	sensor; activity monitoring; activity pattern learning; activity
	recognition; detection strategy; drastic abnormality; hand-picked
	location data; infrared sensor; interacting object configuration
	dynamics; low resolution video; mean shape; moving object; noisy
	observation; object location data; observation noise; particle filter;
	point location noise; point object model; point tracking; polygonal
	shape configuration; probability distribution; radar sensor; shape
	change; slow abnormality; surveillance problem; test sequence; video
	data; visible sensor; computer vision; image motion analysis; image
	sensors; monitoring; object detection; probability; surveillance;
	target tracking; video signal processing;},
  timestamp = {00,080}
}

@ARTICLE{2001_JNL_MotionCorrespondence_Veenman,
  author = {Veenman, C.J. and Reinders, M.J.T. and Backer, E.},
  title = {Resolving motion correspondence for densely moving points},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2001},
  volume = {23},
  pages = {54 -72},
  number = {1},
  month = {jan},
  abstract = {Studies the motion correspondence problem for which a diversity of
	qualitative and statistical solutions exist. We concentrate on qualitative
	modeling, especially in situations where assignment conflicts arise
	either because multiple features compete for one detected point or
	because multiple detected points fit a single feature point. We leave
	out the possibility of point track initiation and termination because
	that principally conflicts with allowing for temporary point occlusion.
	We introduce individual, combined, and global motion models and fit
	existing qualitative solutions in this framework. Additionally, we
	present a tracking algorithm that satisfies these-possibly constrained-models
	in a greedy matching sense, including an effective way to handle
	detection errors and occlusion. The performance evaluation shows
	that the proposed algorithm outperforms existing greedy matching
	algorithms. Finally, we describe an extension to the tracker that
	enables automatic initialization of the point tracks. Several experiments
	show that the extended algorithm is efficient, hardly sensitive to
	its few parameters, and qualitatively better than other algorithms,
	including the presumed optimal statistical multiple hypothesis tracker},
  comment = {trk_correspondence},
  doi = {10.1109/34.899946},
  file = {:papers\\2001 JNL, Resolving motion correspondence for densely moving (Veenman, Reinders, Backer, PAMI, 102).pdf:PDF},
  issn = {0162-8828},
  keywords = {assignment conflicts;densely moving points;detection errors;greedy
	matching;motion correspondence;motion models;occlusion;optimal statistical
	multiple hypothesis tracker;qualitative modeling;qualitative solutions;tracking
	algorithm;image motion analysis;target tracking;},
  timestamp = {00,100}
}

@ARTICLE{2006_JNL_AlgebraicMotionSegmentation_Vidal,
  author = {Vidal, Rene and Ma, Yi},
  title = {A unified algebraic approach to 2-D and 3-D motion segmentation and
	estimation},
  journal = {Journal of Mathematical Imaging and Vision},
  year = {2006},
  volume = {25},
  pages = {403 - 421},
  number = {3},
  abstract = {In this paper, we present an analytic solution to the problem of estimating
	an unknown number of 2-D and 3-D motion models from two-view point
	correspondences or optical flow. The key to our approach is to view
	the estimation of multiple motion models as the estimation of a single
	multibody motion model. This is possible thanks to two important
	algebraic facts. First, we show that all the image measurements,
	regardless of their associated motion model, can be fit with a single
	real or complex polynomial. Second, we show that the parameters of
	the individual motion model associated with an image measurement
	can be obtained from the derivatives of the polynomial at that measurement.
	This leads to an algebraic motion segmentation and estimation algorithm
	that applies to most of the two-view motion models that have been
	adopted in computer vision. Our experiments show that the proposed
	algorithm out-performs existing algebraic and factorization-based
	methods in terms of efficiency and robustness, and provides a good
	initialization for iterative techniques, such as Expectation Maximization,
	whose performance strongly depends on good initialization. &copy;
	2006 Springer Science + Business Media, LLC.},
  comment = {motion},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2006 JNL, A unified algebraic approach to 2-D and 3-D motion segmentation and estimation (Vidal, Ma, JMIV, 53).pdf:PDF},
  issn = {09249907},
  key = {Motion estimation},
  keywords = {Algebra;Image analysis;Mathematical models;Polynomial approximation;Principal
	component analysis;},
  language = {English},
  timestamp = {00,050},
  url = {http://dx.doi.org/10.1007/s10851-006-8286-z}
}

@ARTICLE{2005_JNL_PedestrianTracking_ViolaJones,
  author = {Viola, P. and Jones, M. J. and Snow, D.},
  title = {Detecting pedestrians using patterns of motion and appearance},
  journal = {International Journal of Computer Vision},
  year = {2005},
  volume = {63},
  pages = {153-61},
  number = {Copyright 2005, IEE},
  abstract = {This paper describes a pedestrian detection system that integrates
	image intensity information with motion information. We use a detection
	style algorithm that scans a detector over two consecutive frames
	of a video sequence. The detector is trained (using AdaBoost) to
	take advantage of both motion and appearance information to detect
	a walking person. Past approaches have built detectors based on motion
	information or detectors based on appearance information, but ours
	is the first to combine both sources of information in a single detector.
	The implementation described runs at about 4 frames/second, detects
	pedestrians at very small scales (as small as 20 15 pixels), and
	has a very low false positive rate. Our approach builds on the detection
	work of Viola and Jones (2001). Novel contributions of this paper
	include: (i) development of a representation of image motion which
	is extremely efficient, and (ii) implementation of a state of the
	art pedestrian detection system which operates on low resolution
	images under difficult conditions (such as rain and snow)},
  comment = {det_pedestrian},
  file = {:papers\\2005 JNL, Detecting pedestrians using patterns of motion and appearance (Viola, Jones, IJCV, 656).pdf:PDF},
  keywords = {computer vision feature extraction image motion analysis image representation
	image resolution image sequences object detection},
  owner = {salman},
  timestamp = {00,700}
}

@ARTICLE{1997_JNL_AlignmentMutualInformation_Viola,
  author = {Viola, Paul and Wells III, William M.},
  title = {Alignment by Maximization of Mutual Information},
  journal = {International Journal of Computer Vision},
  year = {1997},
  volume = {24},
  pages = {137--154},
  number = {2},
  month = sep,
  abstract = {A new information-theoretic approach is presented for finding the
	pose of an object in an image. The technique does not require information
	about the surface properties of the object, besides its shape, and
	is robust with respect to variations of illumination. In our derivation
	few assumptions are made about the nature of the imaging process.
	As a result the algorithms are quite general and may foreseeably
	be used in a wide variety of imaging situations.},
  comment = {IT},
  file = {:papers\\1997 JNL, Alignment by Maximization of Mutual Information (Viola, IJCV, 1939).pdf:PDF},
  timestamp = {02,000},
  url = {http://dx.doi.org/10.1023/A:1007958904918}
}

@ARTICLE{1967_JNL_ErrorBoundsConvolutionalCodes_Viterbi,
  author = { Viterbi, A.},
  title = {Error bounds for convolutional codes and an asymptotically optimum
	decoding algorithm},
  journal = {Information Theory, IEEE Transactions on},
  year = {1967},
  volume = {13},
  pages = { 260 - 269},
  number = {2},
  month = {apr},
  abstract = { The probability of error in decoding an optimal convolutional code
	transmitted over a memoryless channel is bounded from above and below
	as a function of the constraint length of the code. For all but pathological
	channels the bounds are asymptotically (exponentially) tight for
	rates aboveR_{0}, the computational cutoff rate of sequential decoding.
	As a function of constraint length the performance of optimal convolutional
	codes is shown to be superior to that of block codes of the same
	length, the relative improvement increasing with rate. The upper
	bound is obtained for a specific probabilistic nonsequential decoding
	algorithm which is shown to be asymptotically optimum for rates aboveR_{0}and
	whose performance bears certain similarities to that of sequential
	decoding algorithms.},
  comment = {IT},
  file = {:papers\\1967 JNL, Error bounds for convolutional codes and an asymptotically optimum decoding algorithm (Viterbi, TIF, 3241).pdf:PDF},
  issn = {0018-9448},
  keywords = { Convolutional codes;},
  timestamp = {03,000}
}

@INPROCEEDINGS{2008_CNF_PoseTracking_Wagner,
  author = {Wagner, D. and Reitmayr, G. and Mulloni, A. and Drummond, T. and
	Schmalstieg, D.},
  title = {Pose tracking from natural features on mobile phones},
  booktitle = {Mixed and Augmented Reality, 2008. ISMAR 2008. 7th IEEE/ACM International
	Symposium on},
  year = {2008},
  pages = {125 -134},
  month = {15-18},
  abstract = {In this paper we present two techniques for natural feature tracking
	in real-time on mobile phones. We achieve interactive frame rates
	of up to 20 Hz for natural feature tracking from textured planar
	targets on current-generation phones. We use an approach based on
	heavily modified state-of-the-art feature descriptors, namely SIFT
	and Ferns. While SIFT is known to be a strong, but computationally
	expensive feature descriptor, Ferns classification is fast, but requires
	large amounts of memory. This renders both original designs unsuitable
	for mobile phones. We give detailed descriptions on how we modified
	both approaches to make them suitable for mobile phones. We present
	evaluations on robustness and performance on various devices and
	finally discuss their appropriateness for augmented reality applications.},
  comment = {trk},
  doi = {10.1109/ISMAR.2008.4637338},
  file = {papers\\2008 CNF, Pose Tracking from Natural Features (Wagner).pdf:PDF},
  keywords = {Ferns feature descriptor;SIFT feature descriptor;augmented reality;current-generation
	phone;mobile phone;natural feature tracking;pose tracking;textured
	planar target;augmented reality;feature extraction;image texture;mobile
	computing;mobile handsets;pose estimation;target tracking;},
  timestamp = {00,070}
}

@INPROCEEDINGS{2010_CNF_TRK_ped_Walk,
  author = {Walk, S. and Majer, N. and Schindler, K. and Schiele, B. },
  title = {New features and insights for pedestrian detection},
  booktitle = {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},
  year = {2010},
  pages = {1030--1037},
  comment = {TRK_ped},
  doi = {10.1109/CVPR.2010.5540102},
  file = {:papers\\2010 CNF, New Features and Insights for Pedestrian Detection (Walk).pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{2000_CNF_UKF_Wan,
  author = {Wan, E.A. and Van Der Merwe, R.},
  title = {The unscented Kalman filter for nonlinear estimation},
  booktitle = {Adaptive Systems for Signal Processing, Communications, and Control
	Symposium 2000. AS-SPCC. The IEEE 2000},
  year = {2000},
  pages = {153 -158},
  abstract = {This paper points out the flaws in using the extended Kalman filter
	(EKE) and introduces an improvement, the unscented Kalman filter
	(UKF), proposed by Julier and Uhlman (1997). A central and vital
	operation performed in the Kalman filter is the propagation of a
	Gaussian random variable (GRV) through the system dynamics. In the
	EKF the state distribution is approximated by a GRV, which is then
	propagated analytically through the first-order linearization of
	the nonlinear system. This can introduce large errors in the true
	posterior mean and covariance of the transformed GRV, which may lead
	to sub-optimal performance and sometimes divergence of the filter.
	The UKF addresses this problem by using a deterministic sampling
	approach. The state distribution is again approximated by a GRV,
	but is now represented using a minimal set of carefully chosen sample
	points. These sample points completely capture the true mean and
	covariance of the GRV, and when propagated through the true nonlinear
	system, captures the posterior mean and covariance accurately to
	the 3rd order (Taylor series expansion) for any nonlinearity. The
	EKF in contrast, only achieves first-order accuracy. Remarkably,
	the computational complexity of the UKF is the same order as that
	of the EKF. Julier and Uhlman demonstrated the substantial performance
	gains of the UKF in the context of state-estimation for nonlinear
	control. Machine learning problems were not considered. We extend
	the use of the UKF to a broader class of nonlinear estimation problems,
	including nonlinear system identification, training of neural networks,
	and dual estimation problems. In this paper, the algorithms are further
	developed and illustrated with a number of additional examples},
  comment = {trk_filtering},
  doi = {10.1109/ASSPCC.2000.882463},
  file = {:papers\\2000 CNF, The unscented Kalman filter for nonlinear estimation (Wan, Merwe, ASSPCC, 598).pdf:PDF},
  keywords = {Gaussian random variable;computational complexity;covariance;deterministic
	sampling approach;dual estimation problems;extended Kalman filter;machine
	learning;neural networks training;nonlinear control;nonlinear estimation;nonlinear
	system;nonlinear system identification;posterior mean;sample points;state
	distribution;state-estimation;system dynamics;unscented Kalman filter;adaptive
	Kalman filters;computational complexity;covariance analysis;filtering
	theory;learning (artificial intelligence);nonlinear estimation;nonlinear
	systems;parameter estimation;state estimation;},
  timestamp = {00,600}
}

@ARTICLE{1994_JNL_TRKmotion_Wang,
  author = {Wang, J.Y.A. and Adelson, E.H.},
  title = {Representing moving images with layers},
  journal = {Image Processing, IEEE Transactions on},
  year = {1994},
  volume = {3},
  pages = {625 -638},
  number = {5},
  month = {sep},
  abstract = {We describe a system for representing moving images with sets of overlapping
	layers. Each layer contains an intensity map that defines the additive
	values of each pixel, along with an alpha map that serves as a mask
	indicating the transparency. The layers are ordered in depth and
	they occlude each other in accord with the rules of compositing.
	Velocity maps define how the layers are to be warped over time. The
	layered representation is more flexible than standard image transforms
	and can capture many important properties of natural image sequences.
	We describe some methods for decomposing image sequences into layers
	using motion analysis, and we discuss how the representation may
	be used for image coding and other applications},
  comment = {?},
  doi = {10.1109/83.334981},
  file = {:papers\\1994 JNL, Representing moving images with layers (Wang, Adelson, TIP, 812).pdf:PDF},
  issn = {1057-7149},
  keywords = {alpha map;image coding;image representation;image sequences decomposition;intensity
	map;layered representation;motion analysis;motion estimation;motion
	segmentation;moving images;overlapping layers;transparency;velocity
	maps;video sequences;image coding;image segmentation;image sequences;motion
	estimation;video signals;},
  timestamp = {00,800}
}

@ARTICLE{4401720,
  author = {Junqiu Wang and Yasushi Yagi},
  title = {Integrating Color and Shape-Texture Features for Adaptive Real-Time
	Object Tracking},
  journal = {Image Processing, IEEE Transactions on},
  year = {2008},
  volume = {17},
  pages = {235 -240},
  number = {2},
  month = {feb. },
  abstract = {We extend the standard mean-shift tracking algorithm to an adaptive
	tracker by selecting reliable features from color and shape-texture
	cues according to their descriptive ability. The target model is
	updated according to the similarity between the initial and current
	models, and this makes the tracker more robust. The proposed algorithm
	has been compared with other trackers using challenging image sequences,
	and it provides better performance.},
  comment = {trk_color},
  doi = {10.1109/TIP.2007.914150},
  file = {:papers\\2008 JNL, Integrating color and shape-texture features for adaptive real-time object tracking (Wang, Yagi).pdf:PDF},
  issn = {1057-7149},
  keywords = {adaptive real-time object tracking;adaptive tracker;color features;image
	sequences;mean-shift tracking algorithm;shape-texture features;target
	model;feature extraction;image colour analysis;image sequences;image
	texture;object detection;target tracking;Algorithms;Artificial Intelligence;Color;Colorimetry;Computer
	Systems;Image Enhancement;Image Interpretation, Computer-Assisted;Motion;Pattern
	Recognition, Automated;Systems Integration;},
  timestamp = {00,020}
}

@ARTICLE{2003_JNL_SURVEYaction_JWang,
  author = {Wang, Jessica JunLin and Singh, Sameer},
  title = {Video analysis of human dynamics - A survey},
  journal = {Real-Time Imaging},
  year = {2003},
  volume = {9},
  pages = {321-346},
  abstract = {Video analysis of human dynamics is an important area of research
	devoted to detecting people and understanding their dynamic physical
	behavior in a complex environment that can be used for biometric
	applications. This paper provides a detailed survey of the various
	studies in areas related to the tracking of people and body parts
	such as face, hands, fingers, legs, etc., and modeling behavior using
	motion analysis.},
  comment = {survey},
  file = {:papers\\2003 JNL, Video analysis of human dynamics - a survey (SURVEY, Singh, RTI, 78).pdf:PDF},
  keywords = {Image analysis Animation Computer graphics Face recognition Image
	understanding Mathematical models Object recognition},
  owner = {salman},
  review = {salman: nice hierarchy of tracking},
  timestamp = {00,080}
}

@ARTICLE{2003_JNL_SURVEYmotion_LWang,
  author = {Wang, Liang and Hu, Weiming and Tan, Tieniu},
  title = {Recent developments in human motion analysis},
  journal = {Pattern Recognition},
  year = {2003},
  volume = {36},
  pages = {585-601},
  abstract = {Visual analysis of human motion is currently one of the most active
	research topics in computer vision. This strong interest is driven
	by a wide spectrum of promising applications in many areas such as
	virtual reality, smart surveillance, perceptual interface, etc. Human
	motion analysis concerns the detection, tracking and recognition
	of people, and more generally, the understanding of human behaviors,
	from image sequences involving humans. This paper provides a comprehensive
	survey of research on computer-vision-based human motion analysis.
	The emphasis is on three major issues involved in a general human
	motion analysis system, namely human detection, tracking and activity
	understanding. Various methods for each issue are discussed in order
	to examine the state of the art. Finally, some research challenges
	and future directions are discussed},
  comment = {survey},
  file = {:papers\\2003 JNL, Recent developments in human motion analysis (SURVEY, Tan, PR, 392).pdf:PDF},
  keywords = {computer vision motion estimation object recognition tracking},
  owner = {salman},
  review = {salman: a good review},
  timestamp = {00,400}
}

@ARTICLE{2003_JNL_gait_Wang,
  author = {Wang, L. and Tan, T. and Ning, H. and Hu, W.},
  title = {Silhouette analysis-based gait recognition for human identification},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2003},
  volume = {25},
  pages = {1505--1518},
  number = {12},
  comment = {TRK_gait},
  file = {:papers\\2003 JNL, Silhouette Analysis-Based Gait Recognition (Wang).pdf:PDF},
  issn = {0162-8828},
  publisher = {IEEE},
  timestamp = {00,350}
}

@INPROCEEDINGS{20094212372474,
  author = {Wang, Robert Y. and Popovic, Jovan},
  title = {Real-time hand-tracking with a color glove},
  year = {2009},
  volume = {28},
  number = {3},
  address = {1515 Broadway, 17th Floor, New York, NY 10036-5701, United States},
  note = {Animation control;Consumer applications;Hand tracking;Interactive
	rates;Motion capture;Nearest-neighbor approaches;Pose estimation;Proof
	of concept;Single cameras;Tracking system;},
  abstract = {Articulated hand-tracking systems have been widely used in virtual
	reality but are rarely deployed in consumer applications due to their
	price and complexity. In this paper, we propose an easy-to-use and
	inexpensive system that facilitates 3-D articulated user-input using
	the hands. Our approach uses a single camera to track a hand wearing
	an ordinary cloth glove that is imprinted with a custom pattern.
	The pattern is designed to simplify the pose estimation problem,
	allowing us to employ a nearest-neighbor approach to track hands
	at interactive rates. We describe several proof-of-concept applications
	enabled by our system that we hope will provide a foundation for
	new interactions in modeling, animation control and augmented reality.
	&copy; 2009 ACM.},
  comment = {trk_color},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2009 JNL, Real-time hand-tracking with a color glove (Wang, Popovic).pdf:PDF},
  issn = {07300301},
  journal = {ACM Transactions on Graphics},
  key = {Virtual reality},
  keywords = {Animation;Augmented reality;Three dimensional;User interfaces;},
  language = {English},
  timestamp = {00,010},
  url = {http://dx.doi.org/10.1145/1531326.1531369}
}

@ARTICLE{2009_JNL_ActionRecognition_Wang,
  author = {Wang,Yang and Mori, G.},
  title = {Human Action Recognition by Semilatent Topic Models},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2009},
  volume = {31},
  pages = {1762 -1774},
  number = {10},
  month = {oct. },
  abstract = {We propose two new models for human action recognition from video
	sequences using topic models. Video sequences are represented by
	a novel "bag-of-words" representation, where each frame corresponds
	to a "word". Our models differ from previous latent topic models
	for visual recognition in two major aspects: first of all, the latent
	topics in our models directly correspond to class labels; second,
	some of the latent variables in previous topic models become observed
	in our case. Our models have several advantages over other latent
	topic models used in visual recognition. First of all, the training
	is much easier due to the decoupling of the model parameters. Second,
	it alleviates the issue of how to choose the appropriate number of
	latent topics. Third, it achieves much better performance by utilizing
	the information provided by the class labels in the training set.
	We present action classification results on five different data sets.
	Our results are either comparable to, or significantly better than
	previously published results on these data sets.},
  comment = {recog_action},
  doi = {10.1109/TPAMI.2009.43},
  file = {:papers\\2009 JNL, Human Action Recognition by Semilatent Topic Models (Wang, Mori, PAMI, 4).pdf:PDF},
  issn = {0162-8828},
  keywords = {action classification;activity understanding;bag-of-words representation;class
	labels;event understanding;human action recognition;latent variables;semilatent
	topic model;video sequences;visual recognition;computer vision;gesture
	recognition;image classification;image motion analysis;image representation;image
	sequences;video signal processing;},
  timestamp = {-}
}

@INPROCEEDINGS{2009_CNF_SURVEYmotion_Wei,
  author = {Wei Wei and An Yunxiao},
  title = {Vision-Based Human Motion Recognition: A Survey},
  booktitle = {Intelligent Networks and Intelligent Systems, 2009. ICINIS '09. Second
	International Conference on},
  year = {2009},
  pages = {386 -389},
  month = nov.,
  abstract = {With the development of computer vision and image processing technology,
	human motion recognition, because of its wide range of applications,
	now has been attracting extensive attention in the field of computer
	vision. Vision-based recognition of people's motion not only includes
	the knowledge of image recognition and computer vision, but also
	involves the theory of recognition and artificial intelligence, so
	it is an extremely challenging interdisciplinary project. The research
	work in recent years have been analyzed and summarized, respectively
	from the classification of human motion, human motion feature extraction
	and recognition algorithm to introduce a number of human motion recognition.},
  comment = {SURVEYmotion},
  doi = {10.1109/ICINIS.2009.105},
  file = {:c\:\\salman\\work\\writing\\papers\\2009 CNF, Vision-based human motion recognition_ A Survey (Wei).pdf:PDF},
  keywords = {artificial intelligence;computer vision;human motion feature extraction;image
	processing technology;image recognition;vision-based human motion
	recognition;artificial intelligence;computer vision;feature extraction;image
	recognition;}
}

@ARTICLE{1992_JNL_CameraCalibration_Weng,
  author = {Weng, J. and Cohen, P. and Herniou, M.},
  title = {Camera calibration with distortion models and accuracy evaluation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1992},
  volume = {14},
  pages = {965 -980},
  number = {10},
  month = {oct},
  abstract = {A camera model that accounts for major sources of camera distortion,
	namely, radial, decentering, and thin prism distortions is presented.
	The proposed calibration procedure consists of two steps: (1) the
	calibration parameters are estimated using a closed-form solution
	based on a distribution-free camera model; and (2) the parameters
	estimated in the first step are improved iteratively through a nonlinear
	optimization, taking into account camera distortions. According to
	minimum variance estimation, the objective function to be minimized
	is the mean-square discrepancy between the observed image points
	and their inferred image projections computed with the estimated
	calibration parameters. The authors introduce a type of measure that
	can be used to directly evaluate the performance of calibration and
	compare calibrations among different systems. The validity and performance
	of the calibration procedure are tested with both synthetic data
	and real images taken by tele- and wide-angle lenses},
  comment = {3D_camera},
  doi = {10.1109/34.159901},
  file = {:papers\\1992 JNL, Camera calibration with distortion models and accuracy evaluation (Weng).pdf:PDF},
  issn = {0162-8828},
  keywords = {CCD camera;accuracy;camera calibration;closed-form solution;computer
	vision;distortion models;inferred image projections;mean-square discrepancy;nonlinear
	optimization;objective function;observed image points;CCD image sensors;calibration;cameras;computer
	vision;optical information processing;optimisation;},
  timestamp = {01,000}
}

@ARTICLE{1838_JNL_Stereo_Wheatstone,
  author = {Wheatstone, Charles},
  title = {Contributions to the Physiology of Vision.Part the First. On some
	remarkable, and hitherto unobserved, Phenomena of Binocular Vision},
  journal = {Philosophical Transactions},
  year = {1838},
  volume = {128},
  pages = {371-394},
  comment = {3D_stereo},
  doi = {http://www.stereoscopy.com/library/wheatstone-paper1838.html},
  file = {:papers\\1838 JNL, Contributions to the Physiology of Vision.Part the First. On some remarkable, and hitherto unobserved, Phenomena of Binocular Vision (Wheatstone).pdf:PDF},
  timestamp = {?}
}

@INPROCEEDINGS{1995_CNF_GestureAnalysis_Wilson,
  author = {Wilson, A. D. and Bobick, A. F.},
  title = {Learning visual behavior for gesture analysis},
  booktitle = {Proceedings of International Symposium on Computer Vision - ISCV,
	21-23 Nov. 1995},
  year = {1995},
  abstract = {A state-based method for learning visual behavior from image sequences
	is presented. The technique is novel for its incorporation of multiple
	representations into the Hidden Markov Model framework. Independent
	representations of the instantaneous visual input at each state of
	the Markov model are estimated concurrently with the learning of
	the temporal characteristics. Measures of the degree to which each
	representation describes the input are combined to determine an input's
	overall membership to a state. We exploit two constraints allowing
	application of the technique to view-based gesture recognition: gestures
	are modal in the space of possible human motion, and gestures are
	viewpoint-dependent. The recovery of the visual behavior of a number
	of simple gestures with a small number of low resolution image sequences
	is shown},
  comment = {recog_action_appearance},
  file = {:papers\\1995 CNF, Learning visual behavior for gesture analysis (Wilson, Bobick, ISCV, 84).pdf:PDF},
  keywords = {computer vision hidden Markov models image recognition image sequences
	motion estimation},
  owner = {salman},
  timestamp = {00,100}
}

@INPROCEEDINGS{1981_CNF_VQspeech_Wong,
  author = {Wong, D. and Juang, B. and Gray, A., Jr.},
  title = {Recent developments in vector quantization for speech processing},
  booktitle = {Acoustics, Speech, and Signal Processing, IEEE International Conference
	on ICASSP '81.},
  year = {1981},
  abstract = {Vector Quantization is applied to modify a 2400 bps LPC vocoder to
	operate at 800 bps, while retaining acceptable intelligibility and
	naturalness of quality. The design of this speech compression system
	is discussed and compared to other very low bit rate vocoders. Advantages
	of vector quantization over a scalar technique are examined in detail,
	and several new properties are presented.},
  comment = {survey, VQ},
  file = {:papers\\1981 CNF, Recent Developments in Vector Quantization for Speech Processing (Wong, Juang, A. Gray, ICASSP, 5).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{1997_JNL_Pfinder_Wren,
  author = {Wren, C. R. and Azarbayejani, A. and Darrell, T. and Pentland, A.
	P.},
  title = {PFINDER: real-time tracking of the human body},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1997},
  volume = {19},
  pages = {780-785},
  number = {7},
  abstract = {Pfinder is a real-time system for tracking people and interpreting
	their behavior. It runs at 10 Hz on a standard SGI Indy computer,
	and has performed reliably on thousands of people in many different
	physical locations. The system uses a multiclass statistical model
	of color and shape to obtain a 2D representation of head and hands
	in a wide range of viewing conditions. Pfinder has been successfully
	used in a wide range of applications including wireless interfaces,
	video databases, and low-bandwidth coding},
  comment = {trk_people},
  file = {:papers\\1997 JNL, Pfinder Real-Time Tracking (Wren, Pentland, PAMI, 2774).pdf:PDF},
  keywords = {image coding image colour analysis image representation image segmentation
	video signal processing 2D representation Pfinder behavior interpretation
	human body low-bandwidth coding multiclass statistical model people
	tracking real-time system standard SGI Indy computer video databases
	wireless interfaces},
  owner = {salman},
  timestamp = {02,800}
}

@ARTICLE{2010_JNL_TRK_eval_Hu,
  author = {Hao Wu and Sankaranarayanan, A.C. and Chellappa, R.},
  title = {Online Empirical Evaluation of Tracking Algorithms},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2010},
  volume = {32},
  pages = {1443 -1458},
  number = {8},
  month = aug.,
  abstract = {Evaluation of tracking algorithms in the absence of ground truth is
	a challenging problem. There exist a variety of approaches for this
	problem, ranging from formal model validation techniques to heuristics
	that look for mismatches between track properties and the observed
	data. However, few of these methods scale up to the task of visual
	tracking, where the models are usually nonlinear and complex and
	typically lie in a high-dimensional space. Further, scenarios that
	cause track failures and/or poor tracking performance are also quite
	diverse for the visual tracking problem. In this paper, we propose
	an online performance evaluation strategy for tracking systems based
	on particle filters using a time-reversed Markov chain. The key intuition
	of our proposed methodology relies on the time-reversible nature
	of physical motion exhibited by most objects, which in turn should
	be possessed by a good tracker. In the presence of tracking failures
	due to occlusion, low SNR, or modeling errors, this reversible nature
	of the tracker is violated. We use this property for detection of
	track failures. To evaluate the performance of the tracker at time
	instant t, we use the posterior of the tracking algorithm to initialize
	a time-reversed Markov chain. We compute the posterior density of
	track parameters at the starting time t = 0 by filtering back in
	time to the initial time instant. The distance between the posterior
	density of the time-reversed chain (at t = 0) and the prior density
	used to initialize the tracking algorithm forms the decision statistic
	for evaluation. It is observed that when the data are generated by
	the underlying models, the decision statistic takes a low value.
	We provide a thorough experimental analysis of the evaluation methodology.
	Specifically, we demonstrate the effectiveness of our approach for
	tackling common challenges such as occlusion, pose, and illumination
	changes and provide the Receiver Operating Characteristic (ROC) curves.
	Finally, we also s- - how the applicability of the core ideas of
	the paper to other tracking algorithms such as the Kanade-Lucas-Tomasi
	(KLT) feature tracker and the mean-shift tracker.},
  comment = {TRK},
  doi = {10.1109/TPAMI.2009.135},
  file = {:papers\\2010 JNL, Online Empirical Evaluation of Tracking Algorithms (Wu).pdf:PDF},
  issn = {0162-8828},
  keywords = {Kanade Lucas Tomasi feature tracker;SNR;formal model validation techniques;ground
	truth;mean shift tracker;online empirical evaluation;receiver operating
	characteristic curves;time reversed Markov chain;tracking algorithms;visual
	tracking;Markov processes;computer vision;formal verification;particle
	filtering (numerical methods);tracking;},
  timestamp = {-}
}

@ARTICLE{2002377077434,
  author = {Wu, Ying and Huang, Thomas S.},
  title = {Nonstationary color tracking for vision-based human - Computer interaction},
  journal = {IEEE Transactions on Neural Networks},
  year = {2002},
  volume = {13},
  pages = {948 - 960},
  number = {4},
  note = {Color tracking;},
  abstract = {Skin color offers a strong cue for efficient localization and tracking
	of human body parts in video sequences for vision-based human-computer
	interaction. Color-based target localization could be achieved by
	analyzing segmented skin color regions. However, one of the challenges
	of color-based target tracking is that color distributions would
	change in different lighting conditions such that fixed color models
	would be inadequate to capture nonstationary color distributions
	over time. Meanwhile, using a fixed skin color model trained by the
	data of a specific person would probably not work well for other
	people. Although some work has been done on adaptive color models,
	this problem still needs further studies. This paper presents our
	investigation of color-based image segmentation and nonstationary
	color-based target tracking, by studying two different representations
	for color distributions. In this paper, we propose the structure
	adaptive self-organizing map (SASOM) neural network that serves as
	a new color model. Our experiments show that such a representation
	is powerful for efficient image segmentation. Then, we formulate
	the nonstationary color tracking problem as a model transduction
	problem, the solution of which offers a way to adapt and transduce
	color classifiers in nonstationary color distributions. To fulfill
	model transduction, this paper proposes two algorithms, the SASOM
	transduction and the discriminant expectation-maximazation (EM),
	based on the SASOM color model and the Gaussian mixture color model,
	respectively. Our extensive experiments on the task of real-time
	face/hand localization show that these two algorithms can successfully
	handle some difficulties in nonstationary color tracking. We also
	implemented a real-time face/hand localization system based on such
	algorithms for vision-based human-computer interaction.},
  comment = {trk_color},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2002 JNL, Nonstationary color tracking  for vision-based human-computer interaction  (Wu, Huang).pdf:PDF},
  issn = {10459227},
  key = {Image segmentation},
  keywords = {Algorithms;Color;Human computer interaction;Mathematical models;Self
	organizing maps;},
  language = {English},
  timestamp = {00,050},
  url = {http://dx.doi.org/10.1109/TNN.2002.1021895}
}

@ARTICLE{1993_JNL_GraphTheoreticClustering_Wu,
  author = {Wu, Z. and Leahy, R.},
  title = {An optimal graph theoretic approach to data clustering: theory and
	its application to image segmentation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1993},
  volume = {15},
  pages = {1101 -1113},
  number = {11},
  month = {nov},
  abstract = {A novel graph theoretic approach for data clustering is presented
	and its application to the image segmentation problem is demonstrated.
	The data to be clustered are represented by an undirected adjacency
	graph Gscr; with arc capacities assigned to reflect the similarity
	between the linked vertices. Clustering is achieved by removing arcs
	of Gscr; to form mutually exclusive subgraphs such that the largest
	inter-subgraph maximum flow is minimized. For graphs of moderate
	size (~ 2000 vertices), the optimal solution is obtained through
	partitioning a flow and cut equivalent tree of Gscr;, which can be
	efficiently constructed using the Gomory-Hu algorithm (1961). However
	for larger graphs this approach is impractical. New theorems for
	subgraph condensation are derived and are then used to develop a
	fast algorithm which hierarchically constructs and partitions a partially
	equivalent tree of much reduced size. This algorithm results in an
	optimal solution equivalent to that obtained by partitioning the
	complete equivalent tree and is able to handle very large graphs
	with several hundred thousand vertices. The new clustering algorithm
	is applied to the image segmentation problem. The segmentation is
	achieved by effectively searching for closed contours of edge elements
	(equivalent to minimum cuts in Gscr;), which consist mostly of strong
	edges, while rejecting contours containing isolated strong edges.
	This method is able to accurately locate region boundaries and at
	the same time guarantees the formation of closed edge contours},
  comment = {PRML},
  doi = {10.1109/34.244673},
  file = {:papers\\1993 JNL, An optimal graph theoretic approach to data clustering_ Theory and its applications (Wu, Leahy, PAMI, 468).pdf:PDF},
  issn = {0162-8828},
  keywords = {arc capacities;closed edge contours;data clustering;flow and cut equivalent
	tree partitioning;image segmentation;largest inter-subgraph maximum
	flow minimization;mutually exclusive subgraphs;optimal graph theoretic
	approach;partially equivalent tree;region boundary location;subgraph
	condensation;undirected adjacency graph;graph theory;image segmentation;minimax
	techniques;pattern recognition;},
  timestamp = {00,500}
}

@ARTICLE{2010_JNL_SURVEYmotion_Ji,
  author = {Xiaofei, Ji and Honghai, Liu},
  title = {Advances in View-Invariant Human Motion Analysis: A Review},
  journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews,
	IEEE Transactions on},
  year = {2010},
  volume = {40},
  pages = {13-24},
  abstract = {As viewpoint issue is becoming a bottleneck for human motion analysis
	and its application, in recent years, researchers have been devoted
	to view-invariant human motion analysis and have achieved inspiring
	progress. The challenge here is to find a methodology that can recognize
	human motion patterns to reach increasingly sophisticated levels
	of human behavior description. This paper provides a comprehensive
	survey of this significant research with the emphasis on view-invariant
	representation, and recognition of poses and actions. In order to
	help readers understand the integrated process of visual analysis
	of human motion, this paper presents recent development in three
	major issues involved in a general human motion analysis system,
	namely, human detection, view-invariant pose representation and estimation,
	and behavior understanding. Public available standard datasets are
	recommended. The concluding discussion assesses the progress so far,
	and outlines some research challenges and future directions, and
	solution to what is essential to achieve the goals of human motion
	analysis.},
  comment = {survey},
  file = {:papers\\2010 JNL, Advances in view-invariant human motion analysis_ a review (SURVEY,  Xiaofei, Liu,TSMCC, __).pdf:PDF},
  keywords = {behavioural sciences computing image motion analysis image representation
	pose estimation action recognition human behavior understanding human
	detection human motion pattern recognition pose recognition view-invariant
	human motion visual analysis view-invariant pose estimation view-invariant
	pose representation Behavior understanding human motion analysis
	pose representation and estimation view invariant},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{2004538756001,
  author = {Xiong, Tao and Debrunner, Christian},
  title = {Stochastic car tracking with line- And color-based features},
  year = {2004},
  volume = {5},
  number = {4},
  pages = {324 - 328},
  note = {Color-based tracking;Edge-based tracking;Feature integration;Monte
	Carlo filter;Multiple hypothesis;Particle filter;},
  abstract = {Color- and edge-based trackers can often be "distracted," causing
	them to track the wrong object. Many researchers have dealt with
	this problem by using multiple features, as it is unlikely that all
	will be distracted at the same time. It is also important for the
	tracker to maintain multiple hypotheses for the state; sequential
	Monte Carlo filters have been shown to be a convenient and straightforward
	means of maintaining multiple hypotheses. In this paper, we improve
	the accuracy and robustness of real-time tracking by combining a
	color histogram feature with an edge-gradient-based shape feature
	under a sequential Monte Carlo framework.},
  comment = {trk_color},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2004 JNL, Stochastic car tracking with line- and color-based features (Xiong, Debrunner).pdf:PDF},
  issn = {15249050},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  key = {Feature extraction},
  keywords = {Algorithms;Automobile drivers;Color image processing;Computer vision;Condensation;Edge
	detection;Highway accidents;Image sensors;Monte Carlo methods;Real
	time systems;Tracking (position);},
  language = {English},
  timestamp = {00,020},
  url = {http://dx.doi.org/10.1109/TITS.2004.838192}
}

@INPROCEEDINGS{2002_CNF_ContourTrackingGraphCutsActiveContours_Xu,
  author = {Xu, N. and Ahuja, N.},
  title = {Object contour tracking using graph cuts based active contours},
  booktitle = {Image Processing. 2002. Proceedings. 2002 International Conference
	on},
  year = {2002},
  volume = {3},
  pages = { III-277 - III-280 vol.3},
  abstract = { In this paper, we present an object contour tracking approach using
	graph cuts based active contours (GCBAC). Our proposed algorithm
	does not need any a priori global shape model, which makes it useful
	for tracking objects with deformable shapes and appearances. GCBAC
	are not sensitive to initial conditions and always converge to the
	optimal contour within the dilated neighborhood of itself. Given
	an initial boundary near the object in the first frame, GCBAC can
	iteratively converge to an optimal object boundary. In each frame
	thereafter, the resulting contour in the previous frame is taken
	as initialization and the algorithm consists of two steps. In the
	first step, GCBAC are applied to the difference between this frame
	and its previous one. The resulting contour is taken as initialization
	of the second step, which applies GCBAC to current frame directly.
	To evaluate the tracking performance, we apply the algorithm to several
	real world video sequences. Experimental results are provided.},
  comment = {trk_contour},
  doi = {10.1109/ICIP.2002.1038959},
  file = {:papers\\2002 CNF, Object contour tracking using graph cuts based active contours (Xu, Ahuja, ICIP, 79).pdf:PDF},
  issn = {1522-4880 },
  keywords = { deformable shapes; dilated neighborhood; graph cuts based active
	contours; iterative object segmentation; object contour tracking;
	optimal object boundary; s-t minimum cut algorithm; video sequences;
	graph theory; image segmentation; image sequences; iterative methods;
	tracking; video signal processing;},
  timestamp = {00,080}
}

@INPROCEEDINGS{1992_CNF_HumanActionHMM_Yamato,
  author = {Yamato, J. and Ohya, J. and Ishii, K.},
  title = {Recognizing human action in time-sequential images using hidden Markov
	model},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  year = {1992},
  abstract = {A human action recognition method based on a hidden Markov model (HMM)
	is proposed. It is a feature-based bottom-up approach that is characterized
	by its learning capability and time-scale invariability. To apply
	HMMs, one set of time-sequential images is transformed into an image
	feature vector sequence, and the sequence is converted into a symbol
	sequence by vector quantization. In learning human action categories,
	the parameters of the HMMs, one per category, are optimized so as
	to best describe the training sequences from the category. To recognize
	an observed sequence, the HMM which best matches the sequence is
	chosen. Experimental results for real time-sequential images of sports
	scenes show recognition rates higher than 90%. The recognition rate
	is improved by increasing the number of people used to generate the
	training data, indicating the possibility of establishing a person-independent
	action recognizer},
  comment = {recog_action_shape},
  file = {:papers\\1992 CNF, Recognizing human action in time-sequential images using hidden Markov (Yamato, CVPR, 586).pdf:PDF},
  keywords = {feature extraction hidden Markov models vector quantisation},
  owner = {salman},
  timestamp = {00,600}
}

@INPROCEEDINGS{2005_CNF_EventDetection_Yan,
  author = {Yan, Ke and Sukthankar, R. and Hebert, M.},
  title = {Efficient visual event detection using volumetric features},
  booktitle = {Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference
	on},
  year = {2005},
  abstract = {This paper studies the use of volumetric features as an alternative
	to popular local descriptor approaches for event detection in video
	sequences. Motivated by the recent success of similar ideas in object
	detection on static images, we generalize the notion of 2D box features
	to 3D spatio-temporal volumetric features. This general framework
	enables us to do real-time video analysis. We construct a realtime
	event detector for each action of interest by learning a cascade
	of filters based on volumetric features that efficiently scans video
	sequences in space and time. This event detector recognizes actions
	that are traditionally problematic for interest point methods - such
	as smooth motions where insufficient space-time interest points are
	available. Our experiments demonstrate that the technique accurately
	detects actions on real-world sequences and is robust to changes
	in viewpoint, scale and action speed. We also adapt our technique
	to the related task of human action classification and confirm that
	it achieves performance comparable to a current interest point based
	human activity recognizer on a standard database of human activities.},
  comment = {recog_action_opticalFlow},
  file = {:papers\\2005 CNF, Efficient visual event detection using volumetric features (Hebert, ICCV, 155).pdf:PDF},
  keywords = {feature extraction image classification image motion analysis image
	sequences object detection video signal processing 2D box feature
	3D spatiotemporal volumetric feature human action classification
	interest point based human activity recognizer local descriptor real-time
	video analysis real-world sequence smooth motion static image video
	sequence visual event detection},
  owner = {salman},
  timestamp = {00,160}
}

@ARTICLE{2007_JNL_H264rateDistortion_Yang,
  author = {En-Hui Yang and Xiang Yu},
  title = {Rate Distortion Optimization for H.264 Interframe Coding: A General
	Framework and Algorithms},
  journal = {Image Processing, IEEE Transactions on},
  year = {2007},
  volume = {16},
  pages = {1774-1784},
  number = {7},
  month = {July },
  abstract = {Rate distortion (RD) optimization for H.264 interframe coding with
	complete baseline decoding compatibility is investigated on a frame
	basis. Using soft decision quantization (SDQ) rather than the standard
	hard decision quantization, we first establish a general framework
	in which motion estimation, quantization, and entropy coding (in
	H.264) for the current frame can be jointly designed to minimize
	a true RD cost given previously coded reference frames. We then propose
	three RD optimization algorithms-a graph-based algorithm for near
	optimal SDQ in H.264 baseline encoding given motion estimation and
	quantization step sizes, an algorithm for near optimal residual coding
	in H.264 baseline encoding given motion estimation, and an iterative
	overall algorithm to optimize H.264 baseline encoding for each individual
	frame given previously coded reference frames-with them embedded
	in the indicated order. The graph-based algorithm for near optimal
	SDQ is the core; given motion estimation and quantization step sizes,
	it is guaranteed to perform optimal SDQ if the weak adjacent block
	dependency utilized in the context adaptive variable length coding
	of H.264 is ignored for optimization. The proposed algorithms have
	been implemented based on the reference encoder JM82 of H.264 with
	complete compatibility to the baseline profile. Experiments show
	that for a set of typical video testing sequences, the graph-based
	algorithm for near optimal SDQ, the algorithm for near optimal residual
	coding, and the overall algorithm achieve on average, 6%, 8%, and
	12%, respectively, rate reduction at the same PSNR (ranging from
	30 to 38 dB) when compared with the RD optimization method implemented
	in the H.264 reference software.},
  doi = {10.1109/TIP.2007.896685},
  issn = {1057-7149},
  keywords = {entropy codes, motion estimation, quantisation (signal), residue codes,
	video codingH.264 reference software, JM82, baseline encoding, entropy
	coding, graph-based algorithm, hard decision quantization, interframe
	coding, motion estimation, rate distortion optimization, residual
	coding, soft decision quantization, video testing sequences}
}

@ARTICLE{2007_CNF_Survey3DmodelRetrieval_Yang,
  author = {Yang, Yubin and Lin, Hui and Zhang, Yao},
  title = {Content-based 3D model retrieval: a survey},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics-Part C (Applications
	and Reviews)},
  year = {2007},
  volume = {37},
  pages = {1081-98},
  number = {Copyright 2008, The Institution of Engineering and Technology},
  abstract = {As the number of available 3D models grows, there is an increasing
	need to index and retrieve them according to their contents. This
	paper provides a survey of the up-to-date methods for content-based
	3D model retrieval. First, the new challenges encountered in 3D model
	retrieval are discussed. Then, the system framework and some key
	techniques of content-based 3D model retrieval are identified and
	explained, including canonical coordinate normalization and preprocessing,
	feature extraction, similarity match, query representation and user
	interface, and performance evaluation. In particular, similarity
	measures using semantic clues and machine learning methods, as well
	as retrieval approaches using nonshape features, are given adequate
	recognition as improvements and complements for traditional shape-matching
	techniques. Typical 3D model retrieval systems and search engines
	are also listed and compared. Finally, future research directions
	are indicated, and an extensive bibliography is provided.},
  comment = {survey},
  file = {:papers\\2007 JNL, Content-based 3D model retrieval_ a survey (Yang, TSMC).pdf:PDF},
  keywords = {content-based retrieval feature extraction learning (artificial intelligence)
	pattern matching search engines user interfaces},
  timestamp = {-}
}

@ARTICLE{2006_JNL_SURVEYtrk_Yilmaz,
  author = {Yilmaz, A. and Javed, O. and Shah, M.},
  title = {Object tracking: a survey},
  journal = {ACM Computing Surveys},
  year = {2006},
  volume = {38},
  pages = {45 pp.},
  number = {Copyright 2007, The Institution of Engineering and Technology},
  abstract = {The goal of this article is to review the state-of-the-art tracking
	methods, classify them into different categories, and identify new
	trends. Object tracking, in general, is a challenging problem. Difficulties
	in tracking objects can arise due to abrupt object motion, changing
	appearance patterns of both the object and the scene, nonrigid object
	structures, object-to-object and object-to-scene occlusions, and
	camera motion. Tracking is usually performed in the context of higher-level
	applications that require the location and/or shape of the object
	in every frame. Typically, assumptions are made to constrain the
	tracking problem in the context of a particular application. In this
	survey, we categorize the tracking methods on the basis of the object
	and motion representations used, provide detailed descriptions of
	representative methods in each category, and examine their pros and
	cons. Moreover, we discuss the important issues related to tracking
	including the use of appropriate image features, selection of motion
	models, and detection of objects.},
  comment = {TRK_survey},
  file = {:papers\\2006 JNL, Object Tracking, a Survey (SURVEY, Yilmaz, Mubarak Shah, CSUR, 335).pdf:PDF},
  keywords = {image representation motion estimation object detection tracking},
  owner = {salman},
  timestamp = {00,350}
}

@ARTICLE{2004_JNL_ContourTrackingOcclusionsMobileCameras_Yilmaz,
  author = {Yilmaz, A. and Xin Li and Shah, M.},
  title = {Contour-based object tracking with occlusion handling in video acquired
	using mobile cameras},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2004},
  volume = {26},
  pages = {1531 -1536},
  number = {11},
  month = {nov. },
  abstract = {We propose a tracking method which tracks the complete object regions,
	adapts to changing visual features, and handles occlusions. Tracking
	is achieved by evolving the contour from frame to frame by minimizing
	some energy functional evaluated in the contour vicinity defined
	by a band. Our approach has two major components related to the visual
	features and the object shape. Visual features (color, texture) are
	modeled by semiparametric models and are fused using independent
	opinion polling. Shape priors consist of shape level sets and are
	used to recover the missing object regions during occlusion. We demonstrate
	the performance of our method in real sequences with and without
	object occlusions.},
  comment = {trk_contour},
  doi = {10.1109/TPAMI.2004.96},
  file = {:papers\\2004 JNL, Contour based object tracking with occlusion handling in video (Yilmaz, Li, Mubarakshah, PAMI, 128).pdf:PDF},
  issn = {0162-8828},
  keywords = {color model;contour based object tracking;contour vicinity;missing
	object regions;mobile cameras;occlusion handling;semiparametric models;texture
	model;visual features;cameras;computer graphics;image sequences;object
	recognition;tracking;Algorithms;Animals;Artificial Intelligence;Cluster
	Analysis;Computer Graphics;Computer Simulation;Humans;Image Enhancement;Image
	Interpretation, Computer-Assisted;Information Storage and Retrieval;Models,
	Biological;Models, Statistical;Movement;Numerical Analysis, Computer-Assisted;Pattern
	Recognition, Automated;Reproducibility of Results;Sensitivity and
	Specificity;Signal Processing, Computer-Assisted;Subtraction Technique;User-Computer
	Interface;Video Recording;},
  timestamp = {00,130}
}

@ARTICLE{2003_JNL_AirborneIRtracking_Yilmaz,
  author = {Yilmaz, A. and Shafique, K. and Shah, M.},
  title = {Target tracking in airborne forward looking infrared imagery},
  journal = {Image and Vision Computing},
  year = {2003},
  volume = {21},
  pages = {623-35},
  number = {Copyright 2004, IEE},
  abstract = {We propose a robust approach for tracking targets in forward looking
	infrared (FLIR) imagery taken from an airborne moving platform. First,
	the targets are detected using fuzzy clustering, edge fusion and
	local texture energy. The position and the size of the detected targets
	are then used to initialize the tracking algorithm. For each detected
	target, intensity and local standard deviation distributions are
	computed, and tracking is performed by computing the mean-shift vector
	that minimizes the distance between the kernel distribution for the
	target in the current frame and the model. In cases when the ego-motion
	of the sensor causes the target to move more than the operational
	limits of the tracking module, we perform a multiresolution global
	motion compensation using the Gabor responses of the consecutive
	frames. The decision whether to compensate the sensor ego-motion
	is based on the distance measure computed from the likelihood of
	target and candidate distributions. To overcome the problems related
	to the changes in the target feature distributions, we automatically
	update the target model. Selection of the new target model is based
	on the same distance measure that is used for motion compensation.
	The experiments performed on the AMCOM FLIR data set show the robustness
	of the proposed method, which combines automatic model update and
	global motion compensation into one framework},
  comment = {trk_cars},
  file = {:papers\\2003 JNL, Target tracking in airborne forward looking imagery (YilmazShafiqueMubarakshah, IVC, 114).pdf:PDF},
  keywords = {edge detection feature extraction fuzzy logic image segmentation image
	sensors image texture infrared imaging motion compensation optical
	tracking pattern clustering target tracking},
  timestamp = {00,100}
}

@INPROCEEDINGS{2010_CNF_TRKsubspace_Yin,
  author = {Shimin Yin and Haan Joo Yoo and Jin Young Choi},
  title = {Enhanced Measurement Model for Subspace-Based Tracking},
  booktitle = {Pattern Recognition (ICPR), 2010 20th International Conference on},
  year = {2010},
  pages = {3492 -3495},
  month = aug.,
  abstract = {We present an efficient and robust measurement model for visual tracking.
	This approach builds on and extends work on measurement model of
	subspace representation. Subspace-based tracking algorithms have
	been introduced to visual tracking literature for a decade and show
	considerable tracking performance due to its robustness in matching.
	However, the measures used in their measurement models are not robust
	enough in cluttered backgrounds. We propose a novel measure of object
	matching referred to as WDIFS, which aims to improve the discriminability
	of matching within the subspace. Our measurement model can distinguish
	target from similar background clutters which often cause erroneous
	drift by conventional DFFS based measure. Experiments demonstrate
	the effectiveness of the proposed tracking algorithm under cluttered
	background.},
  comment = {TRK_subspace},
  doi = {10.1109/ICPR.2010.852},
  file = {:papers\\2010 CNF, Enhanced Measurement Model for Subspace-Based Tracking (Yin).pdf:PDF},
  issn = {1051-4651},
  keywords = {WDIFS measurement;background clutters;enhanced measurement model;matching
	discriminability;object matching;subspace representation;subspace-based
	tracking;visual tracking;weighted difference in feature space;image
	matching;image representation;learning (artificial intelligence);object
	detection;}
}

@TECHREPORT{2010_TECH_2Dto3D_Ying,
  author = {Ying, Chen},
  title = {Low-complexity 2D to 3D video conversion},
  institution = {Qualcomm},
  year = {2010},
  comment = {3D_stereo},
  file = {:papers\\2010 REP, Low-complexity 2D to 3D Video Conversion (Ying, Qualcomm).pdf:PDF},
  timestamp = {-}
}

@INPROCEEDINGS{2010_TRKocclusion_Youssef,
  author = {Youssef, S.M. and Hamza, M.A. and Fayed, A.F.},
  title = {Detection and tracking of multiple moving objects with occlusion
	in smart video surveillance systems},
  booktitle = {Intelligent Systems (IS), 2010 5th IEEE International Conference},
  year = {2010},
  pages = {120 -125},
  month = july,
  abstract = {Autonomous video surveillance and monitoring has a rich history. A
	new method for detecting and tracking multiple moving objects based
	on discrete wavelet transform and identifying the moving objects
	by their color and spatial information is proposed in this paper.
	Since discrete wavelet transform has a nice property that it can
	divide a frame into four different frequency bands without loss of
	the spatial information, it is adopted to solve this problem due
	to the fact that most of the fake motions in the background can be
	decomposed into the high frequency wavelet sub-band. In tracking
	multiple moving objects, many applications have problems when objects
	pass across each other. In this paper, we have developed robust routines
	for detecting and tracking multiple moving objects with occlusion.
	The proposed model has proved to be robust in various environments
	(including indoor and outdoor scenes) and different types of background
	scenes. The experimental results prove the feasibility of the proposed
	method. Experiments on real scenes show that the algorithm is effective
	for object detection and tracking.},
  comment = {TRK_occlusion},
  keywords = {autonomous video monitoring;discrete wavelet transform;frequency bands;high
	frequency wavelet subband;moving object identification;moving object
	tracking;multiple moving object detection;smart video surveillance
	system;spatial information;computer graphics;computerised monitoring;discrete
	wavelet transforms;image colour analysis;image motion analysis;object
	detection;target tracking;video surveillance;},
  timestamp = {-}
}

@ARTICLE{2008_CNF_CornerMatching_Yu,
  author = {Haiyan Yu and Cuihua Ren and Xiaolin Qiao},
  title = {A new corner matching algorithm based on gradient},
  journal = {Signal Processing, 2008. ICSP 2008. 9th International Conference
	on},
  year = {2008},
  pages = {1346-1349},
  month = {Oct.},
  abstract = {In this paper, a novel corner matching algorithm is presented. Gradient
	operator in Harris corner detector is directly used to compute the
	similarity between corners. Then initial matches are obtained. The
	algorithm is contrasted with normalized cross-correlation. Its computational
	complexity is reduced and matching speed is improved effectively
	because this method only adopts the addition and subtraction operations.
	Experiments on several real images test the matching speed, the matching
	precision and matching rate of the algorithm. The results demonstrate
	that the algorithm not only have higher speed but also get higher
	matching precision and correct matching rate. Even though the stereo
	image pairs have brightness differences, it still performs rather
	well.},
  doi = {10.1109/ICOSP.2008.4697381},
  keywords = {computational complexity, image matching, stereo image processingHarris
	corner detector, computational complexity, corner matching algorithm,
	gradient operator, image matching, normalized cross-correlation,
	stereo image processing}
}

@INPROCEEDINGS{2009_CNF_TRKocclusion_Yu,
  author = {Jie Yu and Farin, D. and Loos, H.S.},
  title = {Multi-cue Based Visual Tracking in Clutter Scenes with Occlusions},
  booktitle = {Advanced Video and Signal Based Surveillance, 2009. AVSS '09. Sixth
	IEEE International Conference on},
  year = {2009},
  pages = {158 -163},
  month = sept.,
  abstract = {Object tracking is important for video analysis applications. However,
	tracking through occlusions is a difficult task due to significant
	appearance changes of the objects. Approaches based on either global
	features or one kind of local features can not solve the problem
	completely. In this paper, a multi-cue based tracking approach is
	introduced. It combines a corner tracking with a color and a shape
	model to resolve the object tracking problem through occlusions for
	most scenes (indoor and outdoor).To obtain an objective evaluation
	of the proposed method, a set of detection and tracking measures
	are used to perform a quantitative analysis based on a large sequence
	dataset with ground-truth annotation. The experimental results show
	that the proposed approach works robustly under varying conditions.},
  comment = {TRK_occlusion},
  doi = {10.1109/AVSS.2009.56},
  file = {:papers\\2009 CNF, Multi-cue Based Visual Tracking in Clutter Scenes with Occlusions (Yu).pdf:PDF},
  keywords = {clutter scenes;corner tracking;object detection;object tracking;video
	analysis;visual tracking;hidden feature removal;object detection;target
	tracking;video signal processing;},
  timestamp = {-}
}

@ARTICLE{2004_JNL_SegmentationPartialGroupConstraints_YuShi,
  author = {Yu, S.X. and Jianbo Shi},
  title = {Segmentation given partial grouping constraints},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2004},
  volume = {26},
  pages = {173 -183},
  number = {2},
  month = {feb. },
  abstract = {We consider data clustering problems where partial grouping is known
	a priori. We formulate such biased grouping problems as a constrained
	optimization problem, where structural properties of the data define
	the goodness of a grouping and partial grouping cues define the feasibility
	of a grouping. We enforce grouping smoothness and fairness on labeled
	data points so that sparse partial grouping information can be effectively
	propagated to the unlabeled data. Considering the normalized cuts
	criterion in particular, our formulation leads to a constrained eigenvalue
	problem. By generalizing the Rayleigh-Ritz theorem to projected matrices,
	we find the global optimum in the relaxed continuous domain by eigendecomposition,
	from which a near-global optimum to the discrete labeling problem
	can be obtained effectively. We apply our method to real image segmentation
	problems, where partial grouping priors can often be derived based
	on a crude spatial attentional map that binds places with common
	salient features or focuses on expected object locations. We demonstrate
	not only that it is possible to integrate both image structures and
	priors in a single grouping process, but also that objects can be
	segregated from the background without specific object knowledge.},
  comment = {seg},
  doi = {10.1109/TPAMI.2004.1262179},
  file = {:papers\\2004 JNL, Segmentation given partial grouping constraints (Yu, Shi, PAMI, 80).pdf:PDF},
  issn = {0162-8828},
  keywords = {Rayleigh-Ritz theorem;constrained eigenvalue problem;constrained optimization
	problem;crude spatial attentional map;data clustering problems;discrete
	labeling problem;eigendecomposition;image segmentation problems;image
	structures;normalized cuts criterion;partial grouping constraints;priors;projected
	matrices;relaxed continuous domain;structural properties;Rayleigh-Ritz
	methods;eigenvalues and eigenfunctions;image segmentation;matrix
	algebra;optimisation;pattern clustering;Algorithms;Artificial Intelligence;Cluster
	Analysis;Computer Graphics;Image Enhancement;Image Interpretation,
	Computer-Assisted;Imaging, Three-Dimensional;Information Storage
	and Retrieval;Numerical Analysis, Computer-Assisted;Pattern Recognition,
	Automated;Reproducibility of Results;Sensitivity and Specificity;Signal
	Processing, Computer-Assisted;Subtraction Technique;User-Computer
	Interface;},
  timestamp = {00,080}
}

@INPROCEEDINGS{2009_CNF_MTTcrowd_Yuan,
  author = {Yuan, Li and Chang, Huang and Nevatia, R.},
  title = {Learning to associate: HybridBoosted multi-target tracker for crowded
	scene},
  booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference
	on},
  year = {2009},
  abstract = {We propose a learning-based hierarchical approach of multi-target
	tracking from a single camera by progressively associating detection
	responses into longer and longer track fragments (tracklets) and
	finally the desired target trajectories. To define tracklet affinity
	for association, most previous work relies on heuristically selected
	parametric models; while our approach is able to automatically select
	among various features and corresponding non-parametric models, and
	combine them to maximize the discriminative power on training data
	by virtue of a HybridBoost algorithm. A hybrid loss function is used
	in this algorithm because the association of tracklet is formulated
	as a joint problem of ranking and classification: the ranking part
	aims to rank correct tracklet associations higher than other alternatives;
	the classification part is responsible to reject wrong associations
	when no further association should be done. Experiments are carried
	out by tracking pedestrians in challenging datasets. We compare our
	approach with state-of-the-art algorithms to show its improvement
	in terms of tracking accuracy.},
  comment = {trk_crowd},
  file = {:papers\\2009 CNF, Learning to associate_ HybridBoosted multi-target tracker for crowded scene (Li, Huang, R. Nevatia, CVPR).pdf:PDF},
  keywords = {feature extraction image classification image fusion learning (artificial
	intelligence) object detection target tracking traffic engineering
	computing HybridBoosted multitarget tracker crowded scene data association
	feature selection hybrid loss function learning-based hierarchical
	approach nonparametric model pedestrian tracking single camera},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{1989_CNF_DeformableTemplates_Yuille,
  author = {Yuille, A.L. and Cohen, D.S. and Hallinan, P.W.},
  title = {Feature extraction from faces using deformable templates},
  booktitle = {Computer Vision and Pattern Recognition, 1989. Proceedings CVPR '89.,
	IEEE Computer Society Conference on},
  year = {1989},
  pages = {104 -109},
  month = {4-8},
  abstract = {A method for detecting and describing the features of faces using
	deformable templates is described. The feature of interest, an eye
	for example, is described by a parameterized template. An energy
	function is defined which links edges, peaks, and valleys in the
	image intensity to corresponding properties of the template. The
	template then interacts dynamically with the image, by altering its
	parameter values to minimize the energy function, thereby deforming
	itself to find the best fit. The final parameter values can be used
	as descriptors for the features. This method is demonstrated by showing
	deformable templates detecting eyes and mouths in real images},
  comment = {TRK_contour},
  doi = {10.1109/CVPR.1989.37836},
  file = {:papers\\1989 CNF, Feature extraction from faces using deformable templates (Yuille).pdf:PDF},
  keywords = {deformable templates;energy function;feature extraction;image intensity;pattern
	recognition;picture processing;pattern recognition;picture processing;},
  timestamp = {01,400}
}

@ARTICLE{2002_JNL_SigmaTrees_Barnes,
  author = {Zaidan, M.Y. and Barnes, C.F. and Wicker, S.B.},
  title = {Use of $\sigma$-trees as constellations in trellis-coded modulation},
  journal = {Information Theory, IEEE Transactions on},
  year = {2002},
  volume = {43},
  pages = {2005--2012},
  number = {6},
  comment = {VQ_RVQ},
  file = {:papers\\1997 JNL, Use of Sigma Trees as Constellations in Trellis-Coded Modulation (Barnes).pdf:PDF},
  issn = {0018-9448},
  publisher = {IEEE},
  timestamp = {-}
}

@INPROCEEDINGS{2001_CNF_EventBasedAnalysisVideo_Manor,
  author = {Zelnik-Manor, L. and Irani, M.},
  title = {Event-based analysis of video},
  booktitle = {Proceedings of the 2001 IEEE Computer Society Conference on Computer
	Vision and Pattern Recognition. CVPR 2001, 8-14 Dec. 2001},
  year = {2001},
  abstract = {Dynamic events can be regarded as long-term temporal objects, which
	are characterized by spatio-temporal features at multiple temporal
	scales. Based on this, we design a simple statistical distance measure
	between video sequences (possibly of different lengths) based on
	their behavioral content. This measure is non-parametric and can
	thus handle a wide range of dynamic events. We use this measure for
	isolating and clustering events within long continuous video sequences.
	This is done without prior knowledge of the types of events, their
	models, or their temporal extent. An outcome of such a clustering
	process is a temporal segmentation of long video sequences into event-consistent
	sub-sequences, and their grouping into event-consistent clusters.
	Our event representation and associated distance measure can also
	be used for event-based indexing into long video sequences, even
	when only one short example-clip is available. However, when multiple
	example-clips of the same event are available (either as a result
	of the clustering process, or given manually), these can be used
	to refine the event representation, the associated distance measure,
	and accordingly the quality of the detection and clustering process},
  comment = {analysis},
  file = {:papers\\2001 CNF, Event-based analysis of video (Irani, CVPR, 175).pdf:PDF},
  keywords = {image segmentation image sequences indexing pattern clustering video
	signal processing},
  owner = {salman},
  timestamp = {00,200}
}

@ARTICLE{5784945,
  author = {Jianchao Zeng and Yue Wang and Freedman, M. and Mun, S.K.},
  title = {Finger tracking for breast palpation quantification using color image
	features},
  journal = {Optical Engineering},
  year = {1997},
  volume = { 36},
  pages = {3455 - 61},
  number = { 12},
  note = {finger tracking;breast palpation quantification;color image features;vision-based
	finger motion tracking approach;quantitative data;cancer detection;finger
	positions;search pattern;coverage area;prototype palpation training
	system;color markers;feature extraction methods;real-time performance;color
	space transform;RGB values;clinical environment;color attributes;three-finger
	pattern topology;extracted features;3D positions;color marked fingers;stereo
	vision principle;},
  abstract = {A vision-based finger motion tracking approach is presented to gather
	quantitative data about breast palpation for cancer detection, such
	as finger positions, search pattern and coverage area, and this approach
	is used to develop a prototype palpation training system. Special
	color markers are used as features of interest because in breast
	palpation the background of the image is the breast itself which
	is similar to the fingers in color. This color similarity can hinder
	the ability or efficiency of other feature extraction methods if
	real-time performance is required. To simplify the feature extraction
	process, color space transform is utilized instead of directly using
	the original RGB values of the image. Although the clinical environment
	will be well illuminated, normalization of color attributes is applied
	to compensate for minor changes in illumination. A neighbor search
	is employed to ensure real-time performance, and a three-finger pattern
	topology is checked for the extracted features to avoid any possible
	false features. After detecting the features in the images, 3-D positions
	of the color marked fingers are calculated using the stereo vision
	principle. Experimental results with the prototype training system
	are given to show the performance and effectiveness of the proposed
	approach. This approach is expected to significantly improve the
	training quality of breast palpation, thus increasing the detection
	rate and accuracy of breast cancer},
  address = {USA},
  comment = {trk_color},
  copyright = {Copyright 1997, IEE},
  file = {:papers\\1997 JNL, Finger tracking for breast palpation quantification using color image features (Zeng, Wang, Freedman, Mun).pdf:PDF},
  issn = {0091-3286},
  keywords = {biomedical measurement;feature extraction;image colour analysis;medical
	image processing;optical information processing;real-time systems;stereo
	image processing;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1117/1.601585}
}

@ARTICLE{zha2002spectral,
  author = {Zha, H. and He, X. and Ding, C. and Gu, M. and Simon, H.},
  title = {Spectral relaxation for k-means clustering},
  journal = {Advances in Neural Information Processing Systems},
  year = {2002},
  volume = {2},
  pages = {1057--1064},
  comment = {TRK_subspace},
  file = {:papers\\2002 JNL, Spectral relaxation for k-means clustering (Zha).pdf:PDF},
  issn = {1049-5258},
  publisher = {Citeseer},
  timestamp = {00,240}
}

@ARTICLE{2008_JNL_SurveyCrowdAnalysis,
  author = {Zhan, Beibei and Monekosso, Dorothy N. and Remagnino, Paolo and Velastin,
	Sergio A. and Xu, Li-Qun},
  title = {Crowd analysis: A survey},
  journal = {Machine Vision and Applications},
  year = {2008},
  volume = {19},
  pages = {345-357},
  abstract = {In the year 1999 the world population reached 6 billion, doubling
	the previous census estimate of 1960. Recently, the United States
	Census Bureau issued a revised forecast for world population showing
	a projected growth to 9.4 billion by 2050 (US Census Bureau, http://www.census.gov/ipc/www/worldpop.html
	). Different research disci- plines have studied the crowd phenomenon
	and its dynamics from a social, psychological and computational standpoint
	respectively. This paper presents a survey on crowd analysis methods
	employed in computer vision research and discusses perspectives from
	other research disciplines and how they can contribute to the computer
	vision approach. Compilation and indexing terms, Copyright 2009 Elsevier
	Inc. 20084111635302 Analysis methods Census bureau Crowd dynamics
	Crowd simulations Crowd studies Socio-dynamics United States US Census
	Bureau Vision research World populations},
  comment = {survey},
  file = {:papers\\2008 JNL, Crowd analysis_ a survey (SURVEY, Xu UK, MVA, 11).pdf:PDF},
  keywords = {Surveys Artificial intelligence Computer vision Image processing},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{2010_JNL_ManifoldLearning_Zhang,
  author = {Junping Zhang and Hua Huang and Jue Wang},
  title = {Manifold Learning for Visualizing and Analyzing High-Dimensional
	Data},
  journal = {Intelligent Systems, IEEE},
  year = {2010},
  volume = {25},
  pages = {54 -61},
  number = {4},
  month = july-aug.,
  abstract = {Assuming that high-dimensional data are generated from intrinsic variables
	with lower dimensions, several key manifold-learning algorithms can
	help effectively analyze and visualize such data.},
  doi = {10.1109/MIS.2010.8},
  file = {:papers\\2010 JNL, Manifold Learning for Visualizing and Analyzing High-Dimensional Data (Zhang).pdf:PDF},
  issn = {1541-1672},
  keywords = {high dimensional data analysis;high dimensional data visualization;manifold
	learning;data analysis;data visualisation;learning (artificial intelligence);}
}

@ARTICLE{20094912532081,
  author = {Zhang, Qiu-Yu and Zhang, Mo-Yi and Hu, Jian-Qiang},
  title = {Hand gesture contour tracking based on skin color probability and
	state estimation model},
  journal = {Journal of Multimedia},
  year = {2009},
  volume = {4},
  pages = {349 - 355},
  number = {6},
  note = {Hand gesture;Skin color;State estimation method;State estimation
	methods;},
  abstract = {considering the deficiency of accurate hand gesture contour inaccessible
	and inefficiency in complex dynamic background in existing methods
	of hand gesture tracking, a two dimensional skin color probability
	forecast method is proposed. Based on this, a hand gesture segmentation
	method of multi-mode and a hand gesture tracking method of state
	estimation are extended. When hand gesture is segmented, to locate
	the accurate hand gesture position, this paper combines the Skin
	Color Probability distribution with the statistical motion information
	of image blocking. Then the hand region is initiated by the region
	growth method and the hand gesture segmentation is realized. When
	hand gesture is tracked, the pixel's state model is built to estimate
	the state of pixels after watershed computation. Then the current
	blocking frame is adaptive threshold segmented and the hand gesture
	tracking is realized. Experiments show that this method has a strong
	anti-noise ability in complex background. In addition, it has a better
	application effect in segment and tracking the hand gesture contour
	accurately in a real-time way. &copy; 2009 ACADEMY PUBLISHER.},
  address = {P.O.Box 40, FIN-90571, OULU, Finland},
  comment = {trk_color},
  copyright = {Compilation and indexing terms, Copyright 2009 Elsevier Inc.},
  file = {:papers\\2009 JNL, Hand Gesture Contour Tracking Based on Skin (Zhang, Zhang, Hu).pdf:PDF},
  issn = {17962048},
  key = {Probability distributions},
  keywords = {Color;Estimation;Pixels;Probability density function;Skin;State estimation;Two
	dimensional;},
  language = {English},
  review = {salman: i believe best paper},
  timestamp = {-},
  url = {http://dx.doi.org/10.4304/jmm.4.6.349-355}
}

@TECHREPORT{2010_TECH_2Dto3D_Zhang,
  author = {Zhang, Rong},
  title = {Low-complexity 2D to 3D video conversion},
  institution = {Qualcomm},
  year = {2010},
  comment = {3D_stereo},
  file = {:papers\\2010 TECH, Low-complexity 2D to 3D video conversion (Zhang).docx:Word},
  timestamp = {-}
}

@INPROCEEDINGS{2010_CNF_TRKsubs_Zhang,
  author = {Xiaoqin Zhang and Qiuyun Cheng and Xingchu Shi and Weiming Hu and
	Zhenjie Hong},
  title = {Combining the spatial and temporal eigen-space for visual tracking},
  booktitle = {Computer Application and System Modeling (ICCASM), 2010 International
	Conference on},
  year = {2010},
  volume = {12},
  pages = {V12-152 -V12-155},
  month = oct.,
  abstract = {Visual tracking is an important research topic in computer vision
	community. Most subspace based tracking algorithms focus on the time
	correlation between the image observations of the object, but the
	spatial layout information of the object is ignored. This paper proposes
	a robust visual tracking algorithm which effectively combines the
	spatial and temporal eigen-space of the object. In order to captures
	the variations of object appearance, an incremental updating strategy
	is developed to update the eigen-space and mean of the object. Experimental
	results demonstrate that, compared with the state-of-the-art subspace
	based tracking algorithms, the proposed tracking algorithm is more
	robust and effective.},
  comment = {TRK_subspace},
  doi = {10.1109/ICCASM.2010.5622125},
  file = {:papers\\2010 CNF, Combining the spatial and temporal eigen-space for visual tracking (Zhang).pdf:PDF},
  keywords = {computer vision;image observations;object tracking;spatial eigen-space;temporal
	eigen-space;time correlation algorithms;visual tracking;computer
	vision;correlation methods;object tracking;spatiotemporal phenomena;}
}

@ARTICLE{2000_JNL_CameraCalibration_Zhang,
  author = {Zhang, Z.},
  title = {A flexible new technique for camera calibration},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = { 1330 - 1334},
  number = {11},
  month = {nov},
  abstract = { We propose a flexible technique to easily calibrate a camera. It
	only requires the camera to observe a planar pattern shown at a few
	(at least two) different orientations. Either the camera or the planar
	pattern can be freely moved. The motion need not be known. Radial
	lens distortion is modeled. The proposed procedure consists of a
	closed-form solution, followed by a nonlinear refinement based on
	the maximum likelihood criterion. Both computer simulation and real
	data have been used to test the proposed technique and very good
	results have been obtained. Compared with classical techniques which
	use expensive equipment such as two or three orthogonal planes, the
	proposed technique is easy to use and flexible. It advances 3D computer
	vision one more step from laboratory environments to real world use.},
  comment = {3D_camera},
  doi = {10.1109/34.888718},
  file = {:papers\\2000 JNL, A flexible new technique for camera calibration (Zhang).pdf:PDF},
  issn = {0162-8828},
  keywords = { 3D computer vision; camera calibration; flexible technique; maximum
	likelihood criterion; planar pattern; radial lens distortion; calibration;
	computer vision; geometry; image sensors; matrix algebra; maximum
	likelihood estimation; optimisation;},
  timestamp = {02,500}
}

@INPROCEEDINGS{1999_CNF_CameraCalibration_Zhang,
  author = {Zhengyou Zhang},
  title = {Flexible camera calibration by viewing a plane from unknown orientations},
  booktitle = {Computer Vision, 1999. The Proceedings of the Seventh IEEE International
	Conference on},
  year = {1999},
  volume = {1},
  pages = {666 -673 vol.1},
  abstract = {Proposes a flexible new technique to easily calibrate a camera. It
	only requires the camera to observe a planar pattern shown at a few
	(at least two) different orientations. Either the camera or the planar
	pattern can be freely moved. The motion need not be known. Radial
	lens distortion is modeled. The proposed procedure consists of a
	closed-form solution followed by a nonlinear refinement based on
	the maximum likelihood criterion. Both computer simulation and real
	data have been used to test the proposed technique, and very good
	results have been obtained. Compared with classical techniques which
	use expensive equipment, such as two or three orthogonal planes,
	the proposed technique is easy to use and flexible. It advances 3D
	computer vision one step from laboratory environments to real-world
	use. The corresponding software is available from the author's Web
	page ( lt;http://research.microsoft.com/~zhang gt;)},
  comment = {3D_camera},
  doi = {10.1109/ICCV.1999.791289},
  file = {:papers\\1999 CNF, Flexible camera calibration by viewing a plane from unknown orientations (Zhang).pdf:PDF},
  keywords = {3D computer vision;closed-form solution;computer simulation;flexible
	camera calibration;intrinsic parameters;maximum likelihood criterion;model
	acquisition;motion analysis;nonlinear refinement;planar pattern;plane
	views;public-domain software;radial lens distortion;unknown orientations;calibration;cameras;computer
	vision;digital simulation;image processing equipment;maximum likelihood
	estimation;photographic lenses;public domain software;},
  timestamp = {00,800}
}

@ARTICLE{10448450,
  author = {Qi Zhao and Hai Tao},
  title = {A motion observable representation using color correlogram and its
	applications to tracking},
  journal = {Computer Vision and Image Understanding},
  year = {2009},
  volume = { 113},
  pages = {273 - 90},
  number = { 2},
  note = {motion observable representation;object tracking representation;motion
	observability analysis;simplified color correlogram representation;object
	orientation;estimation algorithm;mean shift algorithm;translation-rotation
	joint domain;object motion estimation;},
  abstract = {This paper presents a special form of color correlogram as representation
	for object tracking and carries out a motion observability analysis
	to obtain the optimal correlogram in a kernel based tracking framework.
	Compared with the color histogram, where the position information
	of each pixel is ignored, a simplified color correlogram (SCC) representation
	encodes the spatial information explicitly and enables an estimation
	algorithm to recover the object orientation. In this paper, based
	on the SCC representation, the mean shift algorithm is developed
	in a translation-rotation joint domain to track the positions and
	orientations of objects. The ability of the SCC in detecting and
	estimating object motion is analyzed and a principled way to obtain
	the optimal SCC as object representation is proposed to ensure reliable
	tracking. Extensive experimental results demonstrate SCC as a viable
	object representation for tracking.[All rights reserved Elsevier].},
  address = {USA},
  comment = {trk_color},
  copyright = {Copyright 2009, The Institution of Engineering and Technology},
  file = {:papers\\2009 JNL, A motion observable representation using color correlogram and its applications to tracking (Zhao, Tao).pdf:PDF},
  issn = {1077-3142},
  keywords = {image colour analysis;image representation;motion estimation;},
  language = {English},
  timestamp = {-},
  url = {http://dx.doi.org/10.1016/j.cviu.2008.10.007}
}

@PHDTHESIS{2003_THE_TRK_shape_Zhao,
  author = {Tao Zhao},
  title = {Model-based Segmentation and Tracking of Multiple Humans in Complex
	Situations},
  school = {University of Southern California},
  year = {2003},
  comment = {TRK_shape},
  file = {:papers\\2003 THE, Model-based Segmentation and Tracking of Multiple Humans in Complex Situations (Zhao).pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{2004_CNF_TrackingPeople_Zhao,
  author = {Zhao, Tao and Nevatia, R.},
  title = {Tracking multiple humans in crowded environment},
  booktitle = {Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings
	of the 2004 IEEE Computer Society Conference on},
  year = {2004},
  volume = {2},
  pages = {II-406 - II-413 Vol.2},
  month = {june-2 july},
  abstract = { Tracking of humans in dynamic scenes has been an important topic
	of research. Most techniques, however, are limited to situations
	where humans appear isolated and occlusion is small. Typical methods
	rely on appearance models that must be acquired when the humans enter
	the scene and are not occluded. We present a method that can track
	humans in crowded environments, with significant and persistent occlusion
	by making use of human shape models in addition to camera models,
	the assumption that humans walk on a plane and acquired appearance
	models. Experimental results and a quantitative evaluation are included.},
  comment = {trk_people},
  doi = {10.1109/CVPR.2004.1315192},
  file = {:papers\\2004 CNF, Tracking Multiple Humans in Crowded Environment (Zhao, Nevatia).pdf:PDF},
  issn = {1063-6919 },
  keywords = { Bayesian inference; camera models; crowded environment; human shape
	models; multiple human tracking; occlusion; video sequences; Bayes
	methods; hidden feature removal; image sequences; object detection;
	target tracking; video cameras;},
  timestamp = {00,200}
}

@ARTICLE{2004_JNL_TRK_shape_Zhao,
  author = {Tao Zhao and Nevatia, R.},
  title = {Tracking multiple humans in complex situations},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2004},
  volume = {26},
  pages = {1208 -1221},
  number = {9},
  month = {sept. },
  abstract = {Tracking multiple humans in complex situations is challenging. The
	difficulties are tackled with appropriate knowledge in the form of
	various models in our approach. Human motion is decomposed into its
	global motion and limb motion. In the first part, we show how multiple
	human objects are segmented and their global motions are tracked
	in 3D using ellipsoid human shape models. Experiments show that it
	successfully applies to the cases where a small number of people
	move together, have occlusion, and cast shadow or reflection. In
	the second part, we estimate the modes (e.g., walking, running, standing)
	of the locomotion and 3D body postures by making inference in a prior
	locomotion model. Camera model and ground plane assumptions provide
	geometric constraints in both parts. Robust results are shown on
	some difficult sequences.},
  comment = {TRK_shape},
  doi = {10.1109/TPAMI.2004.73},
  file = {:papers\\2004 JNL, Tracking multiple humans in complex situations (Zhao, Nevatia, PAMI, 325).pdf:PDF},
  issn = {0162-8828},
  keywords = {3D body postures;camera model;ellipsoid human shape models;geometric
	constraints;human limb motion;image segmentation;locomotion modes
	estimation;multiple human object tracking;object recognition;cameras;image
	segmentation;motion estimation;object detection;object recognition;tracking;Algorithms;Artificial
	Intelligence;Computer Simulation;Humans;Image Enhancement;Image Interpretation,
	Computer-Assisted;Imaging, Three-Dimensional;Information Storage
	and Retrieval;Locomotion;Models, Biological;Models, Statistical;Pattern
	Recognition, Automated;Photography;Reproducibility of Results;Sensitivity
	and Specificity;Subtraction Technique;Video Recording;},
  timestamp = {00,325}
}

@INPROCEEDINGS{2003_CNF_HumanSegmentation_Zhao,
  author = {Zhao, Tao and Nevatia, R.},
  title = {Bayesian human segmentation in crowded situations},
  booktitle = {Computer Vision and Pattern Recognition, 2003. Proceedings. 2003
	IEEE Computer Society Conference on},
  year = {2003},
  volume = {2},
  pages = { II - 459-66 vol.2},
  month = {june},
  abstract = {The problem of segmenting individual humans in crowded situations
	from stationary video camera sequences is exacerbated by object inter-occlusion.
	We pose this problem as a "model-based segmentation" problem in which
	human shape models are used to interpret the foreground in a Bayesian
	framework. The solution is obtained by using an efficient Markov
	chain Monte Carlo (MCMC) method that uses domain knowledge as proposal
	probabilities. Knowledge of various aspects including human shape,
	human height, camera model, and image cues including human head candidates,
	foreground/background separation are integrated in one theoretically
	sound framework. We show promising results and evaluations on some
	challenging data.},
  comment = {det_pedestrian},
  doi = {10.1109/CVPR.2003.1211503},
  file = {:papers\\2003 CNF, Bayesian human segmentation in crowded situations (Zhao, Nevatia, CVPR, 98).pdf:PDF},
  issn = {1063-6919 },
  keywords = { Bayesian framework; Bayesian human segmentation; MCMC method; Markov
	chain Monte Carlo; background separation; camera model; crowded situation;
	domain knowledge; foreground interpretation; foreground separation;
	human head candidate; human height; human shape; human tracking;
	image cue; model-based segmentation; object interocclusion; stationary
	video camera sequence; Bayes methods; Markov processes; Monte Carlo
	methods; edge detection; image segmentation; image sequences; object
	detection;},
  timestamp = {00,100}
}

@ARTICLE{2008_JNL_HumanSegTracking_Zhao,
  author = {Tao Zhao and Nevatia, R. and Bo Wu},
  title = {Segmentation and Tracking of Multiple Humans in Crowded Environments},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2008},
  volume = {30},
  pages = {1198 -1211},
  number = {7},
  month = {july },
  abstract = {Segmentation and tracking of multiple humans in crowded situations
	is made difficult by interobject occlusion. We propose a model-based
	approach to interpret the image observations by multiple partially
	occluded human hypotheses in a Bayesian framework. We define a joint
	image likelihood for multiple humans based on the appearance of the
	humans, the visibility of the body obtained by occlusion reasoning,
	and foreground/background separation. The optimal solution is obtained
	by using an efficient sampling method, data-driven Markov chain Monte
	Carlo (DDMCMC), which uses image observations for proposal probabilities.
	Knowledge of various aspects, including human shape, camera model,
	and image cues, are integrated in one theoretically sound framework.
	We present experimental results and quantitative evaluation, demonstrating
	that the resulting approach is effective for very challenging data.},
  comment = {trk_people},
  doi = {10.1109/TPAMI.2007.70770},
  file = {:papers\\2008 JNL, Segmentation and Tracking of Multiple Humans in Crowded Environments (Zhao, Nevatia, Wu).pdf:PDF},
  issn = {0162-8828},
  keywords = {3D human shape model;Bayesian MAP estimation problem;camera model;crowded
	environment;data-driven Markov chain Monte Carlo;foreground/background
	separation;human detection;human shape;image cues;image observation;joint
	image likelihood;multiple human segmentation;multiple human tracking;multiple
	partially occluded human hypotheses;occlusion reasoning;proposal
	probability;sampling method;Bayes methods;Markov processes;Monte
	Carlo methods;hidden feature removal;image sampling;image segmentation;inference
	mechanisms;maximum likelihood estimation;object detection;probability;tracking;traffic
	engineering computing;},
  timestamp = {00,024}
}

@INPROCEEDINGS{2008_CNF_CrowdMRF_Zhi,
  author = {Zhi, Zhong and Ning, Ding and Xinyu, Wu and Yangsheng, Xu},
  title = {Crowd surveillance using Markov Random Fields},
  booktitle = {Automation and Logistics, 2008. ICAL 2008. IEEE International Conference
	on},
  year = {2008},
  abstract = {Video surveillance in crowd is challenging for public security. This
	paper focuses on the detection of human abnormal behaviors in crowd.
	The issue is crucial in some special localities and has been less
	studied. To achieve this goal, this paper defines a crowd energy
	based on Markov Random Fields. By using wavelet analysis of the energy
	curves, the crowd status of the scene is detected. After testing
	the method in the actual environment in a metro surveillance system,
	we have obtained a result which shows that the method can be used
	to deal with crowd modeling and real-time surveillance satisfactorily.},
  comment = {trk_crowd},
  file = {:papers\\2008 CNF, Crowd surveillance using Markov Random Fields (Zhong, Ding, CAL).pdf:PDF},
  keywords = {Markov processes real-time systems security video surveillance wavelet
	transforms Markov random fields crowd modeling crowd surveillance
	energy curves human abnormal behaviors metro surveillance system
	public security real-time surveillance wavelet analysis},
  owner = {salman},
  timestamp = {-}
}

@INPROCEEDINGS{2003_CNF_SegmentKalmanFilter_Zhong,
  author = {Zhong, Jing and Sclaroff, S.},
  title = {Segmenting foreground objects from a dynamic textured background
	via a robust Kalman filter},
  booktitle = {Computer Vision, 2003. Proceedings. Ninth IEEE International Conference
	on DOI - 10.1109/ICCV.2003.1238312},
  year = {2003},
  pages = {44--50 vol.1},
  abstract = {The algorithm presented aims to segment the foreground objects in
	video (e.g., people) given time-varying, textured backgrounds. Examples
	of time-varying backgrounds include waves on water, clouds moving,
	trees waving in the wind, automobile traffic, moving crowds, escalators,
	etc. We have developed a novel foreground-background segmentation
	algorithm that explicitly accounts for the nonstationary nature and
	clutter-like appearance of many dynamic textures. The dynamic texture
	is modeled by an autoregressive moving average model (ARMA). A robust
	Kalman filter algorithm iteratively estimates the intrinsic appearance
	of the dynamic texture, as well as the regions of the foreground
	objects. Preliminary experiments with this method have demonstrated
	promising results.},
  comment = {seg},
  file = {:papers\\2003 CNF, Segmenting Foreground Objects from a Dynamic Textured Background via a robust Kalman Filter (Zhong, Sclaroff, ICCV, 126).pdf:PDF},
  keywords = {Kalman filters, algorithm theory, autoregressive moving average processes,
	image segmentation, iterative methods, object recognition, ARMA model,
	automobile traffic, autoregressive moving average model, dynamic
	textured background, escalators, foreground objects segmentation,
	foreground-background segmentation algorithm, robust Kalman filter,
	time-varying background, video object},
  owner = {salman},
  timestamp = {00,125}
}

@ARTICLE{2000_JNL_TRK_region_Zhong,
  author = {Yu Zhong and Jain, A.K. and Dubuisson-Jolly, M.-P.},
  title = {Object tracking using deformable templates},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {544 -549},
  number = {5},
  month = may,
  abstract = {We propose a method for object tracking using prototype-based deformable
	template models. To track an object in an image sequence, we use
	a criterion which combines two terms: the frame-to-frame deviations
	of the object shape and the fidelity of the modeled shape to the
	input image. The deformable template model utilizes the prior shape
	information which is extracted from the previous frames along with
	a systematic shape deformation scheme to model the object shape in
	a new frame. The following image information is used in the tracking
	process: 1) edge and gradient information: the object boundary consists
	of pixels with large image gradient, 2) region consistency: the same
	object region possesses consistent color and texture throughout the
	sequence, and 3) interframe motion: the boundary of a moving object
	is characterized by large interframe motion. The tracking proceeds
	by optimizing an objective function which combines both the shape
	deformation and the fidelity of the modeled shape to the current
	image (in terms of gradient, texture, and interframe motion). The
	inherent structure in the deformable template, together with region,
	motion, and image gradient cues, makes the proposed algorithm relatively
	insensitive to the adverse effects of weak image features and moderate
	amounts of occlusion},
  comment = {TRK_region},
  doi = {10.1109/34.857008},
  file = {:papers\\2000 JNL, Object tracking using deformable templates (Zhong).pdf:PDF},
  issn = {0162-8828},
  keywords = {deformable templates;fidelity;frame-to-frame deviations;image gradient;interframe
	motion;large interframe motion;object boundary;object shape;object
	tracking;region consistency;computer vision;image motion analysis;image
	sequences;image texture;},
  timestamp = {00,125}
}

@CONFERENCE{zhou2005real,
  author = {Zhou, J. and Hoang, J.},
  title = {Real time robust human detection and tracking system},
  booktitle = {Computer Vision and Pattern Recognition-Workshops, 2005. CVPR Workshops.
	IEEE Computer Society Conference on},
  year = {2005},
  pages = {149},
  organization = {IEEE},
  comment = {TRK_codebook},
  file = {:papers\\2005 CNF, Real time robust human detection and tracking system (Zhou).pdf:PDF},
  isbn = {0769523722},
  issn = {1063-6919},
  timestamp = {00,050}
}

@CONFERENCE{2001_CNF_combinedTRK_Zhou,
  author = {Zhou, Q. and Aggarwal, J.K.},
  title = {Tracking and classifying moving objects from video},
  booktitle = {Proceedings of IEEE Workshop on Performance Evaluation of Tracking
	and Surveillance},
  year = {2001},
  organization = {Hawaii, USA},
  comment = {TRK_subspace},
  file = {:papers\\2001 CNF, Tracking and Classifying Moving Objects from Video (Zhou).pdf:PDF},
  timestamp = {00,075}
}

@INPROCEEDINGS{2003_CNF_AdaptiveTrackingParticleFilters_Zhou,
  author = {Zhou, Shaohua and Chellappa, R. and Moghaddam, B.},
  title = {Adaptive visual tracking and recognition using particle filters},
  booktitle = {2003 IEEE International Conference on Multimedia and Expo, 6-9 July
	2003},
  year = {2003},
  volume = {vol.2},
  abstract = {This paper presents an improved method for simultaneous tracking and
	recognition of human faces from video, where a time series model
	is used to resolve the uncertainties in tracking and recognition.
	The improvements mainly arise from three aspects: (i) modeling the
	inter-frame appearance changes within the video sequence using an
	adaptive appearance model and an adaptive-velocity motion model;
	(ii) modeling the appearance changes between the video frames and
	gallery images by constructing intra- and extra-personal spaces;
	and (iii) utilization of the fact that the gallery images are in
	frontal views. By embedding them in a particle filter, we are able
	to achieve a stabilized tracker and an accurate recognizer when confronted
	by pose and illumination variations},
  comment = {trk_faces},
  file = {:papers\\2003 CNF, Adaptive visual tracking and recognition using particle filters (Zhou, Chellappa, Moghaddam, ICME, 14).pdf:PDF},
  keywords = {filtering theory image recognition image sequences video signal processing},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{2010_JNL_TRK_Zhu,
  author = {Zhu, J. and Yuanwei Lao and Zheng, Y.F.},
  title = {Object Tracking in Structured Environments for Video Surveillance
	Applications},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  year = {2010},
  volume = {20},
  pages = {223 -235},
  number = {2},
  month = feb.,
  abstract = {We present a novel tracking method for effectively tracking objects
	in structured environments. The tracking method finds applications
	in security surveillance, traffic monitoring, etc. In these applications,
	the movements of objects are constrained by structured environments.
	Therefore, the relationship between objects and environments can
	be exploited as additional information for improving the performance
	of tracking. We use the environment state to model the relationship
	between the objects and environments, and integrate it into the framework
	of Bayesian tracking. In this paper, distance transform is used to
	model the environment state, and particle filtering is employed as
	the paradigm for solving the Bayesian tracking problem. The adaptive
	dynamics model and environment prior are devised for the particle
	filter to fully utilize the environment information in the tracking
	process. Experiments on some video surveillance sequences demonstrate
	the effectiveness and robustness of our approach for tracking object
	motions in structured environments.},
  comment = {TRK},
  doi = {10.1109/TCSVT.2009.2031395},
  file = {:papers\\2010 JNL, Object Tracking in Structured Environments for Video Surveillance Applications (Zhu).pdf:PDF},
  issn = {1051-8215},
  keywords = {Bayesian tracking;adaptive dynamics model;distance transform;object
	motion tracking;particle filtering;structured environment;video surveillance;Bayes
	methods;image motion analysis;object detection;particle filtering
	(numerical methods);video surveillance;},
  timestamp = {-}
}

@ARTICLE{1996_JNL_RegionCompetitionUnifying_Zhu,
  author = {Zhu, Song Chun and Yuille, A.},
  title = {Region competition: unifying snakes, region growing, and Bayes/MDL
	for multiband image segmentation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on DOI
	- 10.1109/34.537343},
  year = {1996},
  volume = {18},
  pages = {884--900},
  number = {9},
  abstract = {We present a novel statistical and variational approach to image segmentation
	based on a new algorithm, named region competition. This algorithm
	is derived by minimizing a generalized Bayes/minimum description
	length (MDL) criterion using the variational principle. The algorithm
	is guaranteed to converge to a local minimum and combines aspects
	of snakes/balloons and region growing. The classic snakes/balloons
	and region growing algorithms can be directly derived from our approach.
	We provide theoretical analysis of region competition including accuracy
	of boundary location, criteria for initial conditions, and the relationship
	to edge detection using filters. It is straightforward to generalize
	the algorithm to multiband segmentation and we demonstrate it on
	gray level images, color images and texture images. The novel color
	model allows us to eliminate intensity gradients and shadows, thereby
	obtaining segmentation based on the albedos of objects. It also helps
	detect highlight regions},
  booktitle = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  comment = {?},
  file = {:papers\\1996 JNL, Region Competition_ Unifying Snakes, Region Growing, and Bayes MDL for Multiband Image Segmentation (Zhu, Yuille, PAMI, 1311).pdf:PDF},
  issn = {0162-8828},
  keywords = {Bayes methods, albedo, convergence of numerical methods, edge detection,
	image colour analysis, image segmentation, optimisation, variational
	techniques, Bayes method, albedos, boundary location, color images,
	convergence, edge detection, gray level images, local minimum, minimum
	description length, multiband image segmentation, region competition,
	region growing, snakes, texture images, uncertainty principle, variational
	principle},
  owner = {salman},
  timestamp = {01,300}
}

@ARTICLE{2006_JNL_MCMCdataAssociation_Khan,
  author = {Zia, Khan and Balch, T. and Dellaert, F.},
  title = {MCMC data association and sparse factorization updating for real
	time multitarget tracking with merged and multiple measurements},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2006},
  volume = {28},
  pages = {1960-72},
  abstract = {In several multitarget tracking applications, a target may return
	more than one measurement per target and interacting targets may
	return multiple merged measurements between targets. Existing algorithms
	for tracking and data association, initially applied to radar tracking,
	do not adequately address these types of measurements. Here, we introduce
	a probabilistic model for interacting targets that addresses both
	types of measurements simultaneously. We provide an algorithm for
	approximate inference in this model using a Markov chain Monte Carlo
	(MCMC)-based auxiliary variable particle filter. We Rao-Blackwellize
	the Markov chain to eliminate sampling over the continuous state
	space of the targets. A major contribution of this work is the use
	of sparse least squares updating and downdating techniques, which
	significantly reduce the computational cost per iteration of the
	Markov chain. Also, when combined with a simple heuristic, they enable
	the algorithm to correctly focus computation on interacting targets.
	We include experimental results on a challenging simulation sequence.
	We test the accuracy of the algorithm using two sensor modalities,
	video, and laser range data. We also show the algorithm exhibits
	real time performance on a conventional PC},
  comment = {trk_correspondence},
  file = {:papers\\2006 JNL, MCMC Data Association and Sparse Factorization  Updating for Real Time Multitarget Tracking with Merged and Multiple Measurements (Khan, Balch, Dellaert, PAMI, 27).pdf:PDF},
  keywords = {least squares approximations Markov processes Monte Carlo methods
	particle filtering (numerical methods) sampling methods sensor fusion
	state-space methods target tracking},
  owner = {salman},
  timestamp = {-}
}

@ARTICLE{2005_JNL_MCMCtracking_Dellaert,
  author = {Zia, Khan and Balch, T. and Dellaert, F.},
  title = {MCMC-based particle filtering for tracking a variable number of interacting
	targets},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2005},
  volume = {27},
  pages = {1805-1819},
  number = {11},
  comment = {trk_region},
  file = {:papers\\2005 JNL, MCMC-Based Particle Filtering for Tracking a Variable Number of Interacting Targets (Zia, Balch, Dellaert, PAMI, 192).pdf:PDF},
  keywords = {Markov processes Monte Carlo methods filtering theory sampling methods
	target tracking Markov chain Monte Carlo sampling step Markov random
	field motion interacting targets multitarget filter particle filtering
	Algorithms Animals Artificial Intelligence Computer Simulation Humans
	Image Enhancement Image Interpretation, Computer-Assisted Information
	Storage and Retrieval Markov Chains Models, Biological Models, Statistical
	Monte Carlo Method Motion Movement Pattern Recognition, Automated
	Subtraction Technique Video Recording},
  owner = {salman},
  timestamp = {00,200}
}

@BOOK{1997_BOOK_SignalCompression_Jayant,
  title = {Signal Compression, Coding of Speech, Audio, Image and Video (Selected
	Topics in Electronics and Systems)},
  publisher = {World Scientific Publishing Company},
  year = {1997},
  editor = {Jayant, N.},
  comment = {IT_book},
  timestamp = {-}
}

@BOOK{1995_BOOK_MCMCinPractice_Gilk,
  title = {Markov Chain Monte Carlo in Practice: Interdisciplinary Statistics
	(Chapman \& Hall/CRC Interdisciplinary Statistics Series)},
  publisher = {Chapman and Hall/CRC},
  year = {1995},
  editor = {Gilks W.R and Richardson, S. and Spiegelhalter, David},
  edition = {1},
  month = {December},
  abstract = {{General state-space Markov chain theory has evolved to make it both
	more accessible and more powerful. Markov Chain Monte Carlo in Practice
	introduces MCMC methods and their applications while also providing
	some theoretical background. Considering the broad audience, the
	editors emphasize practice rather than theory and keep the technical
	content to a minimum. They offer step-by-step instructions for using
	the methods presented and show the importance of MCMC in real applications
	with examples ranging from the simple to the more complex in fields
	such as archaeology, astronomy, biostatistics, genetics, epidemiology,
	and image analysis. }},
  comment = {PRML_book},
  day = {01},
  file = {:papers\\1995 BOOK, Markov Chain Monte Carlo In Practice (Gilks, Richardson, Spiegelhalter, Chapman).pdf:PDF},
  howpublished = {Hardcover},
  isbn = {0412055511},
  posted-at = {2010-04-24 19:35:32},
  priority = {2},
  timestamp = {-},
  url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0412055511}
}

@MISC{2008_SFT_Ross,
  title = {Incremental Visual Tracking software},
  howpublished = {\url{http://www.cs.toronto.edu/~dross/ivt/}}
}

@MISC{Facebook,
  title = {Facebook},
  howpublished = {\url{http://www.facebook.com/}}
}

@MISC{MPEG2,
  title = {MPEG-2 Video Compression},
  howpublished = {\url{http://www.bbc.co.uk/rd/pubs/papers/paper_14/paper_14.shtml}},
  file = {:papers\\1995 JNL, MPEG-2 video compression (Tudor).pdf:PDF}
}

@TECHREPORT{TechRep_PiecewiseInterpolation,
  title = {Piecewise Polynomial Interpolation},
  comment = {TRK_contour},
  file = {:papers\\TechRep, Piecewise Polynomial Interpolation.pdf:PDF},
  owner = {salman},
  timestamp = {-}
}

@MISC{VTK,
  title = {{Visualization Toolkit (VTK)}},
  howpublished = {\url{http://www.vtk.org/}}
}

@MISC{Web_COIL,
  title = {Columbia University Image Library (COIL-100)},
  howpublished = {\url{http://www.cs.columbia.edu/CAVE/software/softlib/coil-100.php}},
  owner = {salman}
}

@MISC{Wikipedia,
  title = {Wikipedia},
  howpublished = {\url{http://www.facebook.com/}}
}

@MISC{xxxx_MatrixCalculus,
  title = {Matrix Calculus},
  howpublished = {\url{http://www.colorado.edu/engineering/cas/courses.d/IFEM.d/IFEM.AppD.d/IFEM.AppD.pdf}},
  file = {:papers\\xxxx, MatrixCalculus.pdf:PDF}
}

@MISC{Youtube,
  title = {Youtube},
  howpublished = {\url{http://www.youtube.com/}}
}

@MANUAL{2010_MAN_CUDA_Nvidia,
  title = {CUDA Programming Guide},
  organization = {Nvidia Corporation},
  year = {2010},
  comment = {GPU},
  owner = {salman},
  timestamp = {-}
}

@comment{jabref-meta: fileDirectory:C:\\salman\\work\\writing\\proposa
l2\\papers;}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Object Tracking: a survey\;0\;1955_JNL_HungarianMethod
_Kuhn\;1967_JNL_ErrorBoundsConvolutionalCodes_Viterbi\;1968_JNL_Rankin
gAssignments_Murty\;1973_JNL_TextureClassification_Haralick\;1979_CNF_
VisualMappingRobotRover_Moravec\;1979_JNL_AccumulativePictureDifferenc
es_Jain\;1979_JNL_MTT_Reid\;1980_PhD_TexturedSegmentation_Laws\;1981_C
NF_IterativeImageRegistration_LucasKanade\;1981_JNL_OpticalFlow_HornSc
hunck\;1986_JNL_Edges_Canny\;1986_JNL_ImageFlowConstraint_Schunck\;198
6_JNL_ObjectMotionNoisyImage_Broida\;1987_JNL_FeatureTrajectories_Seth
i\;1987_JNL_NonGaussianSSmodelingNonStationaryTimeSeries_Kitagawa\;198
8_CNF_AdvancesMultisensorSurveillance_KanadeCollinsLiptonBurtWixson\;1
988_CNF_CombinedCornerEdgeDetector_Harris\;1988_JNL_Snakes_Kass\;1989_
JNL_HierarchyPictureSegmentation_BeaulieuGoldberg\;1989_JNL_KalmanFilt
erDepth_Matthies\;1989_JNL_PiecewiseSmoothFunctions_MumfordShah\;1989_
JNL_TutorialHMM_Rabiner\;1989_JNL_Wavelets_Mallat\;1990_JNL_PointCorre
sp_Salari\;1991_CNF_3DreconstructionDetectionEstimation_Chang\;1991_JN
L_MotionCorrespondence_Rangarajan\;1992_CNF_TrgOptimalMarginClassifier
s_Boser\;1992_JNL_HausdorffMetric_Baddeley\;1993_CNF_TrackingNonrigid_
HuttenlocherNohRucklidge\;1993_JNL_GraphTheoreticClustering_Wu\;1993_J
NL_SURVEYcorresp_Cox\;1994_CNF_FeaturesToTrack_ShiTomasi\;1994_CNF_MLP
MHT_Streit\;1994_CNF_SteerablePyramidFiltersRotationInvariance_Greensp
an\;1994_JNL_ActiveContours_Ronfard\;1994_JNL_PerfOpticalFlow_Barron\;
1994_JNL_TRKmotion_Wang\;1995_CNF_GeodesicActiveContours_Caselles\;199
5_JNL_HashTableClassifier_Grewe\;1996_CNF_MultipleMotionsFlowFields_Bl
ack\;1996_JNL_DefectColorTextures_Song\;1996_JNL_EfficientMHT_Cox\;199
6_JNL_RegionCompetitionUnifying_Zhu\;1996_JNL_StatisticalMosaicsTracki
ng_Rowe\;1997_CNF_ClosedWorldTracking_Intille\;1997_CNF_ColorHeadTrack
ing_Fieguth\;1997_CNF_ObjectIdentificationBayesian_HuangRussell\;1997_
JNL_EigenTRK_Moghaddam\;1997_JNL_GeneralizationofLearningBoosting_Freu
nd\;1997_JNL_Pfinder_Wren\;1997_JNL_SURVEYprml_Blum\;1997_JNL_SplineIm
ageRegistration_Szeliski\;1998_CNF_FaceActiveAppearance_Edwards\;1998_
CNF_GeneralFrameworkObjectDetection_Papageorgiou\;1998_CNF_HeadTrackin
g_Birchfield\;1998_CNF_LabeledUnlabeled_BlumMitchell\;1998_JNL_Additiv
eLogisticRegression\;1998_JNL_Condensation_IsardBlake\;1998_JNL_Eigent
racking_Black\;1998_JNL_MultibodyFactorization_CosteiraKanade\;1998_JN
L_MultibodyGrouping_Gear\;1998_JNL_NNfaceDetection_Rowley\;1998_JNL_Vi
deoIndexingMosaics_Irani\;1999_CNF_3DtrajectoryRecoveryTracking_Rosale
s\;1999_CNF_BayesianMulticamera_KettnakerZabih\;1999_CNF_MeanShiftAnal
ysisAndApplications_Comaniciu\;1999_CNF_MultipleHypothesisFigureTracki
ng_Cham\;1999_CNF_RTtrackingMultiplePeople_Beymer\;1999_CNF_Transducti
veInferenceTextSVM_Joachims\;1999_CNF_Wallflower_Toyama\;1999_JNL_SURV
EYmotion_Gavrila\;1999_JNL_SurveyMotion_Aggarwal\;1999_JNL_Tracking_St
ructuredEnvironmentsDistributed_CaiAggarwal\;2000_CNF_3DshapeImageStre
ams_Bregler\;2000_CNF_BackgroundSubtraction_Elgammal\;2000_CNF_ErrorBa
ckgroundAdaption_Gao\;2000_CNF_ProbabilisticBGforTracking_Rittscher\;2
000_JNL_BayesianHumanInteractions_Oliver\;2000_JNL_GeodesicActiveConto
ursLevelSetsTracking_ParagiosDeriche\;2000_JNL_MonitoringActivitiesMul
tipleStreams_Lee\;2000_JNL_MorphingActiveContours_BertalmioSapiroRanda
ll\;2000_JNL_NormalizedCuts_ShiMalik\;2000_JNL_PeriodicMotion_CutlerDa
vis\;2000_JNL_ProbabilisticExclusionMacCormickBlake\;2000_JNL_W4_Harit
aoglu\;2001_CNF_DetectionRecognitionEvents_Ali\;2001_CNF_JPDAFhmmConto
urTracking_Chen\;2001_CNF_TRKhuman_Isard\;2001_CNF_TopologyFreeHMMsBG_
Stenger\;2001_JNL_AlgorithmsCooperativeMultisensorSurveillance_Collins
\;2001_JNL_ColorTexture_Paschos\;2001_JNL_EdgeDetectorEvaluation_Bowye
r\;2001_JNL_MotionCorrespondence_Veenman\;2001_JNL_MultipleCameraTrack
ing_DockstaderTekalp\;2001_JNL_ProbabilisticDataAssociation_Rasmussen\
;2001_JNL_RVM_Tipping\;2001_JNL_SURVEYmotion_Moeslund\;2001_JNL_Tempor
alObjectVerification_BaoxinChellappaQinfenDer\;2001_JNL_TrackingOcclud
ed_DockstaderTekalp\;2001_THE_OnlineEnsembleLearning_Oza\;2002_CNF_Aff
ineInvariantInterestPointDetector_MikolajczykSchmid\;2002_CNF_ColorSpa
ceSwitchingFaceTracking_SternEfros\;2002_CNF_ContourTrackingGraphCutsA
ctiveContours_Xu\;2002_CNF_FastTemplateMatching_SchweitzerBellWu\;2002
_CNF_NonlinearShapeMumfordShahSegmentation_CremersKohlbergerSchnorr\;2
002_CNF_SpaceTimeTracking_Torresani\;2002_JNL_BGFGkernelDensityEstimat
ion_Elgammal\;2002_JNL_GeodesicActiveRegionsLevelSetsSegmentation_Para
gios\;2002_JNL_IntensityAndTextureRobustChangeDetection_Li\;2002_JNL_M
eanShiftFeatureSpaceAnalysis_Comaniciu\;2002_JNL_MeanShift_Comaniciu\;
2002_JNL_RegionTrackingLevelSetPDEs_Mansouri\;2002_JNL_SequentialMCMTT
_Hue\;2002_JNL_TrackingBayesianEstimationDynamicLayerTaoSawhneyKumar\;
2003_CNF_AdaptiveTrackingParticleFilters_Zhou\;2003_CNF_BGsubtractionD
ynamicScenes_Monnet\;2003_CNF_DRF_SanjivHebert\;2003_CNF_DynamicsOfInt
eractingObjects_Vaswani\;2003_CNF_SegmentKalmanFilter_Zhong\;2003_CNF_
TrackingMultipleCamerasDisjointViews_JavedRasheedShafiqueMubarakshah\;
2003_CNF_TrackingWithinAndAcrossCameraStreams_KangCohenMedioni\;2003_C
NF_TransductionMultiviewLearningEmails_Kockelkorn\;2003_JNL_AirborneIR
tracking_Yilmaz\;2003_JNL_ConsistentLabelingTrackingMultipleCameras_Kh
anMubarakshah\;2003_JNL_ContextualPrimingDetection_Torralba\;2003_JNL_
M2tracker_MittalLarrydavis\;2003_JNL_StatisticalShapeKnowledge_Cremers
\;2003_JNL_TRKkernel_Comaniciu\;2003_JNL_TRKsubspace_Jepson\;2003_NF_U
nsupervisedImprovementVisualDetectors_Levin\;2004_CNF_ProbabilisticTra
ckingMultipleFeatures_Serby\;2004_CNF_ReacquisitionUsingInvariantAppea
rance_KangCohenMedioni\;2004_JNL_BNhumanInteractions_Park\;2004_JNL_Bo
ostingImageRetrieval_Tieu\;2004_JNL_ContourTrackingOcclusionsMobileCam
eras_Yilmaz\;2004_JNL_SIFT_Lowe\;2004_JNL_SVMtracking_Avidan\;2004_JNL
_SegmentationPartialGroupConstraints_YuShi\;2004_JNL_VelocityTransform
_SatoAggarwal\;2005_JNL_DiscriminativeFeaturesTracking_CollinsYanxiLeo
rdeanu\;2005_JNL_EvaluationLocalDescriptors_Mikolajczyk\;2005_JNL_Pede
strianTracking_ViolaJones\;2005_JNL_PointTRK_Shafique\;2006_JNL_Algebr
aicMotionSegmentation_Vidal\;;
1 ExplicitGroup:RVQ\;0\;1982_CNF_SpeechRVQ_JuangGray\;2003_JNL_HighRat
eVQDetection_Hero\;2004_JNL_RateDistortionVsDatabases_Truncel\;;
1 ExplicitGroup:salman\;0\;2008_TECH_3DvideoColorEnhancement_Aslam\;20
08_TECH_MGDSP_Aslam\;2009_CNF_CVcompMS1_Aslam\;2009_CNF_Compensation_A
slam\;2009_TECH_VideoStabilization_Arici\;2010_CNF_HMMRVQ_Aslam\;2010_
CNF_Quant_Aslam\;2010_CNF_TrkRVQ_Aslam\;2010_CNF_VehicleContour_Aslam\
;;
1 ExplicitGroup:Dr Barnes\;0\;1991_CNF_DesignPerformanceRVQ_Frost\;199
2_CNF_ImageCodingRVQ_Kossentini\;1992_JNL_RVQ_Barnes\;1993_JNL_RVQDSC_
Barnes\;1995_JNL_OptimalityRVQ_Kossentini\;1996_CNF_VQclassification_B
arnes\;1996_JNL_AdvancesRVQ_Barnes\;2004_CNF_DSSAdataMining_Barnes\;20
07_JNL_IDDM_Barnes\;;
1 ExplicitGroup:Dr Bobick\;0\;1995_CNF_GestureAnalysis_Wilson\;1996_CN
F_AppearanceAction_Bobick\;1996_TECH_Kidsroom_Bobick\;1997_CNF_ClosedW
orldTracking_Intille\;1999_JNL_Kidsroom_Bobick\;2001_JNL_MotionTemplat
es_Bobick\;;
1 ExplicitGroup:2D to 3D\;0\;;
}

